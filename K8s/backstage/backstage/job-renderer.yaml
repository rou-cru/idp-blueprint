apiVersion: v1
kind: ServiceAccount
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
rules:
  - apiGroups: [""]
    resources: ["configmaps", "secrets"]
    verbs: ["get", "list"]
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["backstage-app-config-override"]
    verbs: ["get", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
subjects:
  - kind: ServiceAccount
    name: config-renderer
    namespace: backstage
roleRef:
  kind: Role
  name: config-renderer
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: batch/v1
kind: Job
metadata:
  name: backstage-config-renderer
  namespace: backstage
  annotations:
    # Run ahead of the chart (wave -1) but as a Sync hook so the SA/Role from the sync phase already exist
    argocd.argoproj.io/sync-wave: "-1"
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
    checkov.io/skip1: CKV_K8S_38=Job needs SA token to run kubectl apply in-cluster
    checkov.io/skip2: CKV_K8S_43=Dev tag moves frequently; digest not pinned during active development
    checkov.io/skip3: CKV_K8S_15=Intentionally IfNotPresent to avoid repeated pulls of ops image
spec:
  template:
    metadata:
      annotations:
        # checkov:skip=CKV_K8S_38:Job requires ServiceAccount token for kubectl
        # checkov:skip=CKV_K8S_43:Using pinned tag instead of digest
        checkov.io/skip1: CKV_K8S_38=Job requires SA token
        checkov.io/skip2: CKV_K8S_43=Using pinned tag
    spec:
      serviceAccountName: config-renderer
      restartPolicy: OnFailure
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        runAsGroup: 10001
        fsGroup: 10001
      containers:
        - name: renderer
          image: roucru/idp-blueprint:ops
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 10001
            runAsGroup: 10001
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop:
                - ALL
          command: ["bash", "-c"]
          args:
            - |
              set -e  # Exit on any error
              set -x  # Print commands for debugging

              echo "=== Step 1: Loading variables from ConfigMap ==="
              # Load variables from individual files in /vars
              if [ -f /vars/DNS_SUFFIX ]; then
                export DNS_SUFFIX=$(cat /vars/DNS_SUFFIX)
                echo "Loaded DNS_SUFFIX: $DNS_SUFFIX"
              fi
              if [ -f /vars/CLUSTER_NAME ]; then
                export CLUSTER_NAME=$(cat /vars/CLUSTER_NAME)
                echo "Loaded CLUSTER_NAME: $CLUSTER_NAME"
              fi
              if [ -f /vars/GITHUB_ORG ]; then
                export GITHUB_ORG=$(cat /vars/GITHUB_ORG)
                echo "Loaded GITHUB_ORG: $GITHUB_ORG"
              fi
              if [ -f /vars/GITHUB_REPO ]; then
                export GITHUB_REPO=$(cat /vars/GITHUB_REPO)
                echo "Loaded GITHUB_REPO: $GITHUB_REPO"
              fi
              if [ -f /vars/GITHUB_BRANCH ]; then
                export GITHUB_BRANCH=$(cat /vars/GITHUB_BRANCH)
                echo "Loaded GITHUB_BRANCH: $GITHUB_BRANCH"
              fi

              # Fallback for dev/manual testing
              if [ -f /vars/env ]; then
                echo "Loading from /vars/env file"
                source /vars/env
              fi

              echo "=== Step 2: Validating required variables ==="
              if [ -z "$DNS_SUFFIX" ]; then
                echo "ERROR: DNS_SUFFIX not found!"
                exit 1
              fi

              if [ -z "$CLUSTER_NAME" ]; then
                echo "WARNING: CLUSTER_NAME not found, using default"
                export CLUSTER_NAME="idp-demo"
              fi

              # Validate GitHub variables are set (no hardcoded defaults)
              if [ -z "$GITHUB_ORG" ] || [ -z "$GITHUB_REPO" ] || [ -z "$GITHUB_BRANCH" ]; then
                echo "ERROR: GitHub variables not properly configured"
                echo "GITHUB_ORG=$GITHUB_ORG, GITHUB_REPO=$GITHUB_REPO, GITHUB_BRANCH=$GITHUB_BRANCH"
                echo "These values should come from config.toml via ApplicationSet patches"
                exit 1
              fi

              echo "=== Step 3: Rendering configuration template ==="
              echo "Using variables:"
              echo "  DNS_SUFFIX=$DNS_SUFFIX"
              echo "  CLUSTER_NAME=$CLUSTER_NAME"
              echo "  GITHUB_ORG=$GITHUB_ORG"
              echo "  GITHUB_REPO=$GITHUB_REPO"
              echo "  GITHUB_BRANCH=$GITHUB_BRANCH"

              # Render template using envsubst (more robust than sed)
              # We explicitly list variables to avoid replacing runtime secrets (like ${BACKEND_SECRET})
              export VARS_TO_SUBST='$DNS_SUFFIX $CLUSTER_NAME $GITHUB_ORG $GITHUB_REPO $GITHUB_BRANCH'
              envsubst "$VARS_TO_SUBST" < /tpl/app-config.override.yaml > /tmp/config.yaml

              echo "=== Rendered configuration preview (first 20 lines) ==="
              head -n 20 /tmp/config.yaml

              echo "=== Step 4: Patching ConfigMap ==="
              kubectl create configmap backstage-app-config-override \
                --from-file=app-config.override.yaml=/tmp/config.yaml \
                --dry-run=client -o yaml | \
                kubectl apply -f -

              echo "=== SUCCESS: ConfigMap patched successfully ==="
          volumeMounts:
            - name: vars
              mountPath: /vars
            - name: tpl
              mountPath: /tpl
            - name: tmp-volume
              mountPath: /tmp
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"
      volumes:
        - name: vars
          configMap:
            name: idp-vars-backstage
            optional: true
        - name: tpl
          configMap:
            name: backstage-config-tpl
        - name: tmp-volume
          emptyDir: {}

apiVersion: v1
kind: ServiceAccount
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
rules:
  - apiGroups: [""]
    resources: ["configmaps", "secrets"]
    verbs: ["get", "list", "create", "update", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
subjects:
  - kind: ServiceAccount
    name: config-renderer
    namespace: backstage
roleRef:
  kind: Role
  name: config-renderer
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: batch/v1
kind: Job
metadata:
  name: backstage-config-renderer
  namespace: backstage
  annotations:
    # Run ahead of the chart (wave -1) but as a Sync hook so the SA/Role from the sync phase already exist
    argocd.argoproj.io/sync-wave: "-1"
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
spec:
  template:
    spec:
      serviceAccountName: config-renderer
      restartPolicy: OnFailure
      containers:
        - name: renderer
          image: bitnami/kubectl:latest
          command: ["/bin/bash", "-c"]
          args:
            - |
              # Load variables
              if [ -f /vars/DNS_SUFFIX ]; then
                export DNS_SUFFIX=$(cat /vars/DNS_SUFFIX)
              fi
              if [ -f /vars/CLUSTER_NAME ]; then
                export CLUSTER_NAME=$(cat /vars/CLUSTER_NAME)
              fi
              if [ -f /vars/GITHUB_ORG ]; then
                export GITHUB_ORG=$(cat /vars/GITHUB_ORG)
              fi
              if [ -f /vars/GITHUB_REPO ]; then
                export GITHUB_REPO=$(cat /vars/GITHUB_REPO)
              fi
              if [ -f /vars/GITHUB_BRANCH ]; then
                export GITHUB_BRANCH=$(cat /vars/GITHUB_BRANCH)
              fi

              # Fallback for dev/manual testing
              if [ -f /vars/env ]; then
                source /vars/env
              fi

              if [ -z "$DNS_SUFFIX" ]; then
                echo "DNS_SUFFIX not found!"
                exit 1
              fi

              if [ -z "$CLUSTER_NAME" ]; then
                echo "Warning: CLUSTER_NAME not found, using default"
                export CLUSTER_NAME="idp-demo"
              fi

              # Validate GitHub variables are set (no hardcoded defaults)
              if [ -z "$GITHUB_ORG" ] || [ -z "$GITHUB_REPO" ] || [ -z "$GITHUB_BRANCH" ]; then
                echo "Error: GitHub variables not properly configured"
                echo "GITHUB_ORG=$GITHUB_ORG, GITHUB_REPO=$GITHUB_REPO, GITHUB_BRANCH=$GITHUB_BRANCH"
                echo "These values should come from config.toml via ApplicationSet patches"
                exit 1
              fi

              echo "Rendering configuration using DNS_SUFFIX=$DNS_SUFFIX, CLUSTER_NAME=$CLUSTER_NAME, GITHUB_ORG=$GITHUB_ORG, GITHUB_REPO=$GITHUB_REPO, GITHUB_BRANCH=$GITHUB_BRANCH"

              # Render template safely with envsubst
              export DNS_SUFFIX CLUSTER_NAME GITHUB_ORG GITHUB_REPO GITHUB_BRANCH
              envsubst '${DNS_SUFFIX} ${CLUSTER_NAME} ${GITHUB_ORG} ${GITHUB_REPO} ${GITHUB_BRANCH}' \
                < /tpl/app-config.override.yaml > /tmp/config.yaml

              # Patch ConfigMap
              kubectl create configmap backstage-app-config-override \
                --from-file=app-config.override.yaml=/tmp/config.yaml \
                --dry-run=client -o yaml | \
                kubectl apply -f -
                
              echo "ConfigMap patched successfully."
          volumeMounts:
            - name: vars
              mountPath: /vars
            - name: tpl
              mountPath: /tpl
      volumes:
        - name: vars
          configMap:
            name: idp-vars-backstage
            optional: true
        - name: tpl
          configMap:
            name: backstage-config-tpl

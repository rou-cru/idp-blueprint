apiVersion: v1
kind: ServiceAccount
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
rules:
  - apiGroups: [""]
    resources: ["configmaps", "secrets"]
    verbs: ["get", "list", "create", "update", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: config-renderer
  namespace: backstage
  annotations:
    argocd.argoproj.io/sync-wave: "-2"
subjects:
  - kind: ServiceAccount
    name: config-renderer
    namespace: backstage
roleRef:
  kind: Role
  name: config-renderer
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: batch/v1
kind: Job
metadata:
  name: backstage-config-renderer
  namespace: backstage
  annotations:
    # Run ahead of the chart (wave -1) but as a Sync hook so the SA/Role from the sync phase already exist
    argocd.argoproj.io/sync-wave: "-1"
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
spec:
  template:
    spec:
      serviceAccountName: config-renderer
      restartPolicy: OnFailure
      containers:
        - name: renderer
          image: alpine:latest
          command: ["/bin/sh", "-c"]
          args:
            - |
              set -e  # Exit on any error
              set -x  # Print commands for debugging

              echo "=== Step 1: Installing dependencies ==="
              apk add --no-cache curl gettext bash

              echo "=== Step 2: Installing kubectl ==="
              curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
              chmod +x kubectl
              mv kubectl /usr/local/bin/
              kubectl version --client

              echo "=== Step 3: Loading variables from ConfigMap ==="
              # Load variables from individual files in /vars
              if [ -f /vars/DNS_SUFFIX ]; then
                export DNS_SUFFIX=$(cat /vars/DNS_SUFFIX)
                echo "Loaded DNS_SUFFIX: $DNS_SUFFIX"
              fi
              if [ -f /vars/CLUSTER_NAME ]; then
                export CLUSTER_NAME=$(cat /vars/CLUSTER_NAME)
                echo "Loaded CLUSTER_NAME: $CLUSTER_NAME"
              fi
              if [ -f /vars/GITHUB_ORG ]; then
                export GITHUB_ORG=$(cat /vars/GITHUB_ORG)
                echo "Loaded GITHUB_ORG: $GITHUB_ORG"
              fi
              if [ -f /vars/GITHUB_REPO ]; then
                export GITHUB_REPO=$(cat /vars/GITHUB_REPO)
                echo "Loaded GITHUB_REPO: $GITHUB_REPO"
              fi
              if [ -f /vars/GITHUB_BRANCH ]; then
                export GITHUB_BRANCH=$(cat /vars/GITHUB_BRANCH)
                echo "Loaded GITHUB_BRANCH: $GITHUB_BRANCH"
              fi

              # Fallback for dev/manual testing
              if [ -f /vars/env ]; then
                echo "Loading from /vars/env file"
                source /vars/env
              fi

              echo "=== Step 4: Validating required variables ==="
              if [ -z "$DNS_SUFFIX" ]; then
                echo "ERROR: DNS_SUFFIX not found!"
                exit 1
              fi

              if [ -z "$CLUSTER_NAME" ]; then
                echo "WARNING: CLUSTER_NAME not found, using default"
                export CLUSTER_NAME="idp-demo"
              fi

              # Validate GitHub variables are set (no hardcoded defaults)
              if [ -z "$GITHUB_ORG" ] || [ -z "$GITHUB_REPO" ] || [ -z "$GITHUB_BRANCH" ]; then
                echo "ERROR: GitHub variables not properly configured"
                echo "GITHUB_ORG=$GITHUB_ORG, GITHUB_REPO=$GITHUB_REPO, GITHUB_BRANCH=$GITHUB_BRANCH"
                echo "These values should come from config.toml via ApplicationSet patches"
                exit 1
              fi

              echo "=== Step 5: Rendering configuration template ==="
              echo "Using variables:"
              echo "  DNS_SUFFIX=$DNS_SUFFIX"
              echo "  CLUSTER_NAME=$CLUSTER_NAME"
              echo "  GITHUB_ORG=$GITHUB_ORG"
              echo "  GITHUB_REPO=$GITHUB_REPO"
              echo "  GITHUB_BRANCH=$GITHUB_BRANCH"

              # Render template safely with envsubst
              export DNS_SUFFIX CLUSTER_NAME GITHUB_ORG GITHUB_REPO GITHUB_BRANCH
              envsubst '${DNS_SUFFIX} ${CLUSTER_NAME} ${GITHUB_ORG} ${GITHUB_REPO} ${GITHUB_BRANCH}' \
                < /tpl/app-config.override.yaml > /tmp/config.yaml

              echo "=== Rendered configuration preview (first 20 lines) ==="
              head -n 20 /tmp/config.yaml

              echo "=== Step 6: Patching ConfigMap ==="
              kubectl create configmap backstage-app-config-override \
                --from-file=app-config.override.yaml=/tmp/config.yaml \
                --dry-run=client -o yaml | \
                kubectl apply -f -

              echo "=== SUCCESS: ConfigMap patched successfully ==="

              # Verify the patch was applied
              echo "=== Verification: Checking ConfigMap content ==="
              kubectl get configmap backstage-app-config-override -n backstage -o jsonpath='{.data.app-config\.override\.yaml}' | head -n 10
          volumeMounts:
            - name: vars
              mountPath: /vars
            - name: tpl
              mountPath: /tpl
      volumes:
        - name: vars
          configMap:
            name: idp-vars-backstage
            optional: true
        - name: tpl
          configMap:
            name: backstage-config-tpl

# @section -- CRDs
# -- Disables the installation of CRDs, as they are managed separately.
crds:
  enabled: false

# @section -- Alertmanager
alertmanager:
  # -- Enable Alertmanager for alert routing (required for Pyrra burn-rate alerts)
  enabled: true
  alertmanagerSpec:
    # -- (string) Priority class for Alertmanager pod
    priorityClassName: platform-observability
    resources:
      requests:
        # -- (string) CPU request
        cpu: 25m
        # -- (string) Memory request
        memory: 64Mi
      limits:
        # -- (string) CPU limit
        cpu: 100m
        # -- (string) Memory limit
        memory: 128Mi
  # -- Alertmanager configuration
  config:
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'argo-events-webhook'
    receivers:
      - name: 'null'
      - name: 'argo-events-webhook'
        webhook_configs:
          - url: 'http://alertmanager-eventsource-svc.events.svc.cluster.local:12000/webhook'
            send_resolved: true

kubeEtcd:
  enabled: false
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false
kubeProxy:
  enabled: false

# @section -- Prometheus
prometheus:
  prometheusSpec:
    priorityClassName: platform-observability
    # -- Select all ServiceMonitors across all namespaces
    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}
    # -- Select all PodMonitors across all namespaces
    podMonitorSelector: {}
    podMonitorNamespaceSelector: {}
    # -- Select PrometheusRules from any namespace (needed for Pyrra rules)
    ruleSelector: {}
    ruleNamespaceSelector: {}
    # -- Global scrape interval for all ServiceMonitors (unless overridden).
    scrapeInterval: 60s
    # -- Global scrape timeout for all ServiceMonitors (unless overridden).
    scrapeTimeout: 40s
    # -- Metrics retention time.
    retention: 6h
    # -- Enable persistence for Prometheus TSDB. 1Gi supports 6h retention for ~50 pods
    # with 4x overhead margin. Data survives pod restarts but is lost on cluster destruction.
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi
    resources:
      requests:
        # -- (string) CPU request
        cpu: 200m
        # -- (string) Memory request
        memory: 512Mi
      limits:
        # -- (string) CPU limit
        cpu: 500m
        # -- (string) Memory limit
        memory: 1536Mi

    additionalScrapeConfigs:
      - job_name: 'kube-scheduler'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_node_label_node_role_kubernetes_io_control_plane]
          regex: true
          action: keep
        - source_labels: [__meta_kubernetes_node_address_InternalIP]
          replacement: $1:10259
          target_label: __address__
        - source_labels: [__meta_kubernetes_node_name]
          target_label: instance

      - job_name: 'kube-controller-manager'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_node_label_node_role_kubernetes_io_control_plane]
          regex: true
          action: keep
        - source_labels: [__meta_kubernetes_node_address_InternalIP]
          replacement: $1:10257
          target_label: __address__
        - source_labels: [__meta_kubernetes_node_name]
          target_label: instance

      - job_name: 'kube-proxy'
        kubernetes_sd_configs:
        - role: node
        scheme: http
        # Kube-proxy corre en todos los nodos, no filtramos por control-plane
        relabel_configs:
        - source_labels: [__meta_kubernetes_node_address_InternalIP]
          replacement: $1:10249
          target_label: __address__
        - source_labels: [__meta_kubernetes_node_name]
          target_label: instance

# @section -- Grafana
grafana:
  priorityClassName: platform-dashboards
  # -- Enable persistence for dashboards and settings
  persistence:
    enabled: true
    type: pvc
    size: 1Gi
    accessModes: [ReadWriteOnce]
  # -- Use existing secret for admin credentials from Vault via ESO.
  admin:
    # checkov:skip=CKV_SECRET_6:The existingSecret field is a reference to a secret, not a secret itself.
    existingSecret: grafana-admin-credentials
    userKey: admin-user
    # checkov:skip=CKV_SECRET_6:The passwordKey field is a key name, not a secret.
    passwordKey: admin-password
  resources:
    requests:
      # -- (string) CPU request
      cpu: 50m
      # -- (string) Memory request
      memory: 128Mi
    limits:
      # -- (string) CPU limit
      cpu: 250m
      # -- (string) Memory limit
      memory: 256Mi
  # -- Additional datasources for Grafana.
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki.observability.svc.cluster.local:3100
      access: proxy
      isDefault: false
  # -- Automatically install useful plugins on startup.
  plugins:
    - grafana-piechart-panel
    - grafana-polystat-panel
    - marcusolsson-json-datasource
  # -- Sidecar to automatically discover and load dashboards from ConfigMaps.
  defaultDashboardsEnabled: false
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      # -- An empty labelValue searches for the presence of the label, regardless of its value.
      labelValue: ""
      # -- Search in all namespaces for dashboards.
      searchNamespace: ALL
  # -- Advanced Grafana configuration via grafana.ini
  grafana.ini:
    users:
      # -- Disables the user sign-up page.
      allow_sign_up: false
      # -- Set the default UI theme to dark.
      default_theme: dark

# @section -- Prometheus Operator
# -- Resource limits and requests for the Prometheus Operator.
prometheusOperator:
  # -- (string) Priority class
  priorityClassName: platform-observability

  # -- Admission webhooks configuration for PrometheusRules validation
  admissionWebhooks:
    enabled: true
    # -- Use cert-manager to manage webhook certificates instead of Helm hook Jobs
    # This avoids compatibility issues with ArgoCD+Kustomize workflow
    certManager:
      enabled: true
      # -- Reference to the ClusterIssuer that will issue webhook certificates
      issuerRef:
        name: ca-issuer
        kind: ClusterIssuer
    # -- Disable Helm hook-based certificate management (incompatible with ArgoCD+Kustomize)
    patch:
      enabled: false

  resources:
    requests:
      # -- (string) CPU request
      cpu: 25m
      # -- (string) Memory request
      memory: 32Mi
    limits:
      # -- (string) CPU limit
      cpu: 50m
      # -- (string) Memory limit
      memory: 64Mi

# @section -- Kube State Metrics
# -- Resource limits and requests for kube-state-metrics.
kube-state-metrics:
  # -- (string) Priority class
  priorityClassName: platform-observability
  # -- (list) Enable only relevant resource types (whitelist approach)
  extraArgs:
    - --resources=cronjobs,daemonsets,deployments,jobs,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,pods,services,statefulsets,storageclasses

  prometheus:
    monitor:
      # -- (list) Drop high-cardinality labels
      metricRelabelings:
        # Drop uid label
        - action: labeldrop
          regex: "uid"
        # Drop container_id
        - action: labeldrop
          regex: "container_id"
        # Drop image_id
        - action: labeldrop
          regex: "image_id"

  resources:
    requests:
      # -- (string) CPU request
      cpu: 25m
      # -- (string) Memory request
      memory: 64Mi
    limits:
      # -- (string) CPU limit
      cpu: 50m
      # -- (string) Memory limit
      memory: 128Mi

# @section -- Prometheus Node Exporter
# -- Resource limits and requests for the node-exporter.
prometheus-node-exporter:
  # -- (string) Priority class
  priorityClassName: platform-observability
  # -- (list) Minimal collector set optimized for K3d
  extraArgs:
    - --collector.disable-defaults
    - --collector.cpu
    - --collector.cpufreq
    - --collector.meminfo
    - --collector.diskstats
    - --collector.filesystem
    - --collector.netdev
    - --collector.loadavg
    - --collector.pressure
    - --collector.vmstat
    - --collector.stat
    - --collector.uname

  resources:
    requests:
      # -- (string) CPU request
      cpu: 15m
      # -- (string) Memory request
      memory: 24Mi
    limits:
      # -- (string) CPU limit
      cpu: 30m
      # -- (string) Memory limit
      memory: 48Mi

# @section -- Windows Exporter
# -- Disables unnecessary components.
windows-exporter:
  enabled: false

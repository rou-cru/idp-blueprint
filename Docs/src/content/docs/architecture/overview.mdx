---
title: Architecture Overview
description: High-level architecture overview of IDP Blueprint platform
layout: ../../../layouts/DocsLayout.astro
sidebar:
  label: Overview
  order: 1
---

## Context and goals

IDP Blueprint provides a compact, self-hosted platform stack for Kubernetes clusters. The architecture supports edge, on-premises, and constrained environments where horizontal scaling is limited, though the same patterns apply to larger deployments. The platform operates GitOps-first with Git as the source of truth and ArgoCD handling reconciliation. No managed control planes or commercial licenses are required, making it fully cloud-agnostic.

The platform serves three primary use cases. Engineers can evaluate a realistic platform stack on a laptop or lab cluster without cloud dependencies. Teams can prototype internal developer platforms without vendor commitments. Organizations can train platform, SRE, and security engineers on GitOps and policy-driven operations with hands-on infrastructure.

## System context

A single Kubernetes cluster sits between engineers and Git. Git owns all intent, ArgoCD reconciles that intent into the cluster, and traffic flows back out through Gateway API. Platform engineers operate the stack while application teams ship workloads through it. External systems include the Git provider as source of truth, container registries for images, and optional cloud services for external secret stores. The deployment target is one cluster—either local k3d or remote—treated as interchangeable infrastructure.

[System Context Diagram](diagrams.d2#L1)

## Container view

:::note
This diagram zooms into the IDP Blueprint system to show the major containers (applications and data stores) and their interactions. Each container is a separately deployable unit.
:::

The architecture is organized into distinct logical planes, designed to separate concerns and maximize stability.

At the foundation lies the **Infrastructure Core**, which provides the compute control plane via the Kubernetes API and etcd. Networking and ingress are handled here by Cilium CNI and the Gateway API, ensuring secure and performant traffic management.

Above this sits the **Platform Services** layer, supplying essential utilities for operation. This includes HashiCorp Vault for secrets management, the External Secrets Operator for syncing credentials, and cert-manager for automated PKI. A comprehensive observability suite—comprising Prometheus, Loki, and Fluent-bit—also resides here to capture metrics and logs.

To ensure stability and compliance, the **Automation and Governance** layer enforces state through ArgoCD's reconciliation loops and continuously audits configuration using Kyverno policies.

Finally, the **Developer-Facing Stacks** provide the actual tools for engineering teams. This includes dashboards (Grafana, Pyrra), CI/CD pipelines (Argo Workflows), code quality gates (SonarQube), and security scanners (Trivy Operator).

All components are either bootstrapped once from the `IT/` directory or continuously reconciled from `K8s/`.

![Container View](../assets/diagrams/architecture/overview-container-view.svg)

## Platform layers

The same components can be viewed as a set of logical layers:

| Layer                       | Components (examples)                                      | Responsibility                              |
|-----------------------------|------------------------------------------------------------|---------------------------------------------|
| Infrastructure core         | Kubernetes, Cilium, Gateway API                            | Scheduling, networking, traffic in/out      |
| Platform services           | Vault, ESO, cert‑manager, Prometheus, Loki, Fluent‑bit     | Secrets, PKI, metrics, logs                 |
| Automation & governance     | ArgoCD, ApplicationSets, Kyverno, Policy Reporter          | GitOps, reconciliation, policies, compliance|
| Developer‑facing stacks     | Grafana, Pyrra, Argo Workflows, SonarQube, Trivy Operator  | Dashboards, pipelines, scanning             |

This layering is reflected in the repository layout and in the deployment order.

## GitOps backbone

The control plane operates through Git-driven automation across three layers. Bootstrap (`IT/`) handles one-time installation of core infrastructure including Cilium, Vault, ESO, cert-manager, ArgoCD, and Gateway API, along with minimal namespaces and RBAC. GitOps (`K8s/`) manages continuously reconciled state through stacks grouped by concern—observability, CI/CD, and security—with one ApplicationSet per stack that discovers subdirectories and generates ArgoCD Applications. Policies (`Policies/`) define Kyverno rules and related configuration.

ArgoCD applies all changes from these directories to the cluster. Manual cluster modifications are treated as drift and automatically reverted. For implementation details, see [GitOps model](../concepts/gitops-model) and [K8s directory architecture](applications).

## Resilience on a small cluster

High availability on a 3-node edge cluster requires a different approach than large cloud deployments. The design prioritizes tiered criticality: the core control plane (Kubernetes API and etcd) receives highest priority, critical infrastructure like ArgoCD and Prometheus must survive node loss, and everything else can degrade or restart later. Scheduling uses PriorityClasses to separate infrastructure from workloads, node labels to define pools (control plane, infra, workloads), and tolerations that allow critical components to use the control plane node as a lifeboat.

When a node fails, the platform preserves visibility through Prometheus and Loki while maintaining the ability to repair via ArgoCD, even if some stacks run in degraded mode. See [Scheduling, priority, and node pools](../concepts/scheduling-nodepools) and [Disaster recovery](../operate/disaster-recovery) for operational details.

## Technology Selection Criteria

The technology stack follows a strict set of design principles tailored for constrained environments. We prioritized components that exhibit **good behavior under limited resources**, avoiding heavyweight solutions that demand extensive node pools. Every selected tool must support **native declarative configuration** via Kubernetes CRDs to ensure full GitOps compatibility.

We also mandate a **healthy open-source ecosystem** and **zero commercial license requirements**, ensuring the platform remains accessible for labs and internal use without vendor lock-in. Finally, the architecture is strictly **cloud-agnostic**, capable of running on bare metal, on-premises virtualization, or any public cloud provider without modification.

The blueprint focuses on what must be provided at the platform layer (networking, security, observability, policy, GitOps). Cloud‑specific services are left out by design.

import { Button } from '../../../components/content/index.ts';

<div class="flex justify-center mt-8">
  <Button href="/architecture/bootstrap/" variant="primary" icon="arrow-right">
    Next: Bootstrap
  </Button>
</div>

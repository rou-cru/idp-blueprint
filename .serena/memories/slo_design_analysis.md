# An√°lisis Cr√≠tico: Dise√±o de SLI/SLO del IDP

**Fecha**: 2025-12-19
**Contexto**: Auditor√≠a SRE del stack de observabilidad con enfoque en SLI/SLO implementation

## Resumen Ejecutivo

**Veredicto**: Los SLOs actuales tienen problemas fundamentales de dise√±o SRE:
- ‚ùå Miden componentes, no user experience
- ‚ùå Solo availability (0% latency SLIs)
- ‚ùå 4 componentes cr√≠ticos sin SLO
- ‚ùå Stack de observabilidad no aprovechado
- ‚ùå 2/6 SLOs t√©cnicamente rotos

## Problemas Fundamentales (Conceptuales)

### 1. Violaci√≥n de 4 Golden Signals (Google SRE)

**Problema**: 100% de SLOs miden solo availability/errors, 0% miden latency

**Google SRE Workbook dice**: Debes medir Latency + Errors + Traffic + Saturation

**Impacto Real**: Un servicio puede estar "disponible" (99%) pero inutilizable (latency 30s)

**Ejemplo**:
- Gateway actual: 98% requests sin 5xx ‚úì
- Gateway latency: ??? (no medido) ‚ùå
- User experience: "El portal carga pero tarda 10 segundos" = MALA experiencia aunque SLO pase

### 2. Miden Componentes, No User Experience

**Principio SRE**: SLIs deben medir lo que importa al USUARIO, no al OPERADOR

**Ejemplos actuales vs correctos**:

```
‚ùå ACTUAL: "ESO sincroniza 97% de secretos correctamente"
   Perspectiva: Operador del sistema
   Problema: No mide si PODS pueden obtener secretos

‚úÖ CORRECTO: "Pods obtienen secretos cuando arrancan 99% del tiempo"
   Perspectiva: Aplicaci√≥n/Developer
   Mide: Experience end-to-end

---

‚ùå ACTUAL: "Argo Workflows controller no crashea 99%"
   Perspectiva: Operador de platform
   Problema: Controller up ‚â† workflows funcionando

‚úÖ CORRECTO: "CI pipelines completan exitosamente 95% del tiempo"
   Perspectiva: Developer ejecutando CI
   Mide: Success rate real de workflows

---

‚ùå ACTUAL: "ArgoCD syncs exitosos 95%"
   Perspectiva: GitOps operator
   Problema: Sync exitoso ‚â† app funcionando

‚úÖ CORRECTO: "Apps est√°n healthy post-deploy 99%"
   Perspectiva: Developer deployando
   Mide: App health, no solo sync
```

### 3. Componentes Cr√≠ticos Sin SLO

**Componentes CORE del IDP sin ning√∫n SLO**:

1. **Backstage** (Portal Developer)
   - Criticidad: ALTA - es la puerta de entrada para developers
   - Sin Backstage: No self-service, no app provisioning
   - SLO faltante: UI availability, Scaffolder success rate

2. **Cert-Manager** (Certificate Management)
   - Criticidad: ALTA - sin certificados v√°lidos, no hay TLS
   - Sin Cert-Manager: Services no accesibles via HTTPS
   - SLO faltante: Certificate renewal success, time-to-ready

3. **Kubernetes API Server** (Platform Core)
   - Criticidad: CR√çTICA - sin K8s API, NADA funciona
   - Es literalmente el cerebro del cluster
   - SLO faltante: API availability, API latency p95

4. **Grafana** (Observability Portal)
   - Criticidad: ALTA - sin Grafana, no hay visibilidad
   - Developers ciegos sin dashboards/logs
   - SLO faltante: UI availability, dashboard load time

### 4. No Miden User Journeys

**Principio SRE**: Los usuarios no usan "componentes", ejecutan "journeys"

**Journeys cr√≠ticos del IDP sin cobertura**:

**Journey 1: "Crear nueva aplicaci√≥n"**
- Pasos: Backstage ‚Üí Template ‚Üí GitHub ‚Üí ArgoCD ‚Üí Kubernetes ‚Üí Healthy
- SLIs necesarios:
  - Backstage UI availability ‚ùå
  - Scaffolder success rate ‚ùå
  - App creation success end-to-end ‚ùå
  - Time to healthy app (latency) ‚ùå
- Situaci√≥n actual: Solo medimos ArgoCD sync (1 paso de 5)

**Journey 2: "Deploy cambios a producci√≥n"**
- Pasos: Git push ‚Üí ArgoCD detect ‚Üí Sync ‚Üí App healthy
- SLIs necesarios:
  - Drift detection time ‚ùå
  - Sync success ‚úì (tenemos)
  - App health post-sync ‚ùå
  - Time to deployment (latency) ‚ùå
- Situaci√≥n actual: Solo medimos sync success

**Journey 3: "Investigar problema via logs"**
- Pasos: Grafana ‚Üí Loki query ‚Üí Results
- SLIs necesarios:
  - Grafana UI availability ‚ùå
  - Loki query success ‚ùå
  - Loki query latency p95 ‚ùå
- Situaci√≥n actual: Solo medimos Loki ingest (write path), no read path

**Journey 4: "Ejecutar CI pipeline"**
- Pasos: Trigger workflow ‚Üí Execute ‚Üí SonarQube scan ‚Üí Success
- SLIs necesarios:
  - Workflow success rate ‚ùå (medimos controller crashes, no success)
  - SonarQube availability ‚ùå
  - Pipeline duration p95 ‚ùå
- Situaci√≥n actual: Solo medimos que controller no crashee

## Problemas T√©cnicos (Implementaci√≥n)

### Estado Actual de 6 SLOs Implementados

| SLO | Estado | Problema T√©cnico | Fix Requerido |
|-----|--------|------------------|---------------|
| ExternalSecrets (97%) | üî¥ ROTO | Label `result` no existe en `externalsecret_sync_calls_total` | Usar `externalsecret_sync_calls_error / externalsecret_sync_calls_total` |
| Vault API (97%) | üî¥ ROTO | Label `status` no existe en `vault_core_handle_request_count` | Investigar m√©tricas alternativas o implement relabeling |
| Gateway API (98%) | üü° PARCIAL | Funciona pero solo mide errors, no latency | Agregar SLO de latency p95 con `envoy_cluster_upstream_rq_time` |
| Loki Ingest (95%) | üü° PARCIAL | Funciona pero solo mide write path, no read | Agregar query latency con `loki_request_duration_seconds{route="query_range"}` |
| ArgoCD Sync (95%) | üü° PARCIAL | Sync exitoso ‚â† app healthy | Agregar SLO de app health con `argocd_app_info{health_status="Healthy"}` |
| Argo Workflows (99%) | üü° PARCIAL | Mide controller crashes, no workflow success | Reemplazar con workflow success rate |

**Resumen**: 2 rotos, 4 incompletos, 0 correctos

### M√©tricas Reales vs Esperadas

**ExternalSecrets - Problema del label `result`**:
```promql
# ‚ùå SLO actual (ROTO):
errors: externalsecret_sync_calls_total{result!~"success|succeeded"}
total:  externalsecret_sync_calls_total

# Problema: La m√©trica NO tiene label `result`
# M√©tricas reales en ESO v0.20.2:
externalsecret_sync_calls_total{name, namespace, ...}  # Contador total
externalsecret_sync_calls_error{name, namespace, ...}  # Contador errores

# ‚úÖ Fix correcto:
errors: externalsecret_sync_calls_error
total:  externalsecret_sync_calls_total
```

**Vault - Problema del label `status`**:
```promql
# ‚ùå SLO actual (ROTO):
errors: vault_core_handle_request_count{status="failure"}
total:  vault_core_handle_request_count

# Problema: La m√©trica NO tiene label `status` en Vault 1.20.4
# Alternativas a investigar:
# 1. vault_core_handle_request_duration_seconds (tiene buckets, inferir errors por timeout?)
# 2. Relabeling desde otras m√©tricas de Vault
# 3. Medir Vault‚ÜíESO integration en vez de Vault internal
```

## Stack de Observabilidad: NO Aprovechado

### Recursos Disponibles Pero Sin Usar

**1. Loki - Solo 1 SLO SOBRE Loki, 0 SLIs DESDE Loki**

Tenemos Loki ingesting logs de todo el cluster, pero NO lo usamos para SLIs.

**Oportunidades con Log-based SLIs**:

```logql
# Application Error Rate (log-based)
SLI: Rate de log lines con level=ERROR o FATAL
Target: <1% de total logs
Query: sum(rate({app="myapp"} |= "ERROR" [5m])) / sum(rate({app="myapp"}[5m]))
Ventaja: Captura errores que no generan HTTP 5xx (logic errors, panics)

# Security Event Rate
SLI: Rate de policy violations en Kyverno logs
Target: <0.1%
Query: rate({app="kyverno"} |= "policy violation" [5m])
Ventaja: Security posture visibility

# Crash Loop Detection
SLI: Rate de CrashLoopBackOff en kubelet logs
Target: <1%
Query: Buscar "CrashLoopBackOff" patterns
Ventaja: Early detection de app instability
```

**2. Dashboards - Pyrra genera, pero 0 configurados**

Pyrra auto-genera dashboards de:
- Error Budget consumption
- Burn Rate alerting
- SLO status overview

**Estado actual**: 0 de 19 dashboards en Grafana son de SLO

**Fix**: Configurar Pyrra dashboard provisioning en Grafana sidecar

**3. Synthetic Monitoring - 0 probes activos**

**No tenemos black-box monitoring**. Todos los SLIs son white-box (m√©tricas internas).

**Qu√© falta**:
```yaml
# Ejemplo: Health Check Prober (usando blackbox-exporter)
- Probe HTTP GET a endpoints cr√≠ticos cada 1min
- Simula usuario real
- Genera: probe_success, probe_duration_seconds
- Detecta: Fallas que m√©tricas internas no ven (DNS, TLS, routing)

# Ejemplo: User Journey Prober
- Probe que ejecuta journey "create app in Backstage"
- Valida integraci√≥n end-to-end
- Detecta: Breaking changes en APIs, integration failures
```

**4. Tracing - No existe (Tempo no deployed)**

Sin tracing, no podemos medir latency end-to-end de journeys.

**Impacto**: No sabemos d√≥nde est√° el bottleneck en "git push ‚Üí pod healthy"

## SLOs Correctos Recomendados

### TIER 1 - Platform Core (Cr√≠tico)

**1. Kubernetes API Server Availability**
```yaml
target: "99.9"  # 99.95% en prod
window: 6h
indicator:
  ratio:
    errors:
      metric: apiserver_request_total{code=~"5.."}
    total:
      metric: apiserver_request_total
```
**Justificaci√≥n**: Sin K8s API, absolutamente NADA funciona. Es el componente m√°s cr√≠tico.

**2. Kubernetes API Server Latency**
```yaml
target: "95"  # 95% de requests <500ms
window: 6h
indicator:
  latency:
    total:
      metric: apiserver_request_duration_seconds_bucket
    threshold: 0.5  # 500ms
    grouping: [verb, resource]
```
**Justificaci√≥n**: API lenta = cluster inutilizable (kubectl timeouts, reconciliation loops lentos)

**3. ArgoCD Application Health** (NUEVO, complementa sync)
```yaml
target: "99.0"
window: 6h
indicator:
  ratio:
    errors:
      metric: argocd_app_info{health_status!="Healthy"}
    total:
      metric: argocd_app_info
    grouping: [name, project]
```
**Justificaci√≥n**: Sync exitoso ‚â† app funcionando. Esto mide la experiencia real.

**4. Secret Availability End-to-End** (REEMPLAZO de ESO + Vault)
```yaml
target: "99.0"
window: 6h
indicator:
  ratio:
    errors:
      metric: externalsecret_status_condition{type="Ready",status!="True"}
    total:
      metric: externalsecret_status_condition{type="Ready"}
    grouping: [name, namespace]
```
**Justificaci√≥n**: Mide Vault‚ÜíESO‚ÜíSecret‚ÜíPod completo, no solo sync del operador.

**5. Cert-Manager Certificate Readiness** (NUEVO)
```yaml
target: "100"  # 0 tolerance para certs expirados
window: 6h
indicator:
  ratio:
    errors:
      metric: certmanager_certificate_ready_status{condition!="True"}
    total:
      metric: certmanager_certificate_ready_status
    grouping: [name, namespace]
```
**Justificaci√≥n**: Certificado expirado = servicio inaccesible. 100% target es apropiado.

**6. Gateway Availability** (YA EXISTE, mantener)
```yaml
# Actual est√° OK
target: "98.0"
window: 6h
indicator:
  ratio:
    errors:
      metric: envoy_cluster_upstream_rq{envoy_response_code=~"5.."}
    total:
      metric: envoy_cluster_upstream_rq
    grouping: [envoy_cluster_name]
```

**7. Gateway Latency p95** (NUEVO)
```yaml
target: "95"  # 95% requests <200ms
window: 6h
indicator:
  latency:
    total:
      metric: envoy_cluster_upstream_rq_time_bucket
    threshold: 0.2  # 200ms
    grouping: [envoy_cluster_name]
```
**Justificaci√≥n**: Complementa availability. Portal lento = mala experiencia aunque "funcione".

### TIER 2 - Developer Experience

**8. Backstage UI Availability** (NUEVO)
```yaml
target: "99.0"
window: 6h
indicator:
  ratio:
    # Requiere instrumentaci√≥n custom o synthetic probe
    # Opci√≥n 1: Instrumentar Backstage con prometheus middleware
    # Opci√≥n 2: Blackbox probe con blackbox-exporter
```
**Justificaci√≥n**: Sin Backstage UI, developers no pueden self-service.

**9. Backstage Scaffolder Success Rate** (NUEVO)
```yaml
target: "95.0"
window: 6h
indicator:
  ratio:
    # Requiere instrumentaci√≥n custom en Backstage
    # M√©trica: backstage_scaffolder_tasks_total{status="completed|failed"}
```
**Justificaci√≥n**: Mide success rate de app creation, el journey m√°s cr√≠tico del IDP.

**10. Argo Workflows Success Rate** (REEMPLAZO del actual)
```yaml
target: "95.0"
window: 6h
indicator:
  ratio:
    errors:
      metric: argo_workflows_count{status!~"Succeeded|Skipped"}
    total:
      metric: argo_workflows_count
```
**Justificaci√≥n**: Mide CI/CD pipeline success, no solo si controller crashea.

**11. Grafana UI Availability** (NUEVO)
```yaml
target: "99.0"
window: 6h
indicator:
  ratio:
    errors:
      metric: grafana_http_request_total{status_code=~"5.."}
    total:
      metric: grafana_http_request_total
```
**Justificaci√≥n**: Sin Grafana, developers ciegos. Critical para observability.

**12. Loki Query Latency** (NUEVO, complementa ingest)
```yaml
target: "95"  # 95% queries <5s
window: 6h
indicator:
  latency:
    total:
      metric: loki_request_duration_seconds_bucket{route="query_range"}
    threshold: 5.0  # 5 seconds
```
**Justificaci√≥n**: Ingest OK pero query lenta = logs inutilizables para troubleshooting.

**13. Loki Ingest Availability** (YA EXISTE, mantener)
```yaml
# Actual est√° OK conceptualmente
target: "95.0"
window: 6h
indicator:
  ratio:
    errors:
      metric: loki_request_duration_seconds_count{status_code=~"5..",route=~"(?i).*(push|ingest).*"}
    total:
      metric: loki_request_duration_seconds_count{route=~"(?i).*(push|ingest).*"}
```

### TIER 3 - Advanced (Opcional v2)

**14. Journey: "Time to Production" (Composite SLO)**
```yaml
# Requiere: Tracing con Tempo o synthetic monitoring
# Mide: Tiempo desde git push hasta pod healthy
target: "95"  # 95% deploys <5min
window: 6h
indicator:
  latency:
    # Trace span: git_push ‚Üí argocd_detect ‚Üí sync ‚Üí health_check
    threshold: 300  # 5 minutes
```

**15. Log-based Error Rate**
```yaml
# Requiere: Configurar recording rules desde Loki
target: "99.0"
window: 6h
indicator:
  ratio:
    # LogQL convertido a metric via recording rule
    errors:
      metric: log_messages_total{level=~"ERROR|FATAL"}
    total:
      metric: log_messages_total
```

## Plan de Acci√≥n Priorizado

### FASE 1 - Quick Wins (Cr√≠tico, 1-2 d√≠as)

**Objetivo**: Arreglar SLOs rotos y agregar componentes cr√≠ticos faltantes

**Tareas**:
1. ‚úÖ **Fix ExternalSecrets SLO**
   - Reemplazar metric con `externalsecret_sync_calls_error / externalsecret_sync_calls_total`
   - Cambiar a medir end-to-end readiness: `externalsecret_status_condition`
   - Complejidad: BAJA (edit YAML)

2. ‚úÖ **Fix Vault SLO**
   - Opci√≥n A: Investigar m√©tricas alternativas de Vault
   - Opci√≥n B: Reemplazar con Vault‚ÜíESO integration metric
   - Opci√≥n C: Remover y depender de ESO end-to-end SLO
   - Complejidad: MEDIA (requiere investigaci√≥n)

3. üÜï **Add K8s API Server SLO (Availability + Latency)**
   - Crear 2 SLOs: availability y latency p95
   - M√©trica: `apiserver_request_total`, `apiserver_request_duration_seconds`
   - Complejidad: BAJA (m√©trica existe, copy template)

4. üÜï **Add Cert-Manager SLO**
   - M√©trica: `certmanager_certificate_ready_status`
   - Target: 100% (zero tolerance)
   - Complejidad: BAJA

5. üÜï **Add Grafana UI SLO**
   - M√©trica: `grafana_http_request_total` con status_code
   - Complejidad: BAJA

6. üîß **Update ArgoCD SLO - Add App Health**
   - Mantener sync SLO existente
   - Agregar nuevo SLO de app health
   - M√©trica: `argocd_app_info{health_status}`
   - Complejidad: BAJA

7. üìä **Configurar Pyrra Dashboards**
   - Habilitar dashboard auto-provisioning en Grafana
   - Verificar aparezcan en Grafana UI
   - Complejidad: MEDIA (requiere entender Pyrra dashboard config)

**Entregables Fase 1**:
- 6 SLOs funcionando correctamente (2 fixes + 4 nuevos)
- Dashboards de Error Budget visibles
- Componentes cr√≠ticos cubiertos (K8s, Cert-Manager, Grafana)

### FASE 2 - Latency SLIs (Importante, 2-3 d√≠as)

**Objetivo**: Agregar SLIs de latency (completar 4 Golden Signals)

**Tareas**:
8. üÜï **Gateway Latency p95 SLO**
   - M√©trica: `envoy_cluster_upstream_rq_time` (histogram)
   - Target: 95% <200ms
   - Complejidad: MEDIA (configurar latency SLO en Pyrra)

9. üÜï **Loki Query Latency p95 SLO**
   - M√©trica: `loki_request_duration_seconds{route="query_range"}`
   - Target: 95% <5s
   - Complejidad: MEDIA

10. üîß **Update Argo Workflows - Success Rate**
    - Reemplazar controller availability con workflow success rate
    - M√©trica: `argo_workflows_count{status}`
    - Complejidad: MEDIA (verificar m√©trica existe)

**Entregables Fase 2**:
- 3 SLOs de latency agregados
- Todas las Golden Signals cubiertas (Latency + Errors)
- Workflows miden success, no crashes

### FASE 3 - Advanced (Opcional v2, semanas)

**Objetivo**: Aprovechar stack completo, SLOs avanzados

**Tareas**:
11. üöÄ **Backstage Instrumentaci√≥n Custom**
    - Agregar prometheus middleware a Backstage
    - Exponer m√©tricas: HTTP requests, scaffolder tasks
    - Crear 2 SLOs: UI availability, scaffolder success
    - Complejidad: ALTA (requiere c√≥digo custom)

12. üöÄ **Log-based SLIs con Loki**
    - Configurar recording rules: Loki ‚Üí Prometheus metrics
    - Crear SLOs de error rate basados en logs
    - Complejidad: ALTA (LogQL + recording rules)

13. üöÄ **Synthetic Monitoring**
    - Deploy blackbox-exporter
    - Configurar probes a endpoints cr√≠ticos
    - Crear SLOs basados en probes
    - Complejidad: ALTA (nueva infra)

14. üöÄ **Journey-based Composite SLOs**
    - Opci√≥n A: Deploy Tempo, instrumentar con tracing
    - Opci√≥n B: Synthetic probes ejecutando journeys
    - Crear SLO "Time to Production"
    - Complejidad: MUY ALTA (tracing full stack)

15. üöÄ **SonarQube SLO**
    - Instrumentar SonarQube con metrics
    - Crear SLO de availability
    - Complejidad: MEDIA

## Matriz Comparativa Final

| Componente | SLO Actual | ‚ùå Problema | ‚úÖ SLO Correcto | Fase |
|------------|-----------|-----------|--------------|------|
| **ESO** | Sync 97% | Mide operador, metric rota | Secret avail e2e 99% | 1 |
| **Vault** | API 97% | Label no existe | Vault‚ÜíESO integration 99% (o remover) | 1 |
| **Gateway** | Avail 98% | Solo errors | Avail 98% + p95 latency <200ms | 1+2 |
| **ArgoCD** | Sync 95% | Sync ‚â† healthy | Sync 95% + App Health 99% | 1 |
| **Workflows** | Controller 99% | Crashes, no success | Workflow success 95% + duration p95 | 2 |
| **Loki** | Ingest 95% | Solo write path | Ingest 95% + Query p95 <5s | 2 |
| **K8s API** | ‚ùå NONE | Core sin SLO | Avail 99.9% + p95 <500ms | 1 |
| **Cert-Mgr** | ‚ùå NONE | Cr√≠tico sin SLO | Cert ready 100% | 1 |
| **Grafana** | ‚ùå NONE | Portal sin SLO | UI avail 99% | 1 |
| **Backstage** | ‚ùå NONE | Portal sin SLO | UI 99% + Scaffolder 95% | 3 |
| **SonarQube** | ‚ùå NONE | CI tool sin SLO | Avail 99% | 3 |

**Resumen Num√©rico**:
- Fase 1: 7 SLOs ‚Üí 6 fixes/adds (cr√≠tico)
- Fase 2: +3 SLOs latency (importante)
- Fase 3: +5 SLOs advanced (opcional)
- **Total propuesto**: 15 SLOs vs 6 actuales

## Trade-offs: Demo vs Producci√≥n

### Aceptable para Demo (Recursos Limitados)

‚úÖ **Windows 6h vs 28d**: Permite ver burn rates r√°pido, apropiado para demos
‚úÖ **Targets relajados**: 95-99% vs 99.9-99.99% en prod
‚úÖ **No synthetic monitoring**: Requiere infra adicional (blackbox-exporter)
‚úÖ **No tracing/Tempo**: Consume recursos significativos
‚úÖ **L√≠mites de cardinalidad**: Protege recursos (dropear labels uid, container_id)

### NO Aceptable (Incluso para Demo)

‚ùå **SLOs t√©cnicamente rotos**: Deben funcionar, no hay excusa
‚ùå **Componentes cr√≠ticos sin SLO**: K8s API, Cert-Manager son CORE
‚ùå **Solo availability, no latency**: Latency es fundamental incluso en demo
‚ùå **No medir user experience**: Demo debe mostrar CONCEPTO correcto
‚ùå **No dashboards**: Pyrra genera gratis, configurar es trivial

### Filosof√≠a Correcta para Demo

**El demo debe**:
- ‚úÖ Demostrar PRINCIPIOS SRE correctos
- ‚úÖ Ser EDUCATIVO sobre mejores pr√°cticas
- ‚úÖ Implementar SLOs conceptualmente CORRECTOS
- ‚úÖ Con targets RELAJADOS apropiados para demo
- ‚úÖ Mostrar qu√© hacer EN PRODUCCI√ìN (aunque simplifiquemos implementaci√≥n)

**El demo NO debe**:
- ‚ùå Tener SLOs rotos que no funcionan
- ‚ùå Mostrar anti-patterns como "solo medir availability"
- ‚ùå Omitir componentes cr√≠ticos
- ‚ùå Sacrificar CORRECCI√ìN por simplicidad

## Metodolog√≠a SRE Correcta (Si Empez√°ramos de Cero)

### 1. Identificar User Journeys (No Componentes)

**Pregunta clave**: "¬øQu√© HACE un developer en este IDP?"

Respuesta:
- Journey 1: Crear nueva app
- Journey 2: Deploy cambios
- Journey 3: Ver logs/m√©tricas
- Journey 4: Ejecutar CI pipeline

**NO empezar con**: "Qu√© componentes tenemos?"

### 2. Definir SLIs desde Perspectiva Usuario

**Pregunta clave**: "¬øCu√°ndo considera el usuario que el servicio funciona bien?"

**Ejemplos**:
- ‚úÖ "Puedo crear apps sin errores"
- ‚úÖ "Mis deploys completan en <5min"
- ‚úÖ "Puedo ver logs de mi app cuando falla"
- ‚ùå "El controller de ArgoCD no crashea" (perspectiva operador)

### 3. Medir 4 Golden Signals (No Solo Availability)

**Para cada journey, medir**:
- **Latency**: ¬øCu√°nto tarda? (p50, p95, p99)
- **Errors**: ¬øCu√°ntas fallas? (error rate %)
- **Traffic**: ¬øCu√°nto uso? (requests/sec)
- **Saturation**: ¬øRecursos saturados? (CPU, mem, queue depth)

### 4. Priorizar por Criticidad del Journey

**Tier 1**: Platform core (K8s API, secrets, certs)
**Tier 2**: Developer experience (Backstage, GitOps, CI/CD)
**Tier 3**: Observability sobre observability (Prometheus, Loki)

**NO**: Todos los componentes con mismo priority

### 5. White-box + Black-box

**White-box**: M√©tricas internas (Prometheus)
**Black-box**: Probes externos (synthetic monitoring)

**Ambos necesarios**: White-box diagnostica, black-box detecta

### 6. Dashboards desde D√≠a 1

**Error Budget debe ser VISIBLE**:
- Developers ven cu√°nto budget queda
- Alerts cuando burn rate alto
- Decisiones basadas en datos (deploy vs estabilizar)

## Referencias y Fuentes

**Google SRE Books**:
- Site Reliability Engineering (Cap√≠tulo 4: Service Level Objectives)
- The Site Reliability Workbook (Cap√≠tulo 2: Implementing SLOs)

**Principios SRE aplicados**:
- User-centric SLIs (no component-centric)
- 4 Golden Signals (Latency, Traffic, Errors, Saturation)
- Error Budget policy
- Burn rate alerting

**Herramientas del stack**:
- Pyrra v0.7+ (SLO engine)
- Prometheus (metrics collection)
- Loki (log aggregation)
- Grafana (visualization)
- Alertmanager (alerting)
- Argo Events (auto-remediation)

---

**√öltima actualizaci√≥n**: 2025-12-19
**Estado**: An√°lisis completado, pendiente implementaci√≥n Fase 1

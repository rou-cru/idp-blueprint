{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IDP-blueprint: Internal Developer Platform","text":"<p>An Internal Developer Platform (IDP) Blueprint - Deploy a complete platform engineering stack (GitOps, Observability, Security &amp; Policy Enforcement) with a single command on your laptop!</p>"},{"location":"#what-is-this","title":"What is this?","text":"<p>An opinionated, resource-optimized IDP running on K3d that demonstrates modern Platform Engineering practices:</p>   | **GitOps-first** | **Policy-as-Code** | **Observability** | **Security Scanning** | |:---:|:---:|:---:|:---:| | ArgoCD | Kyverno | Prometheus + Grafana + Loki | Trivy | | **CI/CD** | **eBPF Service Mesh** | **Secrets Management** | **Certificate Management** | | Argo Workflows + SonarQube | Cilium | Vault + External Secrets | Cert-Manager |   <p>Key Features:</p> <ul> <li>Single command deployment: Deploy the entire platform with <code>task deploy</code></li> <li>Pre-configured environment: VS Code Dev Containers with all tools ready</li> <li>Modular architecture: Components organized in logical stacks</li> <li>Resource optimized: Designed for local development with ~3.5-8.9 cores</li> <li>Production ready patterns: Implements GitOps, policy-as-code, and security-first approach</li> </ul>"},{"location":"#why-this-matters-real-world-value","title":"Why This Matters: Real-World Value","text":""},{"location":"#for-platform-engineers","title":"For Platform Engineers","text":"<p>Prototype and validate infrastructure changes risk-free. Test Kyverno policies, Vault configs, or GitOps workflows locally before proposing to production. Experiment with eBPF networking, policy enforcement, or observability patterns without waiting for cloud resources or breaking shared environments.</p>"},{"location":"#for-devopssre-teams","title":"For DevOps/SRE Teams","text":"<p>Learning lab for modern cloud-native tools. Understand how ArgoCD ApplicationSets work, debug Cilium network policies, or explore Prometheus metrics\u2014all in a realistic multi-node cluster on your laptop. Perfect for training new team members or evaluating tools before adoption.</p>"},{"location":"#for-security-engineers","title":"For Security Engineers","text":"<p>Validate compliance controls in minutes. Draft security policies as code (Kyverno), test them against real workloads, and generate compliance reports (Policy Reporter) without provisioning infrastructure. Demonstrate \"block root containers\" or \"enforce image signing\" policies with concrete evidence.</p> <p>From idea to validated prototype in minutes, now you only need one command: <code>task deploy</code>. Whether you're evaluating new tools, preparing demonstrations or conferences, writing a technical article, or onboarding a junior engineer, you can have your own IDP wherever and whenever you need it.</p>"},{"location":"#deployment-architecture","title":"Deployment Architecture","text":"<pre><code>---\nconfig:\n  layout: dagre\n  look: neo\n---\nflowchart LR\n subgraph Bootstrap[\"Bootstrap\"]\n        StaticPhase[\"Static Infrastructure Phase\"]\n        Cilium[\"Cilium CNI\"]\n        CertMgr[\"Cert-Manager\"]\n        Vault[\"Vault\"]\n        ESO[\"External Secrets Operator\"]\n        ArgoCD[\"ArgoCD\"]\n  end\n subgraph GitOps[\"GitOps\"]\n        GitOpsPhase[\"GitOpsPhase\"]\n        PolicyStack[\"Policy Stack\"]\n        Kyverno[\"Kyverno\"]\n        Reporter[\"Policy Reporter\"]\n        Obs[\"Observability Stack\"]\n        CICD[\"CI/CD Stack\"]\n        Sec[\"Security Stack\"]\n        Prom[\"Prometheus\"]\n        Loki[\"Loki\"]\n        Grafana[\"Grafana\"]\n        Fluent[\"Fluent-bit\"]\n        Workflows[\"Argo Workflows\"]\n        Sonarqube[\"Sonarqube\"]\n        Trivy[\"Trivy\"]\n  end\n    PolicyStack -.-&gt; Kyverno &amp; Reporter\n    Obs -.-&gt; Prom &amp; Grafana &amp; Fluent &amp; Loki\n    Start[\"task deploy\"] ==&gt; K3d[\"Create K3d Cluster\"]\n    K3d ==&gt; NS[\"Create Bootstrap Namespaces\"]\n    NS ==&gt; StaticPhase\n    StaticPhase -.-&gt; Cilium &amp; ESO &amp; ArgoCD &amp; CertMgr &amp; Vault\n    GitOps --&gt; PolicyStack\n    CICD -.-&gt; Workflows &amp; Sonarqube\n    Sec -.-&gt; Trivy\n    ArgoCD ==&gt; GitOpsPhase\n    GitOpsPhase -.-&gt; PolicyStack &amp; Obs &amp; CICD &amp; Sec\n    StaticPhase@{ shape: div-proc}\n    Cilium@{ shape: h-cyl}\n    CertMgr@{ shape: h-cyl}\n    Vault@{ shape: h-cyl}\n    ESO@{ shape: h-cyl}\n    ArgoCD@{ shape: h-cyl}\n    GitOpsPhase@{ shape: div-proc}\n    PolicyStack@{ shape: procs}\n    Kyverno@{ shape: h-cyl}\n    Reporter@{ shape: h-cyl}\n    Obs@{ shape: procs}\n    CICD@{ shape: procs}\n    Sec@{ shape: procs}\n    Prom@{ shape: h-cyl}\n    Loki@{ shape: h-cyl}\n    Grafana@{ shape: h-cyl}\n    Fluent@{ shape: h-cyl}\n    Workflows@{ shape: h-cyl}\n    Sonarqube@{ shape: h-cyl}\n    Trivy@{ shape: h-cyl}\n    Start@{ shape: braces}\n    K3d@{ shape: disk}\n    NS@{ shape: card}</code></pre> <p>Deployment time: ~5-10 minutes | Command: <code>task deploy</code></p>"},{"location":"#node-architecture","title":"Node Architecture","text":"<pre><code>---\nconfig:\n  look: handDrawn\n  theme: redux\n  layout: elk\n---\nflowchart TB\n subgraph Node1[\"Control Plane Node\"]\n        K3sAPI[\"K3s API Server\"]\n        K3sCtrl[\"K3s Controller Manager\"]\n        K3sSched[\"K3s Scheduler\"]\n  end\n subgraph Node2[\"Static Infrastructure Node\"]\n        CertMgr[\"Cert-Manager\"]\n        Vault[\"Vault\"]\n        ESO[\"External Secrets\"]\n        ArgoCD[\"ArgoCD\"]\n  end\n subgraph Node3[\"GitOps Workloads Node\"]\n        Kyverno[\"Kyverno\"]\n        PolicyReporter[\"Policy Reporter\"]\n        Prometheus[\"Prometheus\"]\n        AlertMgr[\"Alertmanager\"]\n        KSM[\"Kube State Metrics\"]\n        Grafana[\"Grafana\"]\n        Loki[\"Loki\"]\n        Workflows[\"Argo Workflows\"]\n        Sonar[\"SonarQube\"]\n        Trivy[\"Trivy\"]\n  end\n subgraph K3s[\"k3d Cluster\"]\n        Cilium[\"Cilium CNI\"]\n        FluentBit[\"Fluent-bit\"]\n        NodeExporter[\"Node Exporter\"]\n        Node1\n        Node2\n        Node3\n  end\n    Git[(\"Git Repository\")] -- policies as code --&gt; ArgoCD\n    Git -- ApplicationSets --&gt; ArgoCD\n    ESO == Deploy Secrets request by ==&gt; ArgoCD\n    Vault == provides secrets === ESO\n    Workflows == CI/CD Execution ==&gt; ArgoCD\n    Sonar == Quality Analysis === Workflows\n    Trivy == Security Scans === ArgoCD\n    ArgoCD == GitOps Policy ==&gt; Kyverno\n    PolicyReporter === Kyverno\n    PolicyReporter == Policy Metrics ==&gt; Grafana\n    Node1 -. scrapes Node metrics .-&gt; NodeExporter\n    Node2 -. scrapes Node metrics .-&gt; NodeExporter\n    Node3 -. scrapes Node metrics .-&gt; NodeExporter\n    NodeExporter == Save Metrics ==&gt; Prometheus\n    KSM == scrapes workload metrics ==&gt; Prometheus\n    Prometheus == sends alerts ==&gt; AlertMgr\n    Prometheus == Metrics ==&gt; Grafana\n    Node1 -. scrapes Pods Logs .-&gt; FluentBit\n    Node2 -. scrapes Pods Logs .-&gt; FluentBit\n    Node3 -. scrapes Pods Logs .-&gt; FluentBit\n    FluentBit == Save Logs ==&gt; Loki\n    Loki == Logs ==&gt; Grafana\n    style CertMgr stroke:#AA00FF\n    style Vault stroke:#AA00FF\n    style ESO stroke:#AA00FF\n    style ArgoCD stroke:#00C853\n    style Kyverno stroke:#FF6D00\n    style PolicyReporter stroke:#FF6D00\n    style Prometheus stroke:#2962FF\n    style AlertMgr stroke:#FFD600\n    style KSM stroke:#2962FF\n    style Loki stroke:#D50000\n    style Workflows stroke:#00C853\n    style Sonar stroke:#FFD600\n    style Trivy stroke:#AA00FF\n    style Cilium stroke:#000000,fill:#757575\n    style FluentBit stroke:#D50000\n    style NodeExporter stroke:#2962FF\n    style Node1 fill:#757575,stroke:#000000\n    style Node2 stroke:#000000,fill:#757575\n    style Node3 fill:#757575,stroke:#000000\n    style Git stroke:#00C853\n    style K3s stroke:#000000,fill:#424242\n    linkStyle 0 stroke:#00C853,stroke-width:2px,fill:none\n    linkStyle 1 stroke:#00C853,stroke-width:2px,fill:none\n    linkStyle 2 stroke:#00C853,fill:none\n    linkStyle 3 stroke:#AA00FF,fill:none\n    linkStyle 4 stroke:#AA00FF,fill:none\n    linkStyle 5 stroke:#FFD600,fill:none\n    linkStyle 6 stroke:#AA00FF,fill:none\n    linkStyle 7 stroke:#FB8C00,stroke-width:2px,fill:none\n    linkStyle 8 stroke:#FB8C00,stroke-width:2px,fill:none\n    linkStyle 9 stroke:#FB8C00,stroke-width:2px,fill:none\n    linkStyle 10 stroke:#2962FF,stroke-width:2px,fill:none\n    linkStyle 11 stroke:#2962FF,stroke-width:2px,fill:none\n    linkStyle 12 stroke:#2962FF,stroke-width:2px,fill:none\n    linkStyle 13 stroke:#2962FF,stroke-width:2px,fill:none\n    linkStyle 14 stroke:#2962FF,fill:none\n    linkStyle 15 stroke:#FFD600,fill:none\n    linkStyle 16 stroke:#2962FF,fill:none\n    linkStyle 17 stroke:#D50000,fill:none\n    linkStyle 18 stroke:#D50000,fill:none\n    linkStyle 19 stroke:#D50000,fill:none\n    linkStyle 20 stroke:#D50000,fill:none\n    linkStyle 21 stroke:#D50000,fill:none</code></pre> <p>Why this architecture?</p> <ul> <li>Node separation ensures resource isolation and easier troubleshooting</li> <li>Static infrastructure (Node 2) deployed via Helm for bootstrap reliability</li> <li>these components don't change frequently</li> <li>GitOps workloads (Node 3) managed by ArgoCD for declarative operations and   easy rollbacks - everything defined as code in Git</li> <li>DaemonSets run on all nodes (Cilium for networking, Fluent-bit for log   collection, Node Exporter for metrics)</li> <li>Policies-first approach ensures all workloads are compliant from   deployment - policies themselves are GitOps managed</li> <li>Vault as source of truth for secrets, synced to Kubernetes via External   Secrets Operator</li> </ul> <p>For detailed architecture documentation, see Architecture Overview</p>"},{"location":"#resource-requirements","title":"Resource Requirements","text":"<p>Optimized for local development environments:</p> Resource Requested Limited CPU ~3.5 cores ~8.9 cores Memory ~5.4 GiB ~11 GiB <p>Recommendation:</p> <ul> <li>Minimum: 4 CPU cores, 8GB RAM</li> <li>Comfortable: 6 CPU cores, 12GB RAM</li> <li>Storage: ~20GB available</li> </ul> <p>Note: These numbers exclude k3d control plane and OS overhead. Real-world usage may vary based on workload.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop</li> <li>Visual Studio Code with Dev Containers   extension</li> <li>Git</li> <li>Docker Hub login required (<code>docker login</code>) to avoid severe rate limiting</li> </ul> <p>Important: This project uses VS Code Dev Containers to provide a pre-configured environment with all required tools (kubectl, helm, k3d, task, etc.). Running <code>task deploy</code> outside the Dev Container will fail unless you manually install all dependencies.</p>"},{"location":"#deploy-the-platform","title":"Deploy the Platform","text":"<pre><code># Clone and open in VS Code\ngit clone https://github.com/rou-cru/idp-blueprint &amp;&amp; cd idp-blueprint\ncode .\n\n# When prompted, click \"Reopen in Container\"\n# Once inside the Dev Container, deploy everything:\ntask deploy\n\n# Deployment takes ~5-10 minutes\n\n# For slow connections, increase timeouts:\n# task deploy KUBECTL_TIMEOUT=600s\n</code></pre> <p>That's it! Your IDP is ready.</p>"},{"location":"#whats-included","title":"What's Included","text":""},{"location":"#core-infrastructure-it","title":"Core Infrastructure (IT/)","text":"<p>Deployed via Helm on Node 2 (static workloads):</p> <ul> <li>Cilium - eBPF-based CNI, network policies, and LoadBalancer with L2   announcements for LAN service exposure</li> <li>Cert-Manager - TLS certificate automation</li> <li>Vault - Secret storage backend</li> <li>External Secrets - Vault-to-Kubernetes secret sync</li> <li>ArgoCD - GitOps engine</li> </ul>"},{"location":"#policy-layer-policies","title":"Policy Layer (Policies/)","text":"<p>First GitOps deployment to ensure compliance from the start - policies as code:</p> <ul> <li>Kyverno - Policy enforcement engine (GitOps managed)</li> <li>Policy Reporter - Compliance monitoring dashboard (GitOps managed)</li> <li>Pre-configured policies: Namespace labels, component labels, best   practices (all in Git)</li> </ul>"},{"location":"#application-stacks-k8s","title":"Application Stacks (K8s/)","text":"<p>Deployed via ArgoCD ApplicationSets on Node 3:</p> <ul> <li>Observability: Prometheus, Grafana, Loki, Fluent-bit</li> <li>CI/CD: Argo Workflows, SonarQube</li> <li>Security: Trivy Operator</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Here's how you can help:</p> <ul> <li>Report bugs via Issues</li> <li>Suggest features or improvements</li> <li>Improve documentation</li> <li>Submit pull requests for:</li> <li>Additional Kyverno policies</li> <li>Resource optimization improvements</li> <li>Integration with other tools</li> <li>Translations</li> </ul> <p>See Contributing Guide for detailed guidelines.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Add Backstage as developer portal</li> <li> Crossplane for infrastructure as code</li> <li> OpenTelemetry Collector + Tempo for distributed tracing</li> <li> Cost optimization dashboard</li> <li> Implement NetworkPolicies for enhanced security and namespace isolation</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p> <p>Feel free to use this as a reference or foundation for your own IDP implementations.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"This project integrates and builds upon excellent open-source tools from the Cloud Native ecosystem:  [![ArgoCD](https://img.shields.io/badge/ArgoCD-2196F3?logo=argoproj&amp;logoColor=white)](https://argo-cd.readthedocs.io/) [![Cilium](https://img.shields.io/badge/Cilium-29BEB0?logo=cilium&amp;logoColor=white)](https://cilium.io/) [![Vault](https://img.shields.io/badge/Vault-2B2B2B?logo=hashicorp&amp;logoColor=white)](https://www.vaultproject.io/) [![Prometheus](https://img.shields.io/badge/Prometheus-E6522C?logo=prometheus&amp;logoColor=white)](https://prometheus.io/) [![Grafana](https://img.shields.io/badge/Grafana-F46800?logo=grafana&amp;logoColor=white)](https://grafana.com/)"},{"location":"#host-setup","title":"Host Setup","text":""},{"location":"#macos-linux-users","title":"macOS / Linux Users","text":"<p>Choose one of the following methods to install everything you need.</p> Option 1: Using Homebrew (Recommended)  Open your terminal and run this command block to install Git, Docker, VS Code, and the required extension:  <pre><code>brew install git &amp;&amp; \\\nbrew install --cask visual-studio-code docker &amp;&amp; \\\ncode --install-extension ms-vscode-remote.remote-containers --force\n</code></pre> Option 2: Manual Installation  Install each of the following manually from their official sources:  - [Git](https://git-scm.com/downloads) - [Docker Desktop](https://www.docker.com/products/docker-desktop/) - [Visual Studio Code](https://code.visualstudio.com/) - [Dev Containers extension for VS   Code](vscode:extension/ms-vscode-remote.remote-containers)"},{"location":"#windows-users","title":"Windows Users","text":""},{"location":"#step-1-install-wsl","title":"Step 1: Install WSL","text":"<p>This is a mandatory first step. Open PowerShell as an Administrator and run this command. A PC reboot will likely be required.</p> <pre><code>wsl --install --distro Ubuntu\n</code></pre> <p>After completing the install, open \"Ubuntu\" from programs and setup a user in the console.</p>"},{"location":"#step-2-install-host-software","title":"Step 2: Install Host Software","text":"<p>Choose one of the following methods:</p> Option 1: Using Chocolatey (Recommended)  From an **Administrator PowerShell**, run this command block:  <pre><code>choco install git vscode docker-desktop -y\n\n. $profile\n\ncode --install-extension ms-vscode-remote.remote-containers\n</code></pre>  Now you can close the PowerShell console.   Option 2: Manual Installation  Install each of the following manually from their official sources:  - [Git](https://git-scm.com/downloads) - [Docker Desktop](https://www.docker.com/products/docker-desktop/) - [Visual Studio Code](https://code.visualstudio.com/) - [Dev Containers extension for VS   Code](vscode:extension/ms-vscode-remote.remote-containers)"},{"location":"#step-3-configure-docker","title":"Step 3: Configure Docker","text":"<p>Open Docker Desktop and go to <code>Settings &gt; Resources &gt; WSL Integration</code>, enable the integration for your <code>Ubuntu</code> distribution.</p>"},{"location":"#step-4-launch-the-project","title":"Step 4: Launch the Project","text":"<ol> <li>Open your <code>Ubuntu</code> terminal from the Start Menu.</li> <li>In the Ubuntu terminal, run:</li> </ol> <pre><code>git clone https://github.com/rou-cru/idp-blueprint\n\ncd idp-blueprint\ncode .\n</code></pre> <ol> <li>When VS Code opens, click \"Reopen in Container\".</li> </ol>"},{"location":"#how-the-dev-environment-works","title":"How the Dev Environment Works","text":"<p>This development environment is designed to be \"smart\". The environment performs an automatic initialization step:</p> <ol> <li>The <code>devbox.json</code> file contains a <code>\"shell\"</code> section with an <code>\"init_hook\"</code>.</li> <li>This hook is configured to run the <code>.devcontainer/init.sh</code> script every time    a new terminal is opened in VS Code.</li> <li>The <code>init.sh</code> script adds and updates all the Helm repositories that the    project needs (ArgoCD, Prometheus, Grafana, etc.).</li> </ol> <p>In short: Thanks to this mechanism, you never need to manage Helm repositories manually and can add other env setup steps without caring how the environment itself is generated. They will always be ready for you to deploy the project.</p>   **If you find this project useful, please consider starring it on GitHub!**  [![Stargazers repo roster for @rou-cru/idp-blueprint](https://reporoster.com/stars/rou-cru/idp-blueprint)](https://github.com/rou-cru/idp-blueprint/stargazers)  [Join our community](https://github.com/rou-cru/idp-blueprint/discussions) | [Report an issue](https://github.com/rou-cru/idp-blueprint/issues) | [Documentation](getting-started/overview.md)"},{"location":"architecture/applications/","title":"K8s Directory Architecture: GitOps with ApplicationSets","text":"<p>This document outlines the GitOps strategy for managing all applications and services (workloads) deployed on the cluster. This directory (<code>K8s/</code>) is the source of truth, managed exclusively by ArgoCD.</p>"},{"location":"architecture/applications/#core-pattern-app-of-appsets","title":"Core Pattern: App of AppSets","text":"<p>We employ a modular, scalable pattern where multiple <code>ApplicationSet</code> resources are used instead of a single monolithic one. This provides flexibility, clear ownership, and reduces the blast radius of any configuration errors.</p> <ul> <li>One <code>ApplicationSet</code> per Stack: Each primary directory within <code>K8s/</code> (e.g.,   <code>observability/</code>, <code>cicd/</code>) represents a \"stack\" of tools. Each stack contains its own   <code>applicationset-&lt;stack&gt;.yaml</code> file.</li> <li><code>ApplicationSet</code> Role: This file is responsible for discovering and managing all   applications within its own stack directory.</li> <li>Root <code>Application</code>: A root ArgoCD <code>Application</code> (managed outside this directory)   is responsible for deploying the <code>ApplicationSet</code> resources themselves.</li> </ul>"},{"location":"architecture/applications/#gitops-workflow-diagram","title":"GitOps Workflow Diagram","text":"<p>This diagram illustrates the entire flow, from the Git repository to the deployed applications in their respective namespaces.</p> <pre><code>graph LR\n    subgraph Git Repository (K8s/)\n        A(Root App) -- deploys --&gt; B(AppSet-CICD);\n        A -- deploys --&gt; C(AppSet-Observability);\n        A -- deploys --&gt; D(AppSet-Security);\n    end\n\n    subgraph ArgoCD\n        B -- discovers --&gt; E(argo-workflows/);\n        C -- discovers --&gt; F(loki/);\n        C -- discovers --&gt; G(prometheus/);\n    end\n\n    subgraph Kubernetes Cluster\n        E -- deploys to --&gt; H(Namespace: cicd);\n        F -- deploys to --&gt; I(Namespace: observability);\n        G -- deploys to --&gt; I;\n    end\n\n    style A fill:#d4a2e8\n    style B fill:#f9d4a8\n    style C fill:#f9d4a8\n    style D fill:#f9d4a8</code></pre>"},{"location":"architecture/applications/#bootstrap-and-standalone-applications","title":"Bootstrap and Standalone Applications","text":"<p>Certain foundational, cross-cutting components like the policy engine are deployed first, before the main application stacks. This ensures the cluster's \"rules of the road\" are active before other workloads are deployed.</p> <ul> <li>Policy Stack: The policy engine (Kyverno) and the policies themselves are managed   by a standalone ArgoCD <code>Application</code> defined in <code>Policies/app-kyverno.yaml</code>. This   application is deployed directly during the bootstrap phase (e.g., via   <code>Taskfile.yaml</code>) and points to the <code>Policies/</code> directory, which uses Kustomize to   orchestrate the deployment of the entire stack.</li> </ul> <p>This approach provides a secure bootstrap process at the cost of being a slight exception to the general \"App of AppSets\" pattern.</p>"},{"location":"architecture/applications/#directory-structure","title":"Directory Structure","text":"<p>The structure is designed to be semantic, self-documenting, and to directly reflect the namespace strategy.</p> <pre><code>graph TD\n    subgraph K8s Directory Structure\n        A(K8s/) --&gt; B(cicd/);\n        A --&gt; C(observability/);\n        A --&gt; D(security/);\n\n        B --&gt; B1(applicationset-cicd.yaml);\n        B --&gt; B2(namespace.yaml);\n        B --&gt; B3(argo-workflows/);\n        B3 --&gt; B3a(kustomization.yaml);\n\n        C --&gt; C1(applicationset-observability.yaml);\n        C --&gt; C2(namespace.yaml);\n        C --&gt; C3(loki/);\n        C3 --&gt; C3a(kustomization.yaml);\n    end</code></pre> <ul> <li><code>namespace.yaml</code>: Defines the Kubernetes <code>Namespace</code> for the entire stack and   contains the common labels and annotations that will be propagated by Kyverno, as per   <code>Policies/tag-policy.md</code>.</li> <li><code>applicationset-&lt;stack&gt;.yaml</code>: The <code>ApplicationSet</code> resource that manages all   applications within the stack.</li> <li><code>&lt;app-name&gt;/kustomization.yaml</code>: The Kustomize entrypoint for a specific   application. ArgoCD will target this file for deployment.</li> </ul>"},{"location":"architecture/applications/#application-manifests-kustomize","title":"Application Manifests &amp; Kustomize","text":"<p>To manage the complexity of modern applications, we use Kustomize as the standard for defining and composing application manifests. Our philosophy is based on resource composition over complex inheritance via <code>overlays</code>.</p> <p>Two primary patterns for Kustomize are established in this project.</p> <pre><code>graph TD\n    subgraph Pattern 1: Local Resource Aggregation\n        Kustomization_P1[\"kustomization.yaml\"] -- \"resources:\" --&gt; Resource1[\"resource-a.yaml\"];\n        Kustomization_P1 -- \"resources:\" --&gt; Resource2[\"resource-b.yaml\"];\n    end\n\n    subgraph Pattern 2: Helm Chart Orchestration\n        Kustomization_P2[\"kustomization.yaml\"] -- \"valuesFile:\" --&gt; Values[\"values.yaml\"];\n        Kustomization_P2 -- \"repo:\" --&gt; HelmRepo[\"Remote Helm Repo\"];\n    end\n\n    style Kustomization_P1 fill:#cde4ff\n    style Kustomization_P2 fill:#cde4ff\n    style HelmRepo fill:#d5f0d5</code></pre>"},{"location":"architecture/applications/#pattern-1-local-resource-aggregation","title":"Pattern 1: Local Resource Aggregation","text":"<p>This is the standard approach for in-house applications or for grouping a set of related Kubernetes manifests.</p> <ul> <li>When to Use It: For internally developed microservices, governance policies     (<code>ResourceQuota</code>, <code>LimitRange</code>), or any set of YAML manifests you manage directly.</li> <li>Structure:</li> <li>A directory is created for the component (e.g., <code>governance/</code>).</li> <li>YAML files (<code>limitrange.yaml</code>, <code>resourcequota.yaml</code>) are placed inside.</li> <li> <p>The <code>kustomization.yaml</code> file lists them in the <code>resources</code> section.</p> </li> <li> <p>Example (<code>K8s/cicd/governance/kustomization.yaml</code>):</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - limitrange.yaml\n  - resourcequota.yaml\n</code></pre> </li> </ul>"},{"location":"architecture/applications/#pattern-2-helm-chart-orchestration","title":"Pattern 2: Helm Chart Orchestration","text":"<p>This is the preferred, standard method for deploying third-party applications or any software available as a Helm chart. It allows us to version and manage the configuration of these tools declaratively.</p> <ul> <li>When to Use It: For tools like Argo Workflows, Loki, Prometheus, Trivy, etc.</li> <li>Structure:</li> <li>A directory is created for the application (e.g., <code>argo-workflows/</code>).</li> <li>A <code>kustomization.yaml</code> file defines the Helm chart in the <code>helmCharts</code> section.</li> <li> <p>A <code>values.yaml</code> file (e.g. <code>argo-workflows-values.yaml</code>) contains all custom         configuration for that chart.</p> </li> <li> <p>Example (<code>K8s/cicd/argo-workflows/kustomization.yaml</code>):</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nhelmCharts:\n  - name: argo-workflows\n    repo: https://argoproj.github.io/argo-helm\n    version: 0.45.11\n    releaseName: argo-workflows\n    namespace: cicd\n    valuesFile: argo-workflows-values.yaml\n</code></pre> </li> </ul>"},{"location":"architecture/applications/#workflow-for-deploying-a-new-application","title":"Workflow for Deploying a New Application","text":"<ol> <li>Choose the Stack: Identify which stack the new application belongs to (e.g.,     <code>observability</code>).</li> <li>Create a Directory: Create a new subdirectory for your application (e.g.,     <code>K8s/observability/my-new-app/</code>).</li> <li>Add Manifests: Add all Kubernetes manifest files for your application to this     new directory.</li> <li>Create <code>kustomization.yaml</code>: Add a <code>kustomization.yaml</code> file that lists all the     manifest files you just created.</li> <li>Commit &amp; Push: Commit your changes to Git. The corresponding <code>ApplicationSet</code>     will automatically detect the new directory and deploy your application into the     correct namespace.</li> </ol>"},{"location":"architecture/applications/#component-versioning-strategy","title":"Component Versioning Strategy","text":"<p>The project uses a two-tiered strategy for managing Helm chart versions, depending on how each component is deployed:</p> <ol> <li> <p>Core Infrastructure Components (Managed by <code>Taskfile.yaml</code>)</p> <ul> <li>Components: Cilium, Cert-Manager, Vault, ArgoCD.</li> <li>Method: These components are deployed imperatively via   <code>helm upgrade</code> tasks within the <code>Taskfile.yaml</code>.</li> <li>Version Source: Their chart versions are centralized in the <code>vars:</code> section of   the <code>Taskfile.yaml</code>. This allows for top-level control over   critical infrastructure versions.</li> </ul> </li> <li> <p>Application Stack Components (Managed by GitOps)</p> <ul> <li>Components: Argo Workflows, SonarQube, Loki, Prometheus, Trivy, etc.</li> <li>Method: These components are deployed declaratively by ArgoCD via   <code>ApplicationSet</code> resources.</li> <li>Version Source: The Helm chart version is specified directly in each   application's respective <code>kustomization.yaml</code> file.   This approach keeps an application's entire configuration,   including its version, co-located in a single place,   adhering to GitOps principles.</li> </ul> </li> </ol>"},{"location":"architecture/infrastructure/","title":"IT Directory Architecture","text":"<p>This directory contains the static or bootstrap layer of the platform. These are the core components required to bring up a functional Kubernetes cluster before the GitOps engine (e.g., ArgoCD) takes over.</p>"},{"location":"architecture/infrastructure/#guiding-principles","title":"Guiding Principles","text":"<p>The structure follows two simple rules to ensure consistency and clarity.</p>"},{"location":"architecture/infrastructure/#1-helm-chart-configurations","title":"1. Helm Chart Configurations","text":"<p>Configuration for a core component deployed via a Helm chart is defined in a single <code>*-values.yaml</code> file.</p> <ul> <li>Location: Root of the <code>IT/</code> directory.</li> <li>Naming: The filename is simple and references the component (e.g.,   <code>cilium-values.yaml</code>, <code>vault-values.yaml</code>).</li> </ul>"},{"location":"architecture/infrastructure/#2-raw-kubernetes-manifests","title":"2. Raw Kubernetes Manifests","text":"<p>Static Kubernetes resources that are not part of a Helm chart installation (e.g., a <code>ClusterIssuer</code> or a <code>ClusterSecretStore</code>) are organized as raw YAML files.</p> <ul> <li>Location: Placed inside a subdirectory named after the parent component (e.g.,   <code>cert-manager/</code>).</li> <li>Naming: The filename must exactly match the <code>metadata.name</code> of the resource   defined within it (e.g., a <code>ClusterIssuer</code> with <code>name: ca-issuer</code> is saved in   <code>ca-issuer.yaml</code>).</li> </ul>"},{"location":"architecture/infrastructure/#3-bootstrap-resources-via-kustomize","title":"3. Bootstrap Resources via Kustomize","text":"<p>Some components require orchestrated deployment of multiple resources. These use Kustomize for composition and are deployed via <code>kustomize build &lt;dir&gt;/ | kubectl apply -f -</code>.</p> <ul> <li>Location: Subdirectories with their own <code>kustomization.yaml</code>.</li> <li>Purpose:</li> <li><code>namespaces/</code>: Bootstrap namespace definitions for core components.</li> <li><code>cert-manager/</code>: Issuers and Certificate definitions.</li> <li><code>external-secrets/</code>: The <code>ClusterSecretStore</code> to connect to Vault and     the <code>ExternalSecret</code> for ArgoCD.</li> <li><code>argocd/</code>: Kustomization to support the Helm chart.</li> </ul>"},{"location":"architecture/infrastructure/#visual-structure","title":"Visual Structure","text":"<pre><code>graph TD\n    A[IT/] --&gt; B(cilium-values.yaml);\n    A --&gt; C(eso-values.yaml);\n    A --&gt; D(k3d-cluster.yaml);\n    A --&gt; E(vault-values.yaml);\n    A --&gt; AA(cert-manager-values.yaml);\n    A --&gt; AB(argocd-values.yaml);\n    A --&gt; AC(kustomization.yaml);\n\n    A --&gt; F[cert-manager/];\n    A --&gt; G[external-secrets/];\n    A --&gt; N[namespaces/];\n    A --&gt; V[vault/];\n    A --&gt; X[argocd/];\n\n    subgraph Raw Manifests &amp; Kustomize\n        F --&gt; H(ca-issuer.yaml);\n        G --&gt; K(argocd-secretstore.yaml);\n        G --&gt; L(argocd-admin-externalsecret.yaml);\n        N --&gt; N1(kustomization.yaml);\n        V --&gt; V1(vault-manual-init.sh);\n        X --&gt; X1(kustomization.yaml);\n    end\n\n    subgraph Helm Values\n        B\n        C\n        D\n        E\n        AA\n        AB\n    end</code></pre>"},{"location":"architecture/infrastructure/#quick-reference","title":"Quick Reference","text":"File / Directory Purpose Type <code>k3d-cluster.yaml</code> Defines the k3d cluster itself. k3d Config <code>kustomization.yaml</code> Root Kustomize orchestrator (currently minimal). Kustomize <code>*-values.yaml</code> Configures a core component's Helm chart. Helm Values <code>namespaces/</code> Bootstrap namespace definitions for core components. Kustomize Bootstrap <code>vault/</code> Contains the manual Vault initialization script (<code>vault-manual-init.sh</code>). Shell Script <code>cert-manager/</code> Raw manifests for cert-manager (ClusterIssuers, CA certificates). Raw Manifests <code>external-secrets/</code> Raw manifests for External Secrets Operator (SecretStore, ExternalSecret). Raw Manifests <code>argocd/</code> Kustomization to support the ArgoCD Helm chart deployment. Kustomize"},{"location":"architecture/infrastructure/#deployment-workflow","title":"Deployment Workflow","text":"<p>The bootstrap process follows this order (orchestrated by <code>Taskfile.yaml</code>):</p> <ol> <li>Create k3d cluster (<code>k3d-cluster-cached.yaml</code> with persistent registry cache).</li> <li>Apply bootstrap namespaces via Kustomize: <code>kustomize build namespaces/ | kubectl apply -f -</code>.</li> <li>Deploy Cilium CNI via Helm (<code>cilium-values.yaml</code>).</li> <li>Deploy Prometheus CRDs via a dedicated Helm chart install.    This is done using the <code>kube-prometheus-stack</code> chart with the <code>crdsOnly=true</code> flag.</li> <li>Deploy Cert-Manager via Helm (<code>cert-manager-values.yaml</code>).</li> <li>Wait for webhook readiness.</li> <li>Then apply Cert-Manager resources: <code>kustomize build cert-manager/ | kubectl apply -f -</code>.</li> <li>Deploy Vault Stack:</li> <li>Deploy Vault via Helm (<code>vault-values.yaml</code>).</li> <li>Execute the initialization script (<code>vault-init.sh</code>) to initialize and unseal Vault.</li> <li>Deploy External Secrets Operator via Helm (<code>eso-values.yaml</code>).</li> <li>Then apply ESO resources (<code>ClusterSecretStore</code>, <code>ExternalSecret</code> for ArgoCD)      via Kustomize: <code>kustomize build external-secrets/ | kubectl apply -f -</code>.</li> <li>Deploy ArgoCD via Helm (<code>argocd-values.yaml</code>).</li> <li>Deploy Gateway API via Kustomize: <code>kustomize build gateway/ | kubectl apply -f -</code>.</li> <li>Creates the Gateway resource with wildcard TLS certificate.</li> <li>Deploy Policies via ArgoCD Application: <code>kubectl apply -f Policies/app-kyverno.yaml</code>.</li> <li>Deploy Application Stacks via ApplicationSets: ArgoCD auto-discovers and deploys all apps.</li> </ol> <p>Key Insight: The bootstrap process is a carefully orchestrated sequence of Helm deployments  and Kustomize applications, managed entirely by <code>Taskfile.yaml</code>.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>This section details the architecture of the IDP Blueprint platform.</p>"},{"location":"architecture/overview/#architecture-layers","title":"Architecture Layers","text":"<p>The platform architecture is organized into distinct layers:</p> <ul> <li>Visual Architecture: Graphical representation of the platform architecture</li> <li>Infrastructure Layer: Static bootstrap components and core infrastructure</li> <li>Application Layer: GitOps-managed application stacks</li> <li>Secrets Management: Architecture for secrets management and synchronization</li> </ul>"},{"location":"architecture/overview/#components","title":"Components","text":"<p>The platform consists of several key component categories:</p> <ul> <li>Infrastructure: Core foundational services including Cilium CNI, Cert Manager, Vault, External Secrets, and ArgoCD</li> <li>Policy: Policy enforcement with Kyverno and Policy Reporter</li> <li>Observability: Monitoring and logging with Prometheus, Grafana, Loki, and Fluent-bit</li> <li>CI/CD: Workflow orchestration with Argo Workflows and code quality analysis with SonarQube</li> <li>Security: Vulnerability scanning and compliance checking with Trivy</li> </ul>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":"<p>The architecture follows key design principles: - GitOps-first approach - Policy-as-code - Resource optimization - Security-first - Observability-driven</p>"},{"location":"architecture/secrets/","title":"Secrets Management Architecture","text":""},{"location":"architecture/secrets/#overview","title":"Overview","text":"<p>This IDP implements a centralized, cloud-agnostic secrets management strategy where HashiCorp Vault serves as the single source of truth for all sensitive data. The architecture uses External Secrets Operator (ESO) as the unified tool to synchronize secrets both within the Kubernetes cluster and to external cloud providers.</p>"},{"location":"architecture/secrets/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"external-secrets-system namespace\"\n            Vault[(\"HashiCorp Vault&lt;br/&gt;(Single Source of Truth)\")]\n            ESO[\"External Secrets Operator&lt;br/&gt;(ESO)\"]\n        end\n\n        subgraph \"Application Namespaces\"\n            K8sSecrets[\"Kubernetes Secrets\"]\n            Pods[\"Application Pods\"]\n        end\n\n        Vault --&gt;|\"read\"| ESO\n        ESO --&gt;|\"create/update\"| K8sSecrets\n        K8sSecrets --&gt;|\"mount\"| Pods\n    end\n\n    subgraph \"External Cloud Providers\"\n        AWS[\"AWS Secrets Manager\"]\n        GCP[\"GCP Secret Manager\"]\n        Azure[\"Azure Key Vault\"]\n    end\n\n    ESO --&gt;|\"PushSecret&lt;br/&gt;propagate\"| AWS\n    ESO --&gt;|\"PushSecret&lt;br/&gt;propagate\"| GCP\n    ESO --&gt;|\"PushSecret&lt;br/&gt;propagate\"| Azure\n\n    subgraph \"External Workloads\"\n        Lambda[\"AWS Lambda\"]\n        CloudRun[\"GCP Cloud Run\"]\n        AzureFunc[\"Azure Functions\"]\n        CrossplaneRDS[\"Crossplane-managed RDS\"]\n    end\n\n    AWS -.-&gt;|\"consume\"| Lambda\n    AWS -.-&gt;|\"consume\"| CrossplaneRDS\n    GCP -.-&gt;|\"consume\"| CloudRun\n    Azure -.-&gt;|\"consume\"| AzureFunc\n\n    style Vault fill:#6366f1,stroke:#4338ca,stroke-width:3px,color:#fff\n    style ESO fill:#f59e0b,stroke:#d97706,stroke-width:2px,color:#fff\n    style AWS fill:#ff9900,stroke:#ff6600,color:#000\n    style GCP fill:#4285f4,stroke:#1a73e8,color:#fff\n    style Azure fill:#0078d4,stroke:#005a9e,color:#fff</code></pre>"},{"location":"architecture/secrets/#flow-explanation","title":"Flow Explanation","text":""},{"location":"architecture/secrets/#inside-cluster-eso-externalsecret","title":"Inside Cluster (ESO <code>ExternalSecret</code>)","text":"<p>Vault \u2192 ESO \u2192 Kubernetes Secrets \u2192 Pods</p> <ol> <li>Vault stores secrets in its KV v2 engine at paths like <code>secret/data/*</code>.</li> <li>ESO watches for <code>ExternalSecret</code> custom resources in application namespaces.</li> <li>ESO authenticates to Vault using a configured <code>ClusterSecretStore</code> which   leverages Kubernetes service account authentication.</li> <li>ESO fetches the specified secrets from Vault and creates   or updates native Kubernetes Secrets.</li> <li>Application Pods mount these Kubernetes Secrets as volumes   or environment variables, completely unaware of Vault.</li> </ol> <p>Example:</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: db-credentials\nspec:\n  secretStoreRef:\n    name: vault-backend\n    kind: ClusterSecretStore\n  target:\n    name: db-secret # \u2190 Creates or updates this K8s Secret\n  dataFrom:\n  - extract:\n      key: secret/data/prod/database\n</code></pre>"},{"location":"architecture/secrets/#outside-cluster-eso-pushsecret","title":"Outside Cluster (ESO <code>PushSecret</code>)","text":"<p>Vault \u2192 ESO \u2192 Cloud Secret Managers \u2192 External Workloads</p> <ol> <li>Vault stores secrets (same source of truth).</li> <li>ESO uses a <code>PushSecret</code> custom resource to read secrets from Vault.</li> <li>ESO propagates/pushes these secrets to external providers like   AWS Secrets Manager, GCP Secret Manager, or Azure Key Vault.</li> <li>External workloads (e.g., AWS Lambda, GCP Cloud Run) consume secrets   from their native cloud secret manager.</li> </ol> <p>Example - Push to AWS:</p> <pre><code>apiVersion: external-secrets.io/v1alpha1\nkind: PushSecret\nmetadata:\n  name: push-to-aws\nspec:\n  secretStoreRefs:\n    - name: vault-secretstore # Source: Vault\n      kind: ClusterSecretStore\n    - name: aws-secrets-manager # Destination: AWS\n      kind: SecretStore\n  selector:\n    secret:\n      name: lambda-db-password # Secret from Vault\n  data:\n    - match:\n        secretKey: password\n        remoteRef:\n          remoteKey: /prod/lambda/db-password # AWS SM path\n</code></pre>"},{"location":"architecture/secrets/#key-principles","title":"Key Principles","text":""},{"location":"architecture/secrets/#1-single-source-of-truth","title":"1. Single Source of Truth","text":"<p>Vault is authoritative for all secrets. No secrets are created directly in:</p> <ul> <li>Kubernetes Secrets (ESO creates them from Vault).</li> <li>AWS/GCP/Azure Secret Managers (ESO pushes them from Vault).</li> </ul> <p>Benefits:</p> <ul> <li>Centralized Audit Trail: All secret access is logged in Vault.</li> <li>Consistent Rotation: Rotate a secret in Vault, and ESO propagates the change everywhere.</li> <li>No Vendor Lock-in: The core secret store remains vendor-neutral.</li> </ul>"},{"location":"architecture/secrets/#2-unified-operator","title":"2. Unified Operator","text":"Component Responsibility Scope Vault Store, rotate, and audit secrets Everything ESO Sync Vault to K8s &amp; push to Cloud In-cluster &amp; External <p>External Secrets Operator (ESO) is used for all secret synchronization tasks. It provides a robust and consistent mechanism for both pulling secrets into the cluster and pushing them to external systems.</p>"},{"location":"architecture/secrets/#3-zero-touch-secret-consumption","title":"3. Zero-Touch Secret Consumption","text":"<p>Developers never handle raw secrets. The process is declarative:</p> <ol> <li>A developer defines an <code>ExternalSecret</code> or <code>PushSecret</code> manifest in their   application's Git repository.</li> <li>The GitOps controller (ArgoCD) applies the manifest.</li> <li>ESO automatically fetches the secret from Vault and makes it available to the application.</li> <li>Rotation is transparent: when the secret is updated in Vault,   ESO updates the corresponding Kubernetes Secret or pushes the change to the cloud provider.</li> </ol>"},{"location":"architecture/secrets/#use-cases","title":"Use Cases","text":""},{"location":"architecture/secrets/#in-cluster-workloads-eso-externalsecret","title":"In-Cluster Workloads (ESO <code>ExternalSecret</code>)","text":"<p>\u2705 Use <code>ExternalSecret</code> when:</p> <ul> <li>Your workload runs inside the Kubernetes cluster.</li> <li>You need secrets available as standard Kubernetes <code>Secret</code> objects (for volume mounts   or environment variables).</li> </ul> <p>Examples:</p> <ul> <li>Microservices needing database passwords.</li> <li>CI/CD pipelines running in Jenkins pods.</li> <li>Web applications requiring API keys.</li> </ul>"},{"location":"architecture/secrets/#external-workloads-eso-pushsecret","title":"External Workloads (ESO <code>PushSecret</code>)","text":"<p>\u2705 Use <code>PushSecret</code> when:</p> <ul> <li>Your workload runs outside Kubernetes (e.g., serverless, VMs).</li> <li>The workload cannot call the Vault API directly (common in legacy applications).</li> <li>A cloud provider's service (like AWS RDS) requires credentials to be present   in its native secret manager.</li> </ul> <p>Examples:</p> <ul> <li>AWS Lambda functions reading from AWS Secrets Manager.</li> <li>Crossplane provisioning an RDS instance, which requires credentials in AWS.</li> <li>GCP Cloud Run services reading from GCP Secret Manager.</li> </ul>"},{"location":"architecture/secrets/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture/secrets/#demo-environment-current","title":"Demo Environment (Current)","text":"<p>\u26a0\ufe0f NOT for production:</p> <ul> <li>TLS is disabled (<code>skipTLSVerify: true</code>).</li> <li>Vault is initialized with a single unseal key.</li> <li>The root token is logged during the init script.</li> <li>Unseal keys and the root token are stored in a Kubernetes Secret.</li> </ul>"},{"location":"architecture/secrets/#production-hardening","title":"Production Hardening","text":"<p>Must implement:</p> <ol> <li>TLS Everywhere: Enforce encrypted traffic between all components.</li> <li>Auto-Unseal with Cloud KMS: Use a cloud provider's KMS to automatically unseal Vault.</li> <li>Multi-Share Unseal Keys: Require a quorum of operators to unseal Vault manually if auto-unseal fails.</li> <li>RBAC Policies: Implement least-privilege access control within Vault.</li> <li>Network Policies: Restrict network access to the Vault service, ideally only allowing ESO to connect.</li> </ol>"},{"location":"architecture/secrets/#deployment-workflow","title":"Deployment Workflow","text":""},{"location":"architecture/secrets/#initial-setup-one-time","title":"Initial Setup (One-Time)","text":"<pre><code># 1. Deploy Vault (sealed, uninitialized)\ntask vault:deploy\n\n# 2. Initialize Vault manually\ntask vault:init\n# \u2192 Generates unseal keys, root token\n# \u2192 Configures Kubernetes auth for ESO\n\n# 3. Deploy ESO\ntask external-secrets:deploy\ntask it:external-secrets:apply-resources\n</code></pre>"},{"location":"architecture/secrets/#day-2-operations","title":"Day-2 Operations","text":"<p>Add a new secret to Vault:</p> <pre><code>kubectl exec -n vault-system vault-0 -- \\\n  vault kv put secret/prod/app-credentials \\\n  username=app_user \\\n  password=secure_password\n</code></pre> <p>Consume in-cluster (ESO <code>ExternalSecret</code>):</p> <p>This is the standard way to make secrets available to pods.</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: app-credentials\n  namespace: production\nspec:\n  secretStoreRef:\n    name: vault-backend\n    kind: ClusterSecretStore\n  target:\n    name: app-secret # ESO creates this K8s Secret\n    creationPolicy: Owner\n  dataFrom:\n  - extract:\n      key: secret/data/prod/app-credentials\n</code></pre> <p>Real-world example: ArgoCD admin password management (see <code>IT/external-secrets/argocd-admin-externalsecret.yaml</code>):</p> <p>This <code>ExternalSecret</code> targets an existing secret managed by Helm (<code>argocd-secret</code>) and merges the password into it.</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: argocd-admin-password\n  namespace: argocd\nspec:\n  refreshInterval: 5m\n  secretStoreRef:\n    kind: SecretStore\n    name: vault-backend\n  target:\n    name: argocd-secret\n    creationPolicy: Merge\n    deletionPolicy: Retain\n  data:\n    - secretKey: admin.password\n      remoteRef:\n        key: secret/data/argocd/admin\n        property: admin.password\n</code></pre> <p>Push to AWS (ESO <code>PushSecret</code>):</p> <pre><code>apiVersion: external-secrets.io/v1alpha1\nkind: PushSecret\nmetadata:\n  name: push-app-creds-to-aws\nspec:\n  secretStoreRefs:\n    - name: vault-secretstore\n    - name: aws-secrets-manager\n  selector:\n    secret:\n      name: app-credentials # References a K8s secret synced by another ExternalSecret\n  data:\n    - match:\n        secretKey: password\n        remoteRef:\n          remoteKey: /prod/lambda/app-password\n</code></pre>"},{"location":"architecture/secrets/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"architecture/secrets/#metrics","title":"Metrics","text":"<p>Vault:</p> <ul> <li><code>vault_core_unsealed</code> - Seal status (0=sealed, 1=unsealed).</li> <li><code>vault_token_count_by_policy</code> - Active tokens per policy.</li> <li>Prometheus ServiceMonitor is enabled in <code>vault-values.yaml</code>.</li> </ul> <p>ESO:</p> <ul> <li><code>externalsecret_sync_calls_total</code> - Total sync operations.</li> <li><code>externalsecret_sync_calls_error</code> - Total sync failures.</li> <li>Available via a ServiceMonitor.</li> </ul>"},{"location":"architecture/secrets/#alerts","title":"Alerts","text":"<p>Critical:</p> <ul> <li>Vault is sealed unexpectedly.</li> <li>ESO sync failures &gt; 5 in the last 10 minutes.</li> <li>A Vault token used by ESO is approaching its expiration.</li> </ul> <p>Warning:</p> <ul> <li>Secret rotation delay &gt; 1 hour.</li> <li>High Vault API latency (&gt;500ms).</li> </ul>"},{"location":"architecture/secrets/#migration-path-from-current-state","title":"Migration Path from Current State","text":""},{"location":"architecture/secrets/#from-custom-init-sidecar-old","title":"From: Custom Init Sidecar (Old)","text":"<p>A previous version of this blueprint used a custom sidecar container with a bash script to initialize Vault and inject secrets. This is now deprecated.</p>"},{"location":"architecture/secrets/#to-eso-manual-init-current","title":"To: ESO + Manual Init (Current)","text":"<p>The current, stable architecture uses a one-time manual initialization script for Vault (<code>task vault:init</code>) and relies on External Secrets Operator for all subsequent secret synchronization.</p>"},{"location":"architecture/secrets/#future-vault-operator-bank-vaults","title":"Future: Vault Operator (Bank-Vaults)","text":"<p>For environments requiring fully automated, declarative management of Vault itself (including auto-unseal, HA configuration, etc.), migrating to an operator like Bank-Vaults is the recommended next step.</p> <p>When to migrate:</p> <ul> <li>When you need a High-Availability Vault cluster (e.g., 3+ replicas with Raft).</li> <li>When you require fully automatic unsealing using a cloud KMS.</li> <li>When zero manual intervention in Vault's lifecycle is a hard requirement.</li> </ul>"},{"location":"architecture/secrets/#references","title":"References","text":"<ul> <li>External Secrets Operator Docs</li> <li>ESO <code>PushSecret</code></li> <li>Vault Production Hardening</li> <li>Bank-Vaults Operator</li> </ul>"},{"location":"architecture/visual/","title":"IDP Visual Architecture","text":"<p>This document describes the platform's architecture, its workflows, and its execution environment through diagrams.</p>"},{"location":"architecture/visual/#1-general-architecture-and-gitops-flow","title":"1. General Architecture and GitOps Flow","text":"<p>This diagram shows the high-level view of the workflow, from the definition in a Git repository to the deployment and operation of the components within the Kubernetes cluster.</p> <pre><code>graph TD\n    GitRepo[Git Repository]\n\n    subgraph K8sCluster [Kubernetes Cluster]\n        direction TB\n\n        subgraph CoreInfrastructure [Core Infrastructure]\n            direction LR\n            Cilium[Cilium]\n            CertManager[Cert-Manager]\n            Vault[Vault]\n            ESO[External Secrets Operator]\n        end\n\n        subgraph GitOpsPolicyEngine [GitOps and Policy Engine]\n            direction LR\n            ArgoCD[ArgoCD]\n            Kyverno[Kyverno]\n            PolicyReporter[Policy Reporter]\n        end\n\n        subgraph AppStacks [Application Stacks]\n            direction TB\n\n            subgraph ObservabilityStack [Observability Stack]\n                direction LR\n                Prometheus[Prometheus]\n                Grafana[Grafana]\n                Loki[Loki]\n                FluentBit[Fluent-bit]\n            end\n\n            subgraph CICDStack [CI/CD Stack]\n                direction LR\n                Workflows[Argo Workflows]\n                SonarQube[SonarQube]\n            end\n\n            subgraph SecurityStack [Security Stack]\n                direction LR\n                Trivy[Trivy Operator]\n            end\n        end\n\n        K8sApi[Kubernetes API Server]\n    end\n\n    GitRepo --&gt;|Defines State| ArgoCD\n    ArgoCD --&gt;|Applies Manifests| AppStacks\n    ArgoCD --&gt;|Creates Resources| K8sApi\n    K8sApi --&gt;|Validates Requests| Kyverno\n    Kyverno --&gt;|Enforces Policies| K8sApi\n    Kyverno --&gt;|Reports Status| PolicyReporter\n    Vault --&gt;|Provides Secrets| ESO\n    ESO --&gt;|Syncs Secrets| K8sApi\n    FluentBit --&gt;|Collects Logs| Loki\n    Prometheus --&gt;|Scrapes Metrics| Grafana\n    Loki --&gt;|Provides Logs| Grafana\n    Workflows --&gt;|Triggers analysis in| SonarQube\n    Trivy --&gt;|Scans Resources| K8sApi\n    CertManager --&gt;|Manages Certificates| K8sApi</code></pre>"},{"location":"architecture/visual/#2-helm-to-pods-deployment-flow","title":"2. Helm to Pods Deployment Flow","text":"<p>This diagram shows the complete deployment chain from Helm charts to running pods, illustrating how different layers (Bootstrap, GitOps) interact.</p> <pre><code>graph TB\n    subgraph Bootstrap [Bootstrap Layer - IT/]\n        H1[Helm: cilium v1.18.2]\n        H2[Helm: vault v0.31.0]\n        H3[Helm: argocd v8.5.8]\n        H4[Helm: cert-manager v1.18.2]\n        H5[Helm: external-secrets v0.20.2]\n    end\n\n    subgraph GitOps [GitOps Layer - K8s/]\n        direction TB\n\n        subgraph ArgoCD_Core [ArgoCD Applications]\n            APP1[App: observability-fluent-bit]\n            APP2[App: observability-loki]\n            APP3[App: observability-kube-prometheus-stack]\n            APP4[App: cicd-argo-workflows]\n            APP5[App: security-trivy]\n            APP6[App: platform-policies]\n        end\n\n        subgraph Kustomize [Kustomize + Helm Charts]\n            K1[Kustomize: fluent-bit&lt;br/&gt;helmCharts: fluent-bit v0.54.0]\n            K2[Kustomize: loki&lt;br/&gt;helmCharts: loki v6.42.0]\n            K3[Kustomize: kube-prometheus-stack&lt;br/&gt;helmCharts: v77.14.0]\n            K4[Kustomize: argo-workflows&lt;br/&gt;helmCharts: argo-workflows v0.45.11]\n            K5[Kustomize: trivy&lt;br/&gt;helmCharts: trivy-operator v0.31.0]\n            K6[Kustomize: kyverno&lt;br/&gt;helmCharts: kyverno v3.5.2]\n        end\n    end\n\n    subgraph K8s [Kubernetes Resources]\n        direction TB\n\n        subgraph Workloads [Running Workloads]\n            DS1[DaemonSet: cilium-agent]\n            STS1[StatefulSet: vault-0]\n            DEP1[Deployment: argocd-server]\n\n            DS2[DaemonSet: fluent-bit]\n            STS2[StatefulSet: loki]\n            STS3[StatefulSet: prometheus-prometheus]\n            DEP2[Deployment: prometheus-grafana]\n\n            DEP3[Deployment: argo-workflows-server]\n            DEP4[Deployment: argo-workflows-controller]\n            DEP5[Deployment: trivy-operator]\n            DEP6[Deployment: kyverno-admission-controller]\n        end\n    end\n\n    H1 --&gt;|Deployed via Taskfile| DS1\n    H2 --&gt;|Deployed via Taskfile| STS1\n    H3 --&gt;|Deployed via Taskfile| DEP1\n\n    DEP1 --&gt;|Manages| APP1\n    DEP1 --&gt;|Manages| APP2\n    DEP1 --&gt;|Manages| APP3\n    DEP1 --&gt;|Manages| APP4\n    DEP1 --&gt;|Manages| APP5\n    DEP1 --&gt;|Manages| APP6\n\n    APP1 --&gt;|Builds| K1\n    APP2 --&gt;|Builds| K2\n    APP3 --&gt;|Builds| K3\n    APP4 --&gt;|Builds| K4\n    APP5 --&gt;|Builds| K5\n    APP6 --&gt;|Builds| K6\n\n    K1 --&gt;|Renders Helm + Applies| DS2\n    K2 --&gt;|Renders Helm + Applies| STS2\n    K3 --&gt;|Renders Helm + Applies| STS3\n    K3 --&gt;|Renders Helm + Applies| DEP2\n    K4 --&gt;|Renders Helm + Applies| STS4\n    K5 --&gt;|Renders Helm + Applies| DEP3\n    K6 --&gt;|Renders Helm + Applies| DEP4</code></pre>"},{"location":"architecture/visual/#3-node-pools-and-workload-deployment","title":"3. Node Pools and Workload Deployment","text":"<p>Within the Hub cluster, nodes are segmented into logical \"Node Pools\" using labels to isolate workloads. This classification is the basis for future scheduling rules with <code>tolerations</code> and <code>affinity</code>.</p> <pre><code>graph TD\n    subgraph IDPHubCluster [IDP Hub Cluster - k3d-idp-demo]\n        subgraph NodePool_Infra [Node Pool: IT Infrastructure]\n            direction TB\n            infra_node[k3d-idp-demo-agent-0&lt;br/&gt;Label: node-role=it-infra]\n        end\n\n        subgraph NodePool_Apps [Node Pool: GitOps Workloads]\n            direction TB\n            apps_node[k3d-idp-demo-agent-1&lt;br/&gt;Label: node-role=k8s-workloads]\n        end\n\n        subgraph NodePool_CP [Node Pool: Control Plane]\n            direction TB\n            cp_node[k3d-idp-demo-server-0&lt;br/&gt;Control Plane + etcd]\n        end\n\n        subgraph Workloads_Platform [Platform Services]\n            direction LR\n            argo[ArgoCD]\n            vault[Vault]\n            prom[Prometheus]\n            kyv[Kyverno]\n        end\n\n        subgraph Workloads_Apps [Application Workloads]\n            direction LR\n            workflows[Argo Workflows]\n            sonar[SonarQube]\n        end\n\n        subgraph DaemonSets_AllNodes [DaemonSets - All Nodes]\n            direction LR\n            cilium[Cilium Agent]\n            fluent[Fluent-bit]\n            node_exp[Node Exporter]\n        end\n    end\n\n    Workloads_Platform -.-&gt;|Scheduled on| NodePool_Infra\n    Workloads_Apps -.-&gt;|Scheduled on| NodePool_Apps\n    DaemonSets_AllNodes --&gt;|Runs on| NodePool_CP\n    DaemonSets_AllNodes --&gt;|Runs on| NodePool_Infra\n    DaemonSets_AllNodes --&gt;|Runs on| NodePool_Apps</code></pre>"},{"location":"architecture/visual/#3-certificate-management-flow","title":"3. Certificate Management Flow","text":"<p>This flow shows how a Gateway resource automatically obtains a TLS certificate via cert-manager annotation.</p> <pre><code>sequenceDiagram\n    participant GW as Gateway Resource\n    participant CM as cert-manager\n    participant CI as ClusterIssuer\n    participant CAS as CA Secret\n    participant FinalTLS as TLS Certificate Secret\n\n    GW-&gt;&gt;+CM: 1. Requests certificate via annotation\n    Note over GW,CM: cert-manager.io/cluster-issuer: ca-issuer\n    CM-&gt;&gt;+CI: 2. Reads the ClusterIssuer configuration\n    CI--&gt;&gt;-CM: spec.ca.secretName: idp-demo-ca-secret\n    CM-&gt;&gt;+CAS: 3. Loads the root CA from the Secret\n    CAS--&gt;&gt;-CM: CA private key and certificate\n    CM--&gt;&gt;CM: 4. Signs wildcard certificate (*.127-0-0-1.sslip.io)\n    CM-&gt;&gt;+FinalTLS: 5. Creates Certificate Secret (idp-wildcard-cert)\n    FinalTLS--&gt;&gt;-GW: 6. Referenced in Gateway spec.listeners.tls</code></pre>"},{"location":"architecture/visual/#4-secret-management-flow","title":"4. Secret Management Flow","text":"<p>This flow details how an application securely consumes a secret from Vault without having direct credentials.</p> <pre><code>sequenceDiagram\n    participant App as Application Pod\n    participant K8S_Secret as Kubernetes Secret\n    participant ES_CR as ExternalSecret CR\n    participant ESO as External Secrets Operator\n    participant CSS as ClusterSecretStore\n    participant Vault\n\n    App-&gt;&gt;+K8S_Secret: 1. Needs to mount a K8s Secret\n    K8S_Secret--&gt;&gt;-App: Not found or needs update\n    ESO-&gt;&gt;+ES_CR: 2. Watches the ExternalSecret resource\n    ES_CR--&gt;&gt;-ESO: spec.secretStoreRef: vault-secretstore\n    ESO-&gt;&gt;+CSS: 3. Reads the referenced ClusterSecretStore\n    CSS--&gt;&gt;-ESO: provider: vault, auth: kubernetes\n    ESO-&gt;&gt;+Vault: 4. Authenticates and requests the secret\n    Vault--&gt;&gt;-ESO: Returns the secret data\n    ESO-&gt;&gt;+K8S_Secret: 5. Creates/Updates the Kubernetes Secret\n    K8S_Secret--&gt;&gt;App: 6. The Secret is mounted in the Pod</code></pre>"},{"location":"architecture/visual/#5-observability-data-flow","title":"5. Observability Data Flow","text":"<p>This diagram details how metrics and logs are collected, processed, and visualized on the platform.</p> <pre><code>graph TD\n    subgraph Kubernetes Nodes\n        direction LR\n        AppPod[App Pod]\n        Kubelet[Kubelet]\n        NodeExporter[Node Exporter]\n        ContainerLogs[Container Log Files]\n    end\n\n    subgraph Observability Namespace\n        direction LR\n        Prometheus[Prometheus]\n        Loki[Loki]\n        Grafana[Grafana]\n        KSM[Kube-State-Metrics]\n        FluentBit[Fluent-bit DaemonSet]\n    end\n\n    AppPod --&gt;|Generates Logs| ContainerLogs\n    AppPod --&gt;|Exposes Metrics| Prometheus\n    Kubelet --&gt;|Exposes Metrics| Prometheus\n\n    ContainerLogs --&gt;|Tailed by| FluentBit\n    FluentBit --&gt;|Forwards Logs| Loki\n\n    NodeExporter --&gt;|Scrapes Node Metrics| Prometheus\n    KSM --&gt;|Scrapes Cluster Metrics| Prometheus\n\n    Prometheus --&gt;|Data Source| Grafana\n    Loki --&gt;|Data Source| Grafana</code></pre>"},{"location":"architecture/visual/#6-security-scanning-flow-with-trivy","title":"6. Security Scanning Flow with Trivy","text":"<p>This diagram illustrates how the Trivy operator scans cluster workloads for vulnerabilities.</p> <pre><code>sequenceDiagram\n    participant User as User/ArgoCD\n    participant K8sApi as Kubernetes API\n    participant TrivyOp as Trivy Operator\n    participant Workload as e.g., Deployment, Pod\n    participant Report as VulnerabilityReport CRD\n\n    User-&gt;&gt;K8sApi: 1. Creates/Updates a Workload\n    K8sApi--&gt;&gt;TrivyOp: 2. Watches resource changes\n    TrivyOp-&gt;&gt;Workload: 3. Discovers container images\n    TrivyOp-&gt;&gt;TrivyOp: 4. Scans images for vulnerabilities\n    TrivyOp-&gt;&gt;K8sApi: 5. Creates/Updates VulnerabilityReport\n    K8sApi--&gt;&gt;Report: 6. Stores the report</code></pre>"},{"location":"architecture/visual/#7-gitops-structure-with-applicationsets","title":"7. GitOps Structure with ApplicationSets","text":"<p>This diagram explains the \"App of Apps\" pattern. The <code>ApplicationSet</code> resources in ArgoCD monitor directories in Git. When they find subdirectories that match their generator, they automatically create child <code>Application</code> resources, one for each stack component.</p> <pre><code>graph LR\n    subgraph Git Repository\n        direction TB\n        RepoDir[K8s/ Directory]\n        ObsDir[observability/]\n        SecDir[security/]\n        CiCdDir[cicd/]\n    end\n\n    subgraph ArgoCD\n        direction TB\n        AppSetObs[ApplicationSet observability]\n        AppSetSec[ApplicationSet security]\n        AppSetCiCd[ApplicationSet cicd]\n\n        AppPrometheus[App: obs-prometheus]\n        AppGrafana[App: obs-grafana]\n        AppLoki[App: obs-loki]\n        AppTrivy[App: sec-trivy]\n    end\n\n    GitRepo --&gt;|Monitored by| AppSetObs\n    GitRepo --&gt;|Monitored by| AppSetSec\n    GitRepo --&gt;|Monitored by| AppSetCiCd\n\n    AppSetObs --&gt;|Generates| AppPrometheus\n    AppSetObs --&gt;|Generates| AppGrafana\n    AppSetObs --&gt;|Generates| AppLoki\n    AppSetSec --&gt;|Generates| AppTrivy</code></pre>"},{"location":"architecture/visual/#8-gateway-api-service-exposure","title":"8. Gateway API Service Exposure","text":"<p>This diagram shows how services are exposed via Gateway API with wildcard TLS and sslip.io DNS (zero configuration required).</p> <pre><code>graph TB\n    subgraph External Access\n        Browser[Browser: https://grafana.127-0-0-1.sslip.io]\n    end\n\n    subgraph Gateway API Layer - Namespace: kube-system\n        Gateway[Gateway: idp-gateway&lt;br/&gt;Listener: HTTPS:443&lt;br/&gt;TLS: idp-wildcard-cert]\n        Cert[Certificate: idp-wildcard-cert&lt;br/&gt;*.127-0-0-1.sslip.io&lt;br/&gt;Issuer: ca-issuer]\n    end\n\n    subgraph HTTPRoutes - Distributed\n        HR1[HTTPRoute: argocd&lt;br/&gt;argocd.127-0-0-1.sslip.io]\n        HR2[HTTPRoute: grafana&lt;br/&gt;grafana.127-0-0-1.sslip.io]\n        HR3[HTTPRoute: vault&lt;br/&gt;vault.127-0-0-1.sslip.io]\n        HR4[HTTPRoute: workflows&lt;br/&gt;workflows.127-0-0-1.sslip.io]\n        HR5[HTTPRoute: sonarqube&lt;br/&gt;sonarqube.127-0-0-1.sslip.io]\n    end\n\n    subgraph Backend Services\n        S1[argocd-server:80]\n        S2[prometheus-grafana:80]\n        S3[vault:8200]\n        S4[argo-workflows-server:2746]\n        S5[sonarqube-sonarqube:9000]\n    end\n\n    Browser --&gt;|HTTPS Request| Gateway\n    Gateway --&gt;|Routes by hostname| HR1\n    Gateway --&gt;|Routes by hostname| HR2\n    Gateway --&gt;|Routes by hostname| HR3\n    Gateway --&gt;|Routes by hostname| HR4\n    Gateway --&gt;|Routes by hostname| HR5\n\n    HR1 --&gt; S1\n    HR2 --&gt; S2\n    HR3 --&gt; S3\n    HR4 --&gt; S4\n    HR5 --&gt; S5\n\n    Cert -.-&gt;|Provides TLS| Gateway</code></pre>"},{"location":"architecture/visual/#9-control-loop-overview","title":"9. Control Loop Overview","text":"<p>This diagram illustrates the continuous, cross-reconciling control loops between the core GitOps components, forming the heart of the \"Platform as a System.\" Each component watches the Kubernetes API server for changes and acts to align the cluster's actual state with the desired state defined in Git, policies, or external secret stores.</p> <pre><code>graph LR\n    K8sApi[Kubernetes API Server]\n\n    subgraph GitOps\n        ArgoCD[ArgoCD]\n    end\n\n    subgraph Policy\n        Kyverno[Kyverno]\n    end\n\n    subgraph Secrets\n        ESO[External Secrets Operator]\n    end\n\n    ArgoCD &lt;--&gt;|Reconciles Git State| K8sApi\n    Kyverno &lt;--&gt;|Validates &amp; Mutates Resources| K8sApi\n    ESO &lt;--&gt;|Syncs External Secrets| K8sApi</code></pre>"},{"location":"components/cicd/","title":"CI/CD Components","text":"<p>The CI/CD stack provides workflow orchestration and code quality analysis capabilities.</p>"},{"location":"components/cicd/#core-components","title":"Core Components","text":"<ul> <li>Argo Workflows: Kubernetes-native workflow engine for orchestrating parallel jobs</li> <li>SonarQube: Code quality and security analysis platform</li> </ul>"},{"location":"components/cicd/#cicd-capabilities","title":"CI/CD Capabilities","text":"<p>These components provide: - Workflow orchestration - Code quality analysis - Security scanning integration - Pipeline management</p>"},{"location":"components/cicd/argo-workflows/","title":"argo-workflows","text":"<p>Kubernetes-native workflow engine for orchestrating parallel jobs</p>"},{"location":"components/cicd/argo-workflows/#component-information","title":"Component Information","text":"Property Value Chart Version <code>0.45.27</code> Chart Type <code>application</code> Upstream Project argo-workflows Maintainers Platform Engineering Team (link)"},{"location":"components/cicd/argo-workflows/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/cicd/argo-workflows/#values","title":"Values","text":"Key Type Default Description artifactRepository.archiveLogs bool <code>false</code> Archive logs to the artifact repository. artifactRepository.artifactRepositoryRef object <code>{}</code> Reference to a custom artifact repository. artifactRepository.azure object <code>{}</code> Azure artifact repository configuration. artifactRepository.customArtifactRepository object <code>{}</code> Custom artifact repository configuration. artifactRepository.gcs object <code>{}</code> GCS artifact repository configuration. artifactRepository.s3 object <code>{}</code> S3 artifact repository configuration. controller.metricsConfig object <code>{\"enabled\":true}</code> Enable Prometheus metrics endpoint. controller.nodeEvents object <code>{\"enabled\":true}</code> Enable node events for workflows. controller.parallelism int <code>10</code> Maximum number of parallel workflows. controller.persistence object <code>{\"archive\":false,\"connectionPool\":{\"maxIdleConns\":0,\"maxOpenConns\":0}}</code> Persistence configuration for the controller. Completely disable archive and persistence for k3d environment controller.priorityClassName string <code>\"platform-cicd\"</code> Priority class for the controller pod. controller.rbac object <code>{\"create\":true}</code> Create RBAC resources for the controller. controller.resources object <code>{\"limits\":{\"cpu\":\"500m\",\"memory\":\"512Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"256Mi\"}}</code> Resource requests and limits for the controller. controller.retentionPolicy object <code>{\"completed\":10,\"errored\":5,\"failed\":5}</code> Retention policy for completed workflows. controller.serviceAccount object <code>{\"create\":true,\"name\":\"argo-workflow-controller\"}</code> Service account for the controller. controller.serviceMonitor object <code>{\"additionalLabels\":{\"prometheus\":\"kube-prometheus\"},\"enabled\":true}</code> Create a ServiceMonitor for Prometheus. controller.workflowDefaults object <code>{\"spec\":{\"podGC\":{\"deleteDelayDuration\":\"60s\",\"strategy\":\"OnPodCompletion\"},\"ttlStrategy\":{\"secondsAfterCompletion\":3600,\"secondsAfterFailure\":7200,\"secondsAfterSuccess\":1800}}}</code> Default TTL strategy for completed workflows. controller.workflowDefaults.spec.podGC object <code>{\"deleteDelayDuration\":\"60s\",\"strategy\":\"OnPodCompletion\"}</code> Garbage collection strategy for completed pods. controller.workflowEvents object <code>{\"enabled\":true}</code> Enable workflow events. controller.workflowNamespaces list <code>[\"cicd\"]</code> Namespaces where the controller will manage workflows. controller.workflowRestrictions object <code>{\"templateReferencing\":\"Strict\"}</code> Restrict template referencing to be within the same namespace. crds.install bool <code>true</code> Install and manage CRDs. crds.keep bool <code>true</code> Keep CRDs on chart uninstall. executor.resources object <code>{\"limits\":{\"cpu\":\"250m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"}}</code> Resource requests and limits for the executor. server.authModes list <code>[\"server\"]</code> Authentication modes for the server. server.enabled bool <code>true</code> Enable the Argo Workflows server. Required for web UI and API access server.priorityClassName string <code>\"platform-cicd\"</code> Priority class for the server pod. server.rbac object <code>{\"create\":true}</code> Create RBAC resources for the server. server.resources object <code>{\"limits\":{\"cpu\":\"250m\",\"memory\":\"256Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"128Mi\"}}</code> Resource requests and limits for the server. server.secure bool <code>false</code> Secure mode disabled when behind TLS-terminating gateway The Gateway handles TLS, so the server receives plain HTTP server.serviceAccount object <code>{\"create\":true,\"name\":\"argo-workflow-server\"}</code> Service account for the server. server.sso object <code>{\"enabled\":false}</code> Enable Single Sign-On (SSO). workflow.rbac object <code>{\"create\":true,\"rules\":[{\"apiGroups\":[\"\"],\"resources\":[\"secrets\"],\"verbs\":[\"get\"]}]}</code> RBAC resources for workflows. workflow.serviceAccount object <code>{\"create\":true,\"name\":\"argo-workflow\"}</code> Service account for workflows. workflowDefaults.spec.priorityClassName string <code>\"cicd-execution\"</code> Priority class for workflow pods. workflowDefaults.spec.serviceAccountName string <code>\"argo-workflow\"</code> Service account for workflow pods. <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/cicd/sonarqube/","title":"sonarqube","text":"<p>Code quality and security analysis platform</p>"},{"location":"components/cicd/sonarqube/#component-information","title":"Component Information","text":"Property Value Chart Version <code>2025.5.0</code> Chart Type <code>application</code> Upstream Project sonarqube Maintainers Platform Engineering Team (link)"},{"location":"components/cicd/sonarqube/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/cicd/sonarqube/#values","title":"Values","text":"Key Type Default Description community.enabled bool <code>true</code> initFs object <code>{\"enabled\":true}</code> Required initContainer to set filesystem permissions. initSysctl object <code>{\"enabled\":true}</code> Required initContainer to set kernel parameters for Elasticsearch. monitoringPasscodeSecretKey string <code>\"passcode\"</code> monitoringPasscodeSecretName string <code>\"sonarqube-monitoring-passcode\"</code> Monitoring passcode from Vault ExternalSecret. persistence.enabled bool <code>true</code> persistence.size string <code>\"2Gi\"</code> Minimal size for a demo environment. postgresql object <code>{\"priorityClassName\":\"platform-cicd\",\"resources\":{\"limits\":{\"cpu\":\"250m\",\"memory\":\"256Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}}</code> Configuration for the bundled PostgreSQL database. postgresql.resources object <code>{\"limits\":{\"cpu\":\"250m\",\"memory\":\"256Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Resource requests and limits for the PostgreSQL pod. setAdminPassword object <code>{\"currentPasswordSecretKey\":\"currentPassword\",\"currentPasswordSecretName\":\"sonarqube-admin-credentials\",\"enabled\":true,\"passwordSecretKey\":\"password\",\"passwordSecretName\":\"sonarqube-admin-credentials\"}</code> Admin password from Vault ExternalSecret. sonarqube.env list <code>[{\"name\":\"SONAR_WEB_JAVAADDITIONALOPTS\",\"value\":\"-Dsonar.web.proxyScheme=https\"}]</code> Environment variables for reverse proxy configuration Required when SonarQube is behind a reverse proxy that terminates TLS sonarqube.livenessProbe object <code>{\"exec\":{\"command\":[\"sh\",\"-c\",\"wget --no-proxy --quiet -O /dev/null --timeout=1 --header=\\\"X-Sonar-Passcode: $SONAR_WEB_SYSTEMPASSCODE\\\" \\\"http://localhost:9000/api/system/liveness\\\"\"]},\"failureThreshold\":6,\"initialDelaySeconds\":60,\"periodSeconds\":30,\"timeoutSeconds\":1}</code> Liveness probe to check if the SonarQube server is running. sonarqube.priorityClassName string <code>\"platform-cicd\"</code> sonarqube.readinessProbe object <code>{\"exec\":{\"command\":[\"sh\",\"-c\",\"if curl -s -f http://localhost:9000/api/system/status | grep -q -e '\\\"status\\\":\\\"UP\\\"' -e '\\\"status\\\":\\\"DB_MIGRATION_NEEDED\\\"' -e '\\\"status\\\":\\\"DB_MIGRATION_RUNNING\\\"'; then exit 0 fi exit 1\"]},\"failureThreshold\":6,\"initialDelaySeconds\":60,\"periodSeconds\":30,\"timeoutSeconds\":1}</code> Readiness probe to check if the SonarQube server is ready to accept traffic. sonarqube.resources object <code>{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1.5Gi\"},\"requests\":{\"cpu\":\"250m\",\"memory\":\"1Gi\"}}</code> Resource requests and limits for the SonarQube server. <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/infrastructure/","title":"Infrastructure Components","text":"<p>The infrastructure layer provides the foundational services that power the entire platform.</p>"},{"location":"components/infrastructure/#core-components","title":"Core Components","text":"<ul> <li>Cilium: eBPF-based CNI with Gateway API support and L7 proxy capabilities</li> <li>Cert Manager: Cloud-native certificate management for Kubernetes</li> <li>Vault: Secrets management and data protection platform</li> <li>External Secrets: Synchronize secrets from external sources into Kubernetes</li> <li>ArgoCD: Declarative GitOps continuous delivery for Kubernetes</li> </ul>"},{"location":"components/infrastructure/#component-details","title":"Component Details","text":"<p>Each component is thoroughly documented with: - Version information - Configuration parameters - Resource requirements - Best practices</p>"},{"location":"components/infrastructure/argocd/","title":"argocd","text":"<p>Declarative GitOps continuous delivery for Kubernetes</p>"},{"location":"components/infrastructure/argocd/#component-information","title":"Component Information","text":"Property Value Chart Version <code>latest</code> Chart Type <code>application</code> Upstream Project argocd Maintainers Platform Engineering Team (link)"},{"location":"components/infrastructure/argocd/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/infrastructure/argocd/#values","title":"Values","text":""},{"location":"components/infrastructure/argocd/#rbac","title":"RBAC","text":"Key Type Default Description createClusterRoles bool <code>true</code> Create cluster roles for ArgoCD"},{"location":"components/infrastructure/argocd/#other-values","title":"Other Values","text":"Key Type Default Description applicationSet.enabled bool <code>true</code> Enable ApplicationSet controller applicationSet.metrics.enabled bool <code>true</code> Enable metrics applicationSet.metrics.serviceMonitor.enabled bool <code>true</code> Enable ServiceMonitor applicationSet.metrics.serviceMonitor.interval string <code>\"60s\"</code> Scrape interval for template rendering applicationSet.metrics.serviceMonitor.scrapeTimeout string <code>\"40s\"</code> Scrape timeout applicationSet.priorityClassName string <code>\"platform-infrastructure\"</code> Priority class applicationSet.resources.limits.cpu string <code>\"250m\"</code> CPU limit applicationSet.resources.limits.memory string <code>\"512Mi\"</code> Memory limit applicationSet.resources.requests.cpu string <code>\"125m\"</code> CPU request applicationSet.resources.requests.memory string <code>\"256Mi\"</code> Memory request certificate.enabled bool <code>false</code> Enable certificate management (handled by cert-manager) certificate.format string <code>\"json\"</code> Global logging format certificate.level string <code>\"warn\"</code> Global logging level configs.cm.\"admin.enabled\" bool <code>true</code> Enable local admin user configs.cm.\"application.resourceTrackingMethod\" string <code>\"annotation\"</code> Resource tracking method for performance configs.cm.\"exec.enabled\" bool <code>true</code> Enable exec feature in Argo UI configs.cm.\"kustomize.buildOptions\" string <code>\"--enable-helm\"</code> Enable Helm support in Kustomize builds configs.cm.\"resource.exclusions\" string <code>\"### Network resources created by the Kubernetes control plane and excluded to reduce the number of watched events and UI clutter\\n- apiGroups:\\n  - ''\\n  - discovery.k8s.io\\n  kinds:\\n  - Endpoints\\n  - EndpointSlice\\n### Internal Kubernetes resources excluded reduce the number of watched events\\n- apiGroups:\\n  - coordination.k8s.io\\n  kinds:\\n  - Lease\\n### Internal Kubernetes Authz/Authn resources excluded reduce the number of watched events\\n- apiGroups:\\n  - authentication.k8s.io\\n  - authorization.k8s.io\\n  kinds:\\n  - SelfSubjectReview\\n  - TokenReview\\n  - LocalSubjectAccessReview\\n  - SelfSubjectAccessReview\\n  - SelfSubjectRulesReview\\n  - SubjectAccessReview\\n### Intermediate Certificate Request excluded reduce the number of watched events\\n- apiGroups:\\n  - certificates.k8s.io\\n  kinds:\\n  - CertificateSigningRequest\\n- apiGroups:\\n  - cert-manager.io\\n  kinds:\\n  - CertificateRequest\\n### Cilium internal resources excluded reduce the number of watched events and UI Clutter\\n- apiGroups:\\n  - cilium.io\\n  kinds:\\n  - CiliumIdentity\\n  - CiliumEndpoint\\n  - CiliumEndpointSlice\\n### Kyverno intermediate and reporting resources excluded reduce the number of watched events and improve performance\\n- apiGroups:\\n  - kyverno.io\\n  - reports.kyverno.io\\n  - wgpolicyk8s.io\\n  kinds:\\n  - PolicyReport\\n  - ClusterPolicyReport\\n  - EphemeralReport\\n  - ClusterEphemeralReport\\n  - AdmissionReport\\n  - ClusterAdmissionReport\\n  - BackgroundScanReport\\n  - ClusterBackgroundScanReport\\n  - UpdateRequest\\n\"</code> Exclude high-frequency resources from reconciliation configs.cm.\"statusbadge.enabled\" string <code>true</code> Enable status badges configs.cm.\"timeout.reconciliation\" string <code>\"180s\"</code> Timeout to discover new manifest versions configs.params object <code>{\"server.insecure\":true}</code> ArgoCD server command-line parameters Required for TLS termination at reverse proxy/gateway configs.params.\"server.insecure\" bool <code>true</code> Run server without TLS (required when behind TLS-terminating proxy) The Gateway handles TLS termination, so ArgoCD receives plain HTTP configs.repositories.aqua.name string <code>\"aqua\"</code> Repository name configs.repositories.aqua.type string <code>\"helm\"</code> Repository type configs.repositories.aqua.url string <code>\"https://aquasecurity.github.io/helm-charts/\"</code> Repository URL configs.repositories.argo-project.name string <code>\"argo-project\"</code> Repository name configs.repositories.argo-project.type string <code>\"helm\"</code> Repository type configs.repositories.argo-project.url string <code>\"https://argoproj.github.io/argo-helm\"</code> Repository URL configs.repositories.bitnami.name string <code>\"bitnami\"</code> Repository name configs.repositories.bitnami.type string <code>\"helm\"</code> Repository type configs.repositories.bitnami.url string <code>\"https://charts.bitnami.com/bitnami\"</code> Repository URL configs.repositories.cilium.name string <code>\"cilium\"</code> Repository name configs.repositories.cilium.type string <code>\"helm\"</code> Repository type configs.repositories.cilium.url string <code>\"https://helm.cilium.io/\"</code> Repository URL configs.repositories.external-secrets.name string <code>\"external-secrets\"</code> Repository name configs.repositories.external-secrets.type string <code>\"helm\"</code> Repository type configs.repositories.external-secrets.url string <code>\"https://charts.external-secrets.io\"</code> Repository URL configs.repositories.fluent.name string <code>\"fluent\"</code> Repository name configs.repositories.fluent.type string <code>\"helm\"</code> Repository type configs.repositories.fluent.url string <code>\"https://fluent.github.io/helm-charts\"</code> Repository URL configs.repositories.grafana.name string <code>\"grafana\"</code> Repository name configs.repositories.grafana.type string <code>\"helm\"</code> Repository type configs.repositories.grafana.url string <code>\"https://grafana.github.io/helm-charts\"</code> Repository URL configs.repositories.hashicorp.name string <code>\"hashicorp\"</code> Repository name configs.repositories.hashicorp.type string <code>\"helm\"</code> Repository type configs.repositories.hashicorp.url string <code>\"https://helm.releases.hashicorp.com\"</code> Repository URL configs.repositories.jetstack.name string <code>\"jetstack\"</code> Repository name configs.repositories.jetstack.type string <code>\"helm\"</code> Repository type configs.repositories.jetstack.url string <code>\"https://charts.jetstack.io/\"</code> Repository URL configs.repositories.kyverno.name string <code>\"kyverno\"</code> Repository name configs.repositories.kyverno.type string <code>\"helm\"</code> Repository type configs.repositories.kyverno.url string <code>\"https://kyverno.github.io/kyverno/\"</code> Repository URL configs.repositories.open-telemetry.name string <code>\"open-telemetry\"</code> Repository name configs.repositories.open-telemetry.type string <code>\"helm\"</code> Repository type configs.repositories.open-telemetry.url string <code>\"https://open-telemetry.github.io/opentelemetry-helm-charts\"</code> Repository URL configs.repositories.pixie-operator.name string <code>\"pixie-operator\"</code> Repository name configs.repositories.pixie-operator.type string <code>\"helm\"</code> Repository type configs.repositories.pixie-operator.url string <code>\"https://artifacts.px.dev/helm_charts/operator\"</code> Repository URL configs.repositories.policy-reporter.name string <code>\"policy-reporter\"</code> Repository name configs.repositories.policy-reporter.type string <code>\"helm\"</code> Repository type configs.repositories.policy-reporter.url string <code>\"https://kyverno.github.io/policy-reporter\"</code> Repository URL configs.repositories.prometheus-community.name string <code>\"prometheus-community\"</code> Repository name configs.repositories.prometheus-community.type string <code>\"helm\"</code> Repository type configs.repositories.prometheus-community.url string <code>\"https://prometheus-community.github.io/helm-charts\"</code> Repository URL configs.repositories.sonarsource.name string <code>\"sonarsource\"</code> Repository name configs.repositories.sonarsource.type string <code>\"helm\"</code> Repository type configs.repositories.sonarsource.url string <code>\"https://SonarSource.github.io/helm-chart-sonarqube\"</code> Repository URL configs.secret.argocdServerAdminPassword string Vault-generated bcrypt hash Admin password hash (managed by Vault) configs.secret.createSecret bool <code>true</code> Create secret for admin credentials controller.metrics.enabled bool <code>true</code> Enable Prometheus metrics controller.metrics.serviceMonitor.enabled bool <code>true</code> Enable ServiceMonitor for Prometheus Operator controller.metrics.serviceMonitor.interval string <code>\"30s\"</code> Scrape interval for GitOps reconciliation tracking controller.metrics.serviceMonitor.scrapeTimeout string <code>\"25s\"</code> Scrape timeout controller.priorityClassName string <code>\"platform-infrastructure\"</code> Priority class for controller pods controller.resources.limits.cpu string <code>\"1000m\"</code> CPU limit controller.resources.limits.memory string <code>\"1Gi\"</code> Memory limit controller.resources.requests.cpu string <code>\"250m\"</code> CPU request controller.resources.requests.memory string <code>\"512Mi\"</code> Memory request crds.install bool <code>true</code> Install CRDs crds.keep bool <code>true</code> Keep CRDs on chart uninstall dex.enabled bool <code>false</code> Enable Dex federated OpenID Connect provider ha.enabled bool <code>false</code> Enable High Availability mode for production deployments redis.resources.limits.cpu string <code>\"250m\"</code> CPU limit redis.resources.limits.memory string <code>\"256Mi\"</code> Memory limit redis.resources.requests.cpu string <code>\"100m\"</code> CPU request redis.resources.requests.memory string <code>\"128Mi\"</code> Memory request repoServer.metrics.enabled bool <code>true</code> Enable metrics repoServer.metrics.serviceMonitor.enabled bool <code>true</code> Enable ServiceMonitor repoServer.metrics.serviceMonitor.interval string <code>\"60s\"</code> Scrape interval for background git operations repoServer.metrics.serviceMonitor.scrapeTimeout string <code>\"40s\"</code> Scrape timeout repoServer.priorityClassName string <code>\"platform-infrastructure\"</code> Priority class repoServer.resources.limits.cpu string <code>\"500m\"</code> CPU limit repoServer.resources.limits.memory string <code>\"512Mi\"</code> Memory limit repoServer.resources.requests.cpu string <code>\"250m\"</code> CPU request repoServer.resources.requests.memory string <code>\"256Mi\"</code> Memory request server.ingress.enabled bool <code>false</code> Enable ingress server.ingress.tls bool <code>false</code> Enable TLS server.metrics.enabled bool <code>true</code> Enable metrics server.metrics.serviceMonitor.enabled bool <code>true</code> Enable ServiceMonitor server.metrics.serviceMonitor.interval string <code>\"30s\"</code> Scrape interval for user-facing API latency server.metrics.serviceMonitor.scrapeTimeout string <code>\"25s\"</code> Scrape timeout server.priorityClassName string <code>\"platform-infrastructure\"</code> Priority class for server pods server.resources.limits.cpu string <code>\"250m\"</code> CPU limit server.resources.limits.memory string <code>\"256Mi\"</code> Memory limit server.resources.requests.cpu string <code>\"125m\"</code> CPU request server.resources.requests.memory string <code>\"128Mi\"</code> Memory request <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/infrastructure/cert-manager/","title":"cert-manager","text":"<p>Cloud-native certificate management for Kubernetes</p>"},{"location":"components/infrastructure/cert-manager/#component-information","title":"Component Information","text":"Property Value Chart Version <code>latest</code> Chart Type <code>application</code> Upstream Project cert-manager Maintainers Platform Engineering Team (link)"},{"location":"components/infrastructure/cert-manager/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/infrastructure/cert-manager/#values","title":"Values","text":""},{"location":"components/infrastructure/cert-manager/#ca-injector","title":"CA Injector","text":"Key Type Default Description cainjector object <code>{\"resources\":{\"limits\":{\"cpu\":\"200m\",\"memory\":\"256Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}}</code> CA injector configuration"},{"location":"components/infrastructure/cert-manager/#custom-resource-definitions","title":"Custom Resource Definitions","text":"Key Type Default Description crds object <code>{\"enabled\":true}</code> CRD installation configuration"},{"location":"components/infrastructure/cert-manager/#observability","title":"Observability","text":"Key Type Default Description prometheus object <code>{\"enabled\":true,\"servicemonitor\":{\"enabled\":true,\"interval\":\"60s\",\"scrapeTimeout\":\"40s\"}}</code> Prometheus metrics configuration"},{"location":"components/infrastructure/cert-manager/#webhook","title":"Webhook","text":"Key Type Default Description webhook object <code>{\"livenessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/livez\",\"port\":6080,\"scheme\":\"HTTP\"},\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"readinessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/healthz\",\"port\":6080,\"scheme\":\"HTTP\"},\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{\"limits\":{\"cpu\":\"200m\",\"memory\":\"256Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}}</code> Webhook configuration"},{"location":"components/infrastructure/cert-manager/#other-values","title":"Other Values","text":"Key Type Default Description cainjector.resources object <code>{\"limits\":{\"cpu\":\"200m\",\"memory\":\"256Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Resource settings for cainjector cainjector.resources.limits object <code>{\"cpu\":\"200m\",\"memory\":\"256Mi\"}</code> Resource limits cainjector.resources.limits.cpu string <code>\"200m\"</code> CPU limit cainjector.resources.limits.memory string <code>\"256Mi\"</code> Memory limit cainjector.resources.requests object <code>{\"cpu\":\"100m\",\"memory\":\"128Mi\"}</code> Resource requests cainjector.resources.requests.cpu string <code>\"100m\"</code> CPU request cainjector.resources.requests.memory string <code>\"128Mi\"</code> Memory request crds.enabled bool <code>true</code> Enable the installation of cert-manager CRDs global.priorityClassName string <code>\"platform-infrastructure\"</code> prometheus.enabled bool <code>true</code> Enable Prometheus metrics prometheus.servicemonitor object <code>{\"enabled\":true,\"interval\":\"60s\",\"scrapeTimeout\":\"40s\"}</code> ServiceMonitor configuration prometheus.servicemonitor.enabled bool <code>true</code> Enable ServiceMonitor for cert-manager components prometheus.servicemonitor.interval string <code>\"60s\"</code> Scrape interval prometheus.servicemonitor.scrapeTimeout string <code>\"40s\"</code> Scrape timeout resources.limits object <code>{\"cpu\":\"500m\",\"memory\":\"512Mi\"}</code> Resource limits resources.limits.cpu string <code>\"500m\"</code> CPU limit resources.limits.memory string <code>\"512Mi\"</code> Memory limit resources.requests object <code>{\"cpu\":\"250m\",\"memory\":\"256Mi\"}</code> Resource requests resources.requests.cpu string <code>\"250m\"</code> CPU request resources.requests.memory string <code>\"256Mi\"</code> Memory request webhook.livenessProbe object <code>{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/livez\",\"port\":6080,\"scheme\":\"HTTP\"},\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}</code> Liveness probe for the webhook pod webhook.livenessProbe.failureThreshold int <code>3</code> Failure threshold for liveness probe webhook.livenessProbe.httpGet object <code>{\"path\":\"/livez\",\"port\":6080,\"scheme\":\"HTTP\"}</code> HTTP GET configuration webhook.livenessProbe.httpGet.path string <code>\"/livez\"</code> Liveness probe path webhook.livenessProbe.httpGet.port int <code>6080</code> Liveness probe port webhook.livenessProbe.httpGet.scheme string <code>\"HTTP\"</code> Liveness probe scheme webhook.livenessProbe.initialDelaySeconds int <code>0</code> Initial delay before liveness probe webhook.livenessProbe.periodSeconds int <code>10</code> Period between liveness probes webhook.livenessProbe.successThreshold int <code>1</code> Success threshold for liveness probe webhook.livenessProbe.timeoutSeconds int <code>1</code> Timeout for liveness probe webhook.readinessProbe object <code>{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/healthz\",\"port\":6080,\"scheme\":\"HTTP\"},\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}</code> Readiness probe for the webhook pod webhook.readinessProbe.failureThreshold int <code>3</code> Failure threshold for readiness probe webhook.readinessProbe.httpGet object <code>{\"path\":\"/healthz\",\"port\":6080,\"scheme\":\"HTTP\"}</code> HTTP GET configuration webhook.readinessProbe.httpGet.path string <code>\"/healthz\"</code> Readiness probe path webhook.readinessProbe.httpGet.port int <code>6080</code> Readiness probe port webhook.readinessProbe.httpGet.scheme string <code>\"HTTP\"</code> Readiness probe scheme webhook.readinessProbe.initialDelaySeconds int <code>0</code> Initial delay before readiness probe webhook.readinessProbe.periodSeconds int <code>10</code> Period between readiness probes webhook.readinessProbe.successThreshold int <code>1</code> Success threshold for readiness probe webhook.readinessProbe.timeoutSeconds int <code>1</code> Timeout for readiness probe webhook.resources object <code>{\"limits\":{\"cpu\":\"200m\",\"memory\":\"256Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Resource settings for the webhook webhook.resources.limits object <code>{\"cpu\":\"200m\",\"memory\":\"256Mi\"}</code> Resource limits webhook.resources.limits.cpu string <code>\"200m\"</code> CPU limit webhook.resources.limits.memory string <code>\"256Mi\"</code> Memory limit webhook.resources.requests object <code>{\"cpu\":\"100m\",\"memory\":\"128Mi\"}</code> Resource requests webhook.resources.requests.cpu string <code>\"100m\"</code> CPU request webhook.resources.requests.memory string <code>\"128Mi\"</code> Memory request <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/infrastructure/cilium/","title":"cilium","text":"<p>eBPF-based CNI with Gateway API support and L7 proxy capabilities</p>"},{"location":"components/infrastructure/cilium/#component-information","title":"Component Information","text":"Property Value Chart Version <code>latest</code> Chart Type <code>application</code> Upstream Project cilium Maintainers Platform Engineering Team (link)"},{"location":"components/infrastructure/cilium/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/infrastructure/cilium/#values","title":"Values","text":""},{"location":"components/infrastructure/cilium/#kube-proxy-replacement","title":"Kube-proxy Replacement","text":"Key Type Default Description kubeProxyReplacement bool <code>true</code> Replace kube-proxy with Cilium's eBPF implementation"},{"location":"components/infrastructure/cilium/#network-policy","title":"Network Policy","text":"Key Type Default Description policyEnforcementMode string <code>\"default\"</code> Default enforcement mode for CiliumNetworkPolicy"},{"location":"components/infrastructure/cilium/#other-values","title":"Other Values","text":"Key Type Default Description bgpControlPlane.enabled bool <code>false</code> Enable BGP (disabled for demo, used for on-prem route announcement) bpf.hostRouting bool <code>false</code> Enable BPF host routing for improved performance Disabled for k3d/Docker environments - causes DNS resolution issues Enable only for bare-metal/VM deployments bpf.masquerade bool <code>false</code> Enable BPF masquerading for traffic leaving cluster nodes Disabled for k3d/Docker environments - interferes with Docker bridge networking and causes DNS responses to be malformed (\"server misbehaving\" errors) Enable only for bare-metal/VM deployments bpf.monitorAggregation string <code>\"medium\"</code> Monitor aggregation level bpf.monitorFlags string <code>\"all\"</code> Monitor flags bpf.monitorInterval string <code>\"10s\"</code> Monitor interval cluster.id int <code>1</code> Cluster ID cluster.name string <code>\"idp-demo\"</code> Cluster name cni.chainingMode string <code>\"none\"</code> CNI chaining mode (none = complete CNI replacement) cni.exclusive bool <code>true</code> Exclusive mode (Cilium is the only CNI) commonLabels.\"app.kubernetes.io/component\" string <code>\"cni\"</code> Application component commonLabels.\"app.kubernetes.io/instance\" string <code>\"cilium-demo\"</code> Application instance commonLabels.\"app.kubernetes.io/part-of\" string <code>\"idp\"</code> Part of platform commonLabels.\"app.kubernetes.io/version\" string <code>\"1.18.2\"</code> Application version commonLabels.business-unit string <code>\"engineering\"</code> Business unit commonLabels.environment string <code>\"demo\"</code> Environment commonLabels.owner string <code>\"platform-engineer\"</code> Owner enableK8sEndpointSlice bool <code>true</code> Enable Kubernetes EndpointSlice feature encryption.enabled bool <code>false</code> Enable encryption (disabled for local demo) encryption.type string <code>\"wireguard\"</code> Encryption type envoy.enabled bool <code>true</code> Enable Envoy proxy envoy.prometheus.enabled bool <code>true</code> envoy.prometheus.serviceMonitor.enabled bool <code>true</code> externalIPs.enabled bool <code>false</code> Enable external IPs gatewayAPI.enabled bool <code>true</code> Enable Gateway API hubble.enabled bool <code>true</code> Enable Hubble hubble.metrics.enabled list <code>[\"dns:query;ignoreAAAA\",\"drop\",\"tcp\",\"flow\",\"port-distribution\",\"icmp\",\"http\"]</code> Enabled metrics for Hubble to collect hubble.relay.enabled bool <code>true</code> Enable Hubble Relay hubble.relay.replicas int <code>1</code> Number of replicas hubble.relay.resources.limits.cpu string <code>\"500m\"</code> CPU limit hubble.relay.resources.limits.memory string <code>\"256Mi\"</code> Memory limit hubble.relay.resources.requests.cpu string <code>\"100m\"</code> CPU request hubble.relay.resources.requests.memory string <code>\"128Mi\"</code> Memory request hubble.ui.enabled bool <code>true</code> Enable Hubble UI hubble.ui.ingress.enabled bool <code>false</code> Enable ingress hubble.ui.replicas int <code>1</code> Number of replicas hubble.ui.resources.limits.cpu string <code>\"100m\"</code> CPU limit hubble.ui.resources.limits.memory string <code>\"128Mi\"</code> Memory limit hubble.ui.resources.requests.cpu string <code>\"50m\"</code> CPU request hubble.ui.resources.requests.memory string <code>\"64Mi\"</code> Memory request hubble.ui.service.type string <code>\"ClusterIP\"</code> Service type ingressController.enabled bool <code>false</code> Enable the Ingress Controller ipam.mode string <code>\"cluster-pool\"</code> IPAM mode (cluster-pool recommended for efficient allocation) ipam.operator.clusterPoolIPv4PodCIDRList list <code>[\"10.42.0.0/16\"]</code> Pod CIDR for the cluster (must match k3d default) ipv6.enabled bool <code>false</code> Enable IPv6 support (disabled for demo performance) l2announcements.enabled bool <code>false</code> Enable L2 announcements l2announcements.leaseDuration string <code>\"3s\"</code> Lease duration l2announcements.leaseRenewDeadline string <code>\"1s\"</code> Lease renew deadline l2announcements.leaseRetryPeriod string <code>\"500ms\"</code> Lease retry period l7Proxy bool <code>true</code> operator.priorityClassName string <code>\"platform-infrastructure\"</code> Priority class for the Cilium operator operator.prometheus.enabled bool <code>true</code> Enable Prometheus metrics operator.prometheus.serviceMonitor.enabled bool <code>true</code> Enable ServiceMonitor operator.prometheus.serviceMonitor.interval string <code>\"30s\"</code> Scrape interval for critical CNI metrics operator.prometheus.serviceMonitor.scrapeTimeout string <code>\"25s\"</code> Scrape timeout operator.replicas int <code>1</code> Number of replicas for the operator operator.resources.limits.cpu string <code>\"500m\"</code> CPU limit operator.resources.limits.memory string <code>\"512Mi\"</code> Memory limit operator.resources.requests.cpu string <code>\"100m\"</code> CPU request operator.resources.requests.memory string <code>\"128Mi\"</code> Memory request prometheus.dashboards.enabled bool <code>true</code> Create ConfigMap with official Cilium dashboard prometheus.dashboards.namespace string <code>\"default\"</code> Namespace for dashboard ConfigMap prometheus.enabled bool <code>true</code> Enable metrics exposition prometheus.serviceMonitor.enabled bool <code>true</code> Create ServiceMonitor CRD prometheus.serviceMonitor.interval string <code>\"30s\"</code> Scrape interval for eBPF events and network flows prometheus.serviceMonitor.scrapeTimeout string <code>\"25s\"</code> Scrape timeout <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/infrastructure/external-secrets/","title":"external-secrets","text":"<p>Synchronize secrets from external sources into Kubernetes</p>"},{"location":"components/infrastructure/external-secrets/#component-information","title":"Component Information","text":"Property Value Chart Version <code>latest</code> Chart Type <code>application</code> Upstream Project external-secrets Maintainers Platform Engineering Team (link)"},{"location":"components/infrastructure/external-secrets/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/infrastructure/external-secrets/#values","title":"Values","text":""},{"location":"components/infrastructure/external-secrets/#performance","title":"Performance","text":"Key Type Default Description concurrent int <code>5</code> Concurrent ExternalSecret reconciliations for better performance"},{"location":"components/infrastructure/external-secrets/#crds","title":"CRDs","text":"Key Type Default Description installCRDs bool <code>true</code> Install CRDs. Should be true for the initial installation"},{"location":"components/infrastructure/external-secrets/#general-configuration","title":"General Configuration","text":"Key Type Default Description priorityClassName string <code>\"platform-infrastructure\"</code> Priority class for External Secrets Operator pods"},{"location":"components/infrastructure/external-secrets/#deployment","title":"Deployment","text":"Key Type Default Description replicas int <code>1</code> Number of replicas for the operator"},{"location":"components/infrastructure/external-secrets/#other-values","title":"Other Values","text":"Key Type Default Description livenessProbe.enabled bool <code>false</code> Enable liveness probe readinessProbe.enabled bool <code>true</code> Enable readiness probe readinessProbe.spec.failureThreshold int <code>3</code> Failure threshold for readiness probe readinessProbe.spec.httpGet.path string <code>\"/readyz\"</code> Readiness probe path readinessProbe.spec.httpGet.port int <code>8081</code> Readiness probe port readinessProbe.spec.initialDelaySeconds int <code>20</code> Initial delay before readiness probe readinessProbe.spec.periodSeconds int <code>10</code> Period between readiness probes readinessProbe.spec.timeoutSeconds int <code>5</code> Timeout for readiness probe resources.limits.cpu string <code>\"1000m\"</code> CPU limit resources.limits.memory string <code>\"1Gi\"</code> Memory limit resources.requests.cpu string <code>\"250m\"</code> CPU request resources.requests.memory string <code>\"256Mi\"</code> Memory request serviceAccount.create bool <code>true</code> Create service account serviceAccount.name string <code>\"external-secrets\"</code> Service account name serviceMonitor.enabled bool <code>true</code> Enable ServiceMonitor serviceMonitor.honorLabels bool <code>true</code> Honor labels from service serviceMonitor.interval string <code>\"60s\"</code> Scrape interval serviceMonitor.scrapeTimeout string <code>\"40s\"</code> Scrape timeout webhook.certManager.addInjectorAnnotations bool <code>true</code> Automatically inject CA into webhooks and CRDs webhook.certManager.cert.create bool <code>true</code> Create certificate resource webhook.certManager.cert.duration string <code>\"2160h\"</code> Certificate lifetime (90 days) webhook.certManager.cert.issuerRef.group string <code>\"cert-manager.io\"</code> Issuer group webhook.certManager.cert.issuerRef.kind string <code>\"ClusterIssuer\"</code> Issuer kind webhook.certManager.cert.issuerRef.name string <code>\"ca-issuer\"</code> Issuer name webhook.certManager.cert.privateKey.rotationPolicy string <code>\"Always\"</code> Private key rotation policy webhook.certManager.cert.renewBefore string <code>\"720h\"</code> Renew certificate 30 days before expiry webhook.certManager.enabled bool <code>true</code> Enable cert-manager for webhook certificates webhook.lookaheadInterval string <code>\"168h\"</code> Certificate validity lookahead interval (must be less than cert.renewBefore) <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/infrastructure/vault/","title":"vault","text":"<p>Secrets management and data protection platform</p>"},{"location":"components/infrastructure/vault/#component-information","title":"Component Information","text":"Property Value Chart Version <code>latest</code> Chart Type <code>application</code> Upstream Project vault Maintainers Platform Engineering Team (link)"},{"location":"components/infrastructure/vault/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/infrastructure/vault/#values","title":"Values","text":"Key Type Default Description server.dataStorage.enabled bool <code>true</code> Enable persistence server.dataStorage.size string <code>\"1Gi\"</code> Storage size server.livenessProbe.enabled bool <code>false</code> Enable liveness probe server.livenessProbe.execCommand list <code>[]</code> Exec command server.livenessProbe.failureThreshold int <code>2</code> Failure threshold server.livenessProbe.initialDelaySeconds int <code>5</code> Initial delay seconds server.livenessProbe.periodSeconds int <code>2</code> Period seconds server.livenessProbe.successThreshold int <code>1</code> Success threshold server.livenessProbe.timeoutSeconds int <code>5</code> Timeout seconds server.priorityClassName string <code>\"platform-infrastructure\"</code> Priority class for Vault pods server.readinessProbe.enabled bool <code>true</code> Enable readiness probe server.readinessProbe.failureThreshold int <code>2</code> Failure threshold server.readinessProbe.initialDelaySeconds int <code>5</code> Initial delay seconds server.readinessProbe.path string <code>\"/v1/sys/health?standbyok=true&amp;sealedcode=204&amp;uninitcode=204\"</code> Readiness probe path server.resources.limits.cpu string <code>\"500m\"</code> CPU limit server.resources.limits.memory string <code>\"512Mi\"</code> Memory limit server.resources.requests.cpu string <code>\"250m\"</code> CPU request server.resources.requests.memory string <code>\"256Mi\"</code> Memory request server.serviceMonitor.enabled bool <code>true</code> Enable ServiceMonitor server.serviceMonitor.interval string <code>\"60s\"</code> Scrape interval server.serviceMonitor.scrapeTimeout string <code>\"40s\"</code> Scrape timeout server.standalone.config string <code>\"storage \\\"raft\\\" {\\n  path    = \\\"/vault/data\\\"\\n  node_id = \\\"vault-0\\\"\\n}\\nlistener \\\"tcp\\\" {\\n  address         = \\\"0.0.0.0:8200\\\"\\n  tls_disable     = \\\"true\\\"\\n  telemetry {\\n    unauthenticated_metrics_access = true\\n  }\\n}\\ntelemetry {\\n  prometheus_retention_time = \\\"30s\\\"\\n  disable_hostname          = true\\n}\\naudit \\\"file\\\" {\\n  path = \\\"/vault/logs/audit.log\\\"\\n}\\n\"</code> HCL configuration for the Raft storage backend server.standalone.enabled bool <code>true</code> Enables standalone server configuration ui.enabled bool <code>true</code> Enable Vault UI ui.service.type string <code>\"ClusterIP\"</code> Service type <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/observability/","title":"Observability Components","text":"<p>The observability stack provides monitoring, logging, and alerting capabilities for the platform.</p>"},{"location":"components/observability/#core-components","title":"Core Components","text":"<ul> <li>Prometheus: Prometheus monitoring stack with Grafana and Alertmanager</li> <li>Grafana: Visualization and dashboard platform</li> <li>Loki: Log aggregation system designed to store and query logs</li> <li>Fluent-bit: Fast and lightweight log processor and forwarder</li> </ul>"},{"location":"components/observability/#observability-capabilities","title":"Observability Capabilities","text":"<p>These components provide: - Metrics collection and storage - Log aggregation and querying - Dashboard visualization - Alerting and notification</p>"},{"location":"components/observability/grafana/","title":"grafana","text":"<p>The leading platform for analytics and monitoring</p>"},{"location":"components/observability/grafana/#component-information","title":"Component Information","text":"Property Value Chart Type <code>application</code> Upstream Project grafana Maintainers Platform Engineering Team (link)"},{"location":"components/observability/grafana/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters from the Prometheus chart that includes Grafana:</p> Key Type Default Description grafana.\"grafana.ini\" object <code>{\"users\":{\"allow_sign_up\":false,\"default_theme\":\"dark\"}}</code> Advanced Grafana configuration via grafana.ini grafana.\"grafana.ini\".users.allow_sign_up bool <code>false</code> Disables the user sign-up page. grafana.\"grafana.ini\".users.default_theme string <code>\"dark\"</code> Set the default UI theme to dark. grafana.additionalDataSources list <code>[{\"access\":\"proxy\",\"isDefault\":false,\"name\":\"Loki\",\"type\":\"loki\",\"url\":\"http://loki.observability.svc.cluster.local:3100\"}]</code> Additional datasources for Grafana. grafana.admin object <code>{\"existingSecret\":\"grafana-admin-credentials\",\"passwordKey\":\"admin-password\",\"userKey\":\"admin-user\"}</code> Use existing secret for admin credentials from Vault via ESO. grafana.persistence object <code>{\"accessModes\":[\"ReadWriteOnce\"],\"enabled\":true,\"size\":\"1Gi\",\"type\":\"pvc\"}</code> Enable persistence for dashboards and settings grafana.plugins list <code>[\"grafana-piechart-panel\",\"grafana-polystat-panel\",\"marcusolsson-json-datasource\"]</code> Automatically install useful plugins on startup. grafana.priorityClassName string <code>\"platform-dashboards\"</code> grafana.resources.limits.cpu string <code>\"250m\"</code> CPU limit grafana.resources.limits.memory string <code>\"256Mi\"</code> Memory limit grafana.resources.requests.cpu string <code>\"50m\"</code> CPU request grafana.resources.requests.memory string <code>\"128Mi\"</code> Memory request grafana.sidecar object <code>{\"dashboards\":{\"enabled\":true,\"label\":\"grafana_dashboard\",\"labelValue\":\"\"}}</code> Sidecard to automatically discover and load dashboards from ConfigMaps. grafana.sidecar.dashboards.labelValue string <code>\"\"</code> An empty labelValue searches for the presence of the label, regardless of its value."},{"location":"components/observability/fluent-bit/","title":"fluent-bit","text":"<p>Fast and lightweight log processor and forwarder</p>"},{"location":"components/observability/fluent-bit/#component-information","title":"Component Information","text":"Property Value Chart Version <code>0.54.0</code> Chart Type <code>application</code> Upstream Project fluent-bit Maintainers Platform Engineering Team (link)"},{"location":"components/observability/fluent-bit/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/observability/fluent-bit/#values","title":"Values","text":"Key Type Default Description args list <code>[\"--workdir=/fluent-bit/etc\",\"--config=/fluent-bit/etc/conf/fluent-bit.conf\"]</code> Arguments for the command command list <code>[\"/fluent-bit/bin/fluent-bit\"]</code> Command to run config.filters string <code>\"[FILTER]\\n    Name kubernetes\\n    Match *\\n    # -- Enrich logs with Kubernetes metadata.\\n    Merge_Log On\\n    # -- Do not keep the original log after merging.\\n    Keep_Log Off\\n    # -- Allow pods to suggest a parser.\\n    K8S-Logging.Parser On\\n    # -- Allow pods to be excluded from logging.\\n    K8S-Logging.Exclude On\\n\\n# The following Lua filter removes known high-cardinality labels to protect Loki's\\n# performance. A 'deny-list' approach is used because it's more performant\\n# than a Lua-based 'allow-list' and the built-in Kubernetes filter does not\\n# support selective inclusion.\\n[FILTER]\\n    Name        lua\\n    Match       kube.*\\n    script      /fluent-bit/scripts/remove_labels.lua\\n    call        remove_labels\\n\"</code> config.inputs string <code>\"[INPUT]\\n    Name tail\\n    Path /var/log/containers/*.log\\n    multiline.parser cri\\n    Tag kube.*\\n    Mem_Buf_Limit 5MB\\n    Skip_Long_Lines On\\n    # Disable inotify to prevent \\\"too many open files\\\" errors\\n    # Use stat-based file watching instead\\n    Inotify_Watcher false\\n    DB /var/fluent-bit/state/flb_kube.db\\n    DB.Sync Normal\\n    Path_Key filename\\n    Ignore_Older 24h\\n    Threaded On\\n\"</code> config.outputs string <code>\"[OUTPUT]\\n    Name loki\\n    Match *\\n    Host loki.observability.svc.cluster.local\\n    Port 3100\\n    # -- Automatically add all Kubernetes labels to the log record.\\n    auto_kubernetes_labels true\\n    # -- Number of retries before dropping logs.\\n    Retry_Limit 5\\n\"</code> config.service string <code>\"[SERVICE]\\n    Flush {{ .Values.flush }}\\n    Log_Level {{ .Values.logLevel }}\\n    HTTP_Server On\\n    HTTP_Listen 0.0.0.0\\n    HTTP_Port {{ .Values.metricsPort }}\\n    Health_Check On\\n\"</code> daemonSetVolumeMounts list <code>[{\"mountPath\":\"/var/log\",\"name\":\"varlog\"},{\"mountPath\":\"/var/lib/docker/containers\",\"name\":\"varlibdockercontainers\",\"readOnly\":true},{\"mountPath\":\"/var/fluent-bit/state\",\"name\":\"fluentbitstate\"}]</code> DaemonSet volume mounts daemonSetVolumes list <code>[{\"hostPath\":{\"path\":\"/var/log\"},\"name\":\"varlog\"},{\"hostPath\":{\"path\":\"/var/lib/docker/containers\"},\"name\":\"varlibdockercontainers\"},{\"hostPath\":{\"path\":\"/var/fluent-bit/state\"},\"name\":\"fluentbitstate\"},{\"configMap\":{\"defaultMode\":493,\"name\":\"{{ include \\\"fluent-bit.fullname\\\" . }}-scripts\"},\"name\":\"scripts\"}]</code> DaemonSet volumes dashboards.annotations object <code>{}</code> Annotations dashboards.deterministicUid bool <code>false</code> Deterministic UID dashboards.enabled bool <code>true</code> Enable dashboards dashboards.labelKey string <code>\"grafana_dashboard\"</code> Label key for dashboards dashboards.labelValue int <code>1</code> Label value dashboards.namespace string <code>\"\"</code> Namespace flush int <code>1</code> Time to wait before flushing data (in seconds) hotReload.enabled bool <code>true</code> Enable hot reload hotReload.extraWatchVolumes list <code>[{\"mountPath\":\"/watch/scripts\",\"name\":\"scripts\"}]</code> Extra volumes to watch hotReload.image.digest string <code>nil</code> Image digest hotReload.image.pullPolicy string <code>\"IfNotPresent\"</code> Pull policy hotReload.image.repository string <code>\"ghcr.io/jimmidyson/configmap-reload\"</code> Image repository hotReload.image.tag string <code>\"v0.15.0\"</code> Image tag hotReload.resources.limits.cpu string <code>\"50m\"</code> CPU limit hotReload.resources.limits.memory string <code>\"32Mi\"</code> Memory limit hotReload.resources.requests.cpu string <code>\"10m\"</code> CPU request hotReload.resources.requests.memory string <code>\"16Mi\"</code> Memory request hotReload.securityContext.allowPrivilegeEscalation bool <code>false</code> Allow privilege escalation hotReload.securityContext.capabilities.drop list <code>[\"ALL\"]</code> Dropped capabilities hotReload.securityContext.privileged bool <code>false</code> Privileged mode hotReload.securityContext.readOnlyRootFilesystem bool <code>true</code> Read-only root filesystem hotReload.securityContext.runAsGroup int <code>65532</code> Group ID hotReload.securityContext.runAsNonRoot bool <code>true</code> Run as non-root hotReload.securityContext.runAsUser int <code>65532</code> User ID kind string <code>\"DaemonSet\"</code> DaemonSet or Deployment livenessProbe.failureThreshold int <code>3</code> Failure threshold livenessProbe.httpGet string <code>{\"path\":\"/api/v1/health\",\"port\":\"http\"}</code> Probe path livenessProbe.httpGet.path string <code>\"/api/v1/health\"</code> Health check path livenessProbe.httpGet.port string <code>\"http\"</code> Port livenessProbe.initialDelaySeconds int <code>30</code> Initial delay seconds livenessProbe.periodSeconds int <code>10</code> Period seconds livenessProbe.timeoutSeconds int <code>5</code> Timeout seconds logLevel string <code>\"info\"</code> Default log level luaScripts.\"remove_labels.lua\" string <code>\"function remove_labels(tag, timestamp, record)\\n  if record.kubernetes and record.kubernetes.labels then\\n    record.kubernetes.labels['pod-template-hash'] = nil\\n    record.kubernetes.labels['controller-revision-hash'] = nil\\n  end\\n  return 2, timestamp, record\\nend\\n\"</code> Lua script to remove high-cardinality labels metricsPort int <code>2020</code> Metrics port priorityClassName string <code>\"platform-observability\"</code> Priority class for Fluent Bit pods readinessProbe.failureThreshold int <code>3</code> Failure threshold readinessProbe.httpGet string <code>{\"path\":\"/api/v1/health\",\"port\":\"http\"}</code> Probe path readinessProbe.httpGet.path string <code>\"/api/v1/health\"</code> Health check path readinessProbe.httpGet.port string <code>\"http\"</code> Port readinessProbe.initialDelaySeconds int <code>10</code> Initial delay seconds readinessProbe.periodSeconds int <code>10</code> Period seconds readinessProbe.timeoutSeconds int <code>5</code> Timeout seconds replicaCount int <code>1</code> Only applicable if kind=Deployment resources.limits.cpu string <code>\"100m\"</code> CPU limit resources.limits.memory string <code>\"128Mi\"</code> Memory limit resources.requests.cpu string <code>\"50m\"</code> CPU request resources.requests.memory string <code>\"64Mi\"</code> Memory request serviceMonitor.enabled bool <code>false</code> Enable ServiceMonitor volumeMounts list <code>[{\"mountPath\":\"/fluent-bit/etc/conf\",\"name\":\"config\"}]</code> Volume mounts <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/observability/loki/","title":"loki","text":"<p>Log aggregation system designed to store and query logs</p>"},{"location":"components/observability/loki/#component-information","title":"Component Information","text":"Property Value Chart Version <code>6.42.0</code> Chart Type <code>application</code> Upstream Project loki Maintainers Platform Engineering Team (link)"},{"location":"components/observability/loki/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/observability/loki/#values","title":"Values","text":"Key Type Default Description backend object <code>{\"replicas\":0}</code> Disable backend replicas (not used in single-binary mode). chunksCache object <code>{\"enabled\":false}</code> Disable memcached chunks cache (not needed in SingleBinary mode). deploymentMode string <code>\"SingleBinary\"</code> Deploy Loki as a single monolithic binary. See https://grafana.com/docs/loki/latest/get-started/deployment-modes/ gateway object <code>{\"enabled\":false}</code> Disable gateway (not needed in SingleBinary mode). loki.storage object <code>{\"type\":\"filesystem\"}</code> Storage backend configuration (filesystem for demo). loki.structuredConfig object <code>{\"auth_enabled\":false,\"common\":{\"path_prefix\":\"/var/loki\",\"replication_factor\":1,\"storage\":{\"filesystem\":{\"chunks_directory\":\"/var/loki/chunks\",\"rules_directory\":\"/var/loki/rules\"}}},\"compactor\":{\"delete_request_store\":\"filesystem\",\"retention_enabled\":true,\"working_directory\":\"/var/loki/compactor\"},\"limits_config\":{\"retention_period\":\"6h\"},\"schema_config\":{\"configs\":[{\"from\":\"2024-01-01\",\"index\":{\"period\":\"24h\",\"prefix\":\"index_\"},\"object_store\":\"filesystem\",\"schema\":\"v13\",\"store\":\"tsdb\"}]},\"server\":{\"grpc_listen_port\":9095,\"http_listen_port\":3100},\"storage_config\":{\"boltdb_shipper\":{\"active_index_directory\":\"/var/loki/boltdb-shipper-active\",\"cache_location\":\"/var/loki/boltdb-shipper-cache\",\"cache_ttl\":\"24h\"},\"filesystem\":{\"directory\":\"/var/loki/chunks\"}}}</code> This section uses the modern, structured configuration format. lokiCanary object <code>{\"enabled\":false}</code> Disable canary (testing component, not needed for demo). memberlist object <code>{\"enable_ipv6\":false}</code> Disable IPv6 for the memberlist. read object <code>{\"replicas\":0}</code> Disable read replicas (not used in single-binary mode). resultsCache object <code>{\"enabled\":false}</code> Disable memcached results cache (not needed in SingleBinary mode). singleBinary.livenessProbe object <code>{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":30,\"periodSeconds\":10,\"timeoutSeconds\":1}</code> Liveness probe for the single binary pod. singleBinary.persistence object <code>{\"enabled\":true,\"size\":\"2Gi\"}</code> Persistence configuration for the single binary. singleBinary.priorityClassName string <code>\"platform-observability\"</code> singleBinary.readinessProbe object <code>{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":15,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}</code> Readiness probe for the single binary pod. singleBinary.replicas int <code>1</code> Number of replicas for the single binary. singleBinary.resources object <code>{\"limits\":{\"cpu\":\"500m\",\"memory\":\"1Gi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"512Mi\"}}</code> Resource limits and requests for Loki. test object <code>{\"enabled\":false}</code> Disable test (not needed for demo, requires canary to be enabled). write object <code>{\"replicas\":0}</code> Disable write replicas (not used in single-binary mode). <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/observability/prometheus/","title":"prometheus","text":"<p>Prometheus monitoring stack with Grafana and Alertmanager</p>"},{"location":"components/observability/prometheus/#component-information","title":"Component Information","text":"Property Value Chart Version <code>77.14.0</code> Chart Type <code>application</code> Upstream Project prometheus Maintainers Platform Engineering Team (link)"},{"location":"components/observability/prometheus/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/observability/prometheus/#values","title":"Values","text":"Key Type Default Description alertmanager.enabled bool <code>false</code> Disabled by default. This blueprint uses Grafana Unified Alerting as the primary platform for managing alerts from all datasources (Prometheus, Loki, etc.), providing a single, integrated user experience. crds object <code>{\"enabled\":false}</code> Disables the installation of CRDs, as they are managed separately. grafana.\"grafana.ini\" object <code>{\"users\":{\"allow_sign_up\":false,\"default_theme\":\"dark\"}}</code> Advanced Grafana configuration via grafana.ini grafana.\"grafana.ini\".users.allow_sign_up bool <code>false</code> Disables the user sign-up page. grafana.\"grafana.ini\".users.default_theme string <code>\"dark\"</code> Set the default UI theme to dark. grafana.additionalDataSources list <code>[{\"access\":\"proxy\",\"isDefault\":false,\"name\":\"Loki\",\"type\":\"loki\",\"url\":\"http://loki.observability.svc.cluster.local:3100\"}]</code> Additional datasources for Grafana. grafana.admin object <code>{\"existingSecret\":\"grafana-admin-credentials\",\"passwordKey\":\"admin-password\",\"userKey\":\"admin-user\"}</code> Use existing secret for admin credentials from Vault via ESO. grafana.persistence object <code>{\"accessModes\":[\"ReadWriteOnce\"],\"enabled\":true,\"size\":\"1Gi\",\"type\":\"pvc\"}</code> Enable persistence for dashboards and settings grafana.plugins list <code>[\"grafana-piechart-panel\",\"grafana-polystat-panel\",\"marcusolsson-json-datasource\"]</code> Automatically install useful plugins on startup. grafana.priorityClassName string <code>\"platform-dashboards\"</code> grafana.resources.limits.cpu string <code>\"250m\"</code> CPU limit grafana.resources.limits.memory string <code>\"256Mi\"</code> Memory limit grafana.resources.requests.cpu string <code>\"50m\"</code> CPU request grafana.resources.requests.memory string <code>\"128Mi\"</code> Memory request grafana.sidecar object <code>{\"dashboards\":{\"enabled\":true,\"label\":\"grafana_dashboard\",\"labelValue\":\"\"}}</code> Sidecar to automatically discover and load dashboards from ConfigMaps. grafana.sidecar.dashboards.labelValue string <code>\"\"</code> An empty labelValue searches for the presence of the label, regardless of its value. kube-state-metrics object <code>{\"extraArgs\":[\"--resources=cronjobs,daemonsets,deployments,jobs,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,pods,services,statefulsets,storageclasses\"],\"priorityClassName\":\"platform-observability\",\"prometheus\":{\"monitor\":{\"metricRelabelings\":[{\"action\":\"labeldrop\",\"regex\":\"uid\"},{\"action\":\"labeldrop\",\"regex\":\"container_id\"},{\"action\":\"labeldrop\",\"regex\":\"image_id\"}]}},\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"25m\",\"memory\":\"64Mi\"}}}</code> Resource limits and requests for kube-state-metrics. kube-state-metrics.extraArgs list <code>[\"--resources=cronjobs,daemonsets,deployments,jobs,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,pods,services,statefulsets,storageclasses\"]</code> Enable only relevant resource types (whitelist approach) kube-state-metrics.priorityClassName string <code>\"platform-observability\"</code> Priority class kube-state-metrics.prometheus.monitor.metricRelabelings list <code>[{\"action\":\"labeldrop\",\"regex\":\"uid\"},{\"action\":\"labeldrop\",\"regex\":\"container_id\"},{\"action\":\"labeldrop\",\"regex\":\"image_id\"}]</code> Drop high-cardinality labels kube-state-metrics.resources.limits.cpu string <code>\"50m\"</code> CPU limit kube-state-metrics.resources.limits.memory string <code>\"128Mi\"</code> Memory limit kube-state-metrics.resources.requests.cpu string <code>\"25m\"</code> CPU request kube-state-metrics.resources.requests.memory string <code>\"64Mi\"</code> Memory request prometheus-node-exporter object <code>{\"extraArgs\":[\"--collector.disable-defaults\",\"--collector.cpu\",\"--collector.cpufreq\",\"--collector.meminfo\",\"--collector.diskstats\",\"--collector.filesystem\",\"--collector.netdev\",\"--collector.loadavg\",\"--collector.pressure\",\"--collector.vmstat\",\"--collector.stat\",\"--collector.uname\"],\"priorityClassName\":\"platform-observability\",\"resources\":{\"limits\":{\"cpu\":\"30m\",\"memory\":\"48Mi\"},\"requests\":{\"cpu\":\"15m\",\"memory\":\"24Mi\"}}}</code> Resource limits and requests for the node-exporter. prometheus-node-exporter.extraArgs list <code>[\"--collector.disable-defaults\",\"--collector.cpu\",\"--collector.cpufreq\",\"--collector.meminfo\",\"--collector.diskstats\",\"--collector.filesystem\",\"--collector.netdev\",\"--collector.loadavg\",\"--collector.pressure\",\"--collector.vmstat\",\"--collector.stat\",\"--collector.uname\"]</code> Minimal collector set optimized for K3d prometheus-node-exporter.priorityClassName string <code>\"platform-observability\"</code> Priority class prometheus-node-exporter.resources.limits.cpu string <code>\"30m\"</code> CPU limit prometheus-node-exporter.resources.limits.memory string <code>\"48Mi\"</code> Memory limit prometheus-node-exporter.resources.requests.cpu string <code>\"15m\"</code> CPU request prometheus-node-exporter.resources.requests.memory string <code>\"24Mi\"</code> Memory request prometheus.priorityClassName string <code>\"platform-observability\"</code> prometheus.prometheusSpec.resources.limits.cpu string <code>\"250m\"</code> CPU limit prometheus.prometheusSpec.resources.limits.memory string <code>\"512Mi\"</code> Memory limit prometheus.prometheusSpec.resources.requests.cpu string <code>\"100m\"</code> CPU request prometheus.prometheusSpec.resources.requests.memory string <code>\"384Mi\"</code> Memory request prometheus.prometheusSpec.retention string <code>\"6h\"</code> Metrics retention time. prometheus.prometheusSpec.scrapeInterval string <code>\"60s\"</code> Global scrape interval for all ServiceMonitors (unless overridden). prometheus.prometheusSpec.scrapeTimeout string <code>\"40s\"</code> Global scrape timeout for all ServiceMonitors (unless overridden). prometheus.prometheusSpec.storageSpec object <code>{\"volumeClaimTemplate\":{\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"1Gi\"}}}}}</code> Enable persistence for Prometheus TSDB. 1Gi supports 6h retention for ~50 pods with 4x overhead margin. Data survives pod restarts but is lost on cluster destruction. prometheusOperator object <code>{\"priorityClassName\":\"platform-observability\",\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"},\"requests\":{\"cpu\":\"25m\",\"memory\":\"32Mi\"}}}</code> Resource limits and requests for the Prometheus Operator. prometheusOperator.priorityClassName string <code>\"platform-observability\"</code> Priority class prometheusOperator.resources.limits.cpu string <code>\"50m\"</code> CPU limit prometheusOperator.resources.limits.memory string <code>\"64Mi\"</code> Memory limit prometheusOperator.resources.requests.cpu string <code>\"25m\"</code> CPU request prometheusOperator.resources.requests.memory string <code>\"32Mi\"</code> Memory request windows-exporter object <code>{\"enabled\":false}</code> Disables unnecessary components. <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/policy/","title":"Policy Components","text":"<p>The policy layer enforces organizational standards and security requirements across the platform.</p>"},{"location":"components/policy/#core-components","title":"Core Components","text":"<ul> <li>Kyverno: Kubernetes-native policy management and security engine</li> <li>Policy Reporter: Monitoring and observability for policy engine results</li> </ul>"},{"location":"components/policy/#policy-capabilities","title":"Policy Capabilities","text":"<p>These components provide: - Policy enforcement as code - Compliance monitoring - Automated security checks - Admission control</p>"},{"location":"components/policy/kyverno/","title":"kyverno","text":"<p>Kubernetes-native policy management and security engine</p>"},{"location":"components/policy/kyverno/#component-information","title":"Component Information","text":"Property Value Chart Version <code>3.5.2</code> Chart Type <code>application</code> Upstream Project kyverno Maintainers Platform Engineering Team (link)"},{"location":"components/policy/kyverno/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/policy/kyverno/#values","title":"Values","text":"Key Type Default Description backgroundController.resources.limits.cpu string <code>\"250m\"</code> backgroundController.resources.limits.memory string <code>\"256Mi\"</code> backgroundController.resources.requests.cpu string <code>\"100m\"</code> backgroundController.resources.requests.memory string <code>\"128Mi\"</code> cleanupController.resources.limits.cpu string <code>\"250m\"</code> cleanupController.resources.limits.memory string <code>\"256Mi\"</code> cleanupController.resources.requests.cpu string <code>\"100m\"</code> cleanupController.resources.requests.memory string <code>\"128Mi\"</code> crds.install bool <code>true</code> livenessProbe object <code>{\"failureThreshold\":3,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}</code> Liveness probe for the main admission controller. priorityClassName string <code>\"platform-policy\"</code> Priority class for Kyverno admission controller readinessProbe object <code>{\"failureThreshold\":3,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}</code> Readiness probe for the main admission controller. reportsController.resources.limits.cpu string <code>\"250m\"</code> reportsController.resources.limits.memory string <code>\"256Mi\"</code> reportsController.resources.requests.cpu string <code>\"100m\"</code> reportsController.resources.requests.memory string <code>\"128Mi\"</code> resources object <code>{\"limits\":{\"cpu\":\"250m\",\"memory\":\"256Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Resources for the main admission controller. startupProbe object <code>{\"failureThreshold\":30,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}</code> Startup probe for the main admission controller. <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/policy/policy-reporter/","title":"policy-reporter","text":"<p>Monitoring and observability for policy engine results</p>"},{"location":"components/policy/policy-reporter/#component-information","title":"Component Information","text":"Property Value Chart Version <code>3.5.0</code> Chart Type <code>application</code> Upstream Project policy-reporter Maintainers Platform Engineering Team (link)"},{"location":"components/policy/policy-reporter/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/policy/policy-reporter/#values","title":"Values","text":"Key Type Default Description policyReporter.resources object <code>{\"limits\":{\"cpu\":\"200m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"}}</code> Resource requests and limits for the core engine. priorityClassName string <code>\"platform-observability\"</code> Priority class for Policy Reporter ui.enabled bool <code>true</code> Enables the deployment of the Policy Reporter UI. ui.resources object <code>{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"}}</code> Resource requests and limits for the UI. <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"components/security/","title":"Security Components","text":"<p>The security stack provides vulnerability scanning and compliance checking capabilities.</p>"},{"location":"components/security/#core-components","title":"Core Components","text":"<ul> <li>Trivy: Comprehensive security scanner for vulnerabilities and misconfigurations</li> </ul>"},{"location":"components/security/#security-capabilities","title":"Security Capabilities","text":"<p>These components provide: - Vulnerability scanning - Configuration auditing - Compliance checking - Security reporting</p>"},{"location":"components/security/trivy/","title":"trivy","text":"<p>Comprehensive security scanner for vulnerabilities and misconfigurations</p>"},{"location":"components/security/trivy/#component-information","title":"Component Information","text":"Property Value Chart Version <code>0.31.0</code> Chart Type <code>application</code> Upstream Project trivy Maintainers Platform Engineering Team (link)"},{"location":"components/security/trivy/#configuration-values","title":"Configuration Values","text":"<p>The following table lists the configurable parameters:</p>"},{"location":"components/security/trivy/#values","title":"Values","text":"Key Type Default Description compliance.enabled bool <code>true</code> Enable compliance reporting compliance.specs list <code>[\"k8s-pss-baseline-0.1\"]</code> Compliance specifications to run excludeNamespaces string <code>\"kube-system,argocd,cert-manager,vault-system,kyverno-system\"</code> Namespaces to be excluded from scanning operator.clusterComplianceEnabled bool <code>true</code> Enable cluster compliance scanner operator.configAuditScannerEnabled bool <code>true</code> Enable configuration audit scanner operator.exposedSecretScannerEnabled bool <code>true</code> Enable exposed secret scanner operator.infraAssessmentScannerEnabled bool <code>false</code> Disable infrastructure assessment scanner operator.rbacAssessmentScannerEnabled bool <code>true</code> Enable RBAC assessment scanner operator.vulnerabilityScannerEnabled bool <code>true</code> Enable vulnerability scanner priorityClassName string <code>\"platform-security\"</code> Priority class for Trivy pods resources.limits.cpu string <code>\"500m\"</code> CPU limit resources.limits.memory string <code>\"512Mi\"</code> Memory limit resources.requests.cpu string <code>\"100m\"</code> CPU request resources.requests.memory string <code>\"128Mi\"</code> Memory request serviceMonitor.enabled bool <code>true</code> Enable ServiceMonitor serviceMonitor.honorLabels bool <code>true</code> Honor labels on collisions targetWorkloads string <code>\"pod,replicaset,replicationcontroller,statefulset,daemonset,cronjob,job\"</code> Comma-separated list of target workloads trivy.mode string <code>\"ClientServer\"</code> Scanner mode trivy.serverURL string <code>\"http://trivy-server.security.svc:4954\"</code> Trivy Server URL trivyServer.dbUpdateInterval string <code>\"12h\"</code> Database update interval trivyServer.enabled bool <code>true</code> Enable Trivy Server deployment trivyServer.persistence.enabled bool <code>true</code> Enable persistence trivyServer.persistence.size string <code>\"1Gi\"</code> Storage size trivyServer.persistence.storageClass string <code>\"local-path\"</code> Storage class trivyServer.resources.limits.cpu string <code>\"1000m\"</code> CPU limit trivyServer.resources.limits.memory string <code>\"1Gi\"</code> Memory limit trivyServer.resources.requests.cpu string <code>\"200m\"</code> CPU request trivyServer.resources.requests.memory string <code>\"512Mi\"</code> Memory request <p>Documentation Auto-generated by helm-docs Last Updated: This file is regenerated automatically when values change</p>"},{"location":"getting-started/deployment/","title":"Getting Started - Deployment","text":"<p>This guide provides detailed information about the deployment process and what happens during platform setup.</p>"},{"location":"getting-started/deployment/#deployment-process","title":"Deployment Process","text":"<p>The <code>task deploy</code> command executes the following phases:</p>"},{"location":"getting-started/deployment/#phase-1-bootstrap-cluster","title":"Phase 1: Bootstrap Cluster","text":"<ol> <li>Create K3d Cluster: Creates a 3-node k3d cluster with optimized configuration</li> <li>Apply Namespaces: Creates all required namespaces for different components</li> <li>Bootstrap Infrastructure: Deploys core infrastructure components</li> <li>Deploy Cilium: Sets up eBPF-based networking and service mesh</li> </ol>"},{"location":"getting-started/deployment/#phase-2-secrets-certificates-management","title":"Phase 2: Secrets &amp; Certificates Management","text":"<ol> <li>Deploy Cert-Manager: Sets up certificate automation system</li> <li>Deploy Vault: Configures secrets management backend</li> <li>Deploy External Secrets: Establishes synchronization between Vault and Kubernetes</li> </ol>"},{"location":"getting-started/deployment/#phase-3-gitops-engine","title":"Phase 3: GitOps Engine","text":"<ol> <li>Deploy ArgoCD: Sets up the GitOps engine that will manage the rest of the platform</li> </ol>"},{"location":"getting-started/deployment/#phase-4-gateway-and-policy","title":"Phase 4: Gateway and Policy","text":"<ol> <li>Deploy Gateway: Configures ingress for external access</li> <li>Deploy Policy Engine: Sets up Kyverno for policy enforcement</li> </ol>"},{"location":"getting-started/deployment/#phase-5-application-stacks","title":"Phase 5: Application Stacks","text":"<ol> <li>Deploy Observability Stack: Prometheus, Grafana, Loki, and Fluent-bit</li> <li>Deploy CI/CD Stack: Argo Workflows and SonarQube</li> <li>Deploy Security Stack: Trivy for vulnerability scanning</li> </ol>"},{"location":"getting-started/deployment/#deployment-customization","title":"Deployment Customization","text":"<p>You can customize the deployment using environment variables:</p> <pre><code># Increase timeout for slower systems\ntask deploy KUBECTL_TIMEOUT=600s\n\n# Use different k3d configuration (without registry cache)\ntask deploy:nocache\n</code></pre>"},{"location":"getting-started/deployment/#resource-allocation","title":"Resource Allocation","text":"<p>The platform intelligently allocates resources using PriorityClasses: - <code>platform-infrastructure</code>: For core infrastructure components - <code>platform-observability</code>: For monitoring and observability tools - <code>platform-cicd</code>: For CI/CD workloads - <code>platform-security</code>: For security tools - <code>platform-policy</code>: For policy enforcement - <code>platform-dashboards</code>: For dashboard tools</p>"},{"location":"getting-started/deployment/#post-deployment","title":"Post-Deployment","text":"<p>After successful deployment: 1. All components will be visible in ArgoCD 2. Services will be accessible via the nip.io addresses 3. Default credentials will be available in Vault 4. Monitoring dashboards will start showing metrics 5. Security scanning will begin automatically</p>"},{"location":"getting-started/deployment/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during deployment: 1. Check the Troubleshooting Reference 2. Verify your prerequisites are met 3. Ensure Docker is running and not rate-limited 4. Check that sufficient resources are available on your system</p>"},{"location":"getting-started/overview/","title":"Getting Started Overview","text":"<p>Welcome to the IDP Blueprint! This section will help you get up and running with your own Internal Developer Platform.</p>"},{"location":"getting-started/overview/#what-youll-learn","title":"What You'll Learn","text":"<p>In this getting started section, you'll learn:</p> <ul> <li>Prerequisites: What tools and resources you need before starting</li> <li>Quick Start: How to deploy the platform with a single command</li> <li>Deployment Process: Understanding what happens during the deployment</li> </ul>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<p>Start with the Prerequisites to ensure your environment is ready, then move on to the Quick Start guide to deploy your first IDP instance.</p>"},{"location":"getting-started/prerequisites/","title":"Getting Started - Prerequisites","text":"<p>Before deploying the IDP Blueprint, ensure your system meets the following requirements.</p>"},{"location":"getting-started/prerequisites/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/prerequisites/#minimum-hardware","title":"Minimum Hardware","text":"<ul> <li>CPU: 4 cores (6+ recommended)</li> <li>Memory: 8GB RAM (12GB+ recommended)</li> <li>Storage: 20GB available disk space</li> <li>OS: Linux, macOS, or Windows with WSL2</li> </ul>"},{"location":"getting-started/prerequisites/#software-dependencies","title":"Software Dependencies","text":"<p>The following software must be installed:</p> <ul> <li>Docker: With Docker Hub login (<code>docker login</code>)</li> <li>Git: Version 2.0 or higher</li> <li>Visual Studio Code: With Dev Containers extension</li> <li>Docker Desktop: For macOS/Windows users</li> </ul> <p>Note: This project uses VS Code Dev Containers to provide a pre-configured environment with all required tools (kubectl, helm, k3d, task, etc.).</p>"},{"location":"getting-started/prerequisites/#docker-hub-authentication","title":"Docker Hub Authentication","text":"<p>To avoid severe rate limiting from Docker Hub:</p> <ol> <li>Create a Docker Hub account if you don't have one</li> <li>Run <code>docker login</code> and authenticate with your credentials</li> </ol>"},{"location":"getting-started/prerequisites/#network-requirements","title":"Network Requirements","text":"<ul> <li>Internet access for pulling container images</li> <li>Port availability for services (80, 443, 30080, 30443 by default)</li> </ul>"},{"location":"getting-started/prerequisites/#optional-requirements","title":"Optional Requirements","text":"<p>For enhanced functionality: - A modern web browser with developer tools - An IDE or text editor of your choice</p>"},{"location":"getting-started/quickstart/","title":"Getting Started - Quick Start","text":"<p>Get your IDP Blueprint up and running in minutes with this quick start guide.</p>"},{"location":"getting-started/quickstart/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/rou-cru/idp-blueprint\ncd idp-blueprint\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-open-in-vs-code-dev-container","title":"Step 2: Open in VS Code Dev Container","text":"<pre><code>code .\n</code></pre> <p>When prompted, click \"Reopen in Container\" to start the development environment.</p> <p>Note: The first time you do this, it will take a few minutes to download and set up the development environment.</p>"},{"location":"getting-started/quickstart/#step-3-deploy-the-platform","title":"Step 3: Deploy the Platform","text":"<p>Once inside the Dev Container, run:</p> <pre><code>task deploy\n</code></pre> <p>Time: Deployment takes approximately 5-10 minutes depending on your system and internet connection.</p>"},{"location":"getting-started/quickstart/#step-4-access-the-platform","title":"Step 4: Access the Platform","text":"<p>After deployment completes, you can access the platform components:</p> <ul> <li>ArgoCD: <code>https://argocd.&lt;your-ip&gt;.nip.io</code></li> <li>Grafana: <code>https://grafana.&lt;your-ip&gt;.nip.io</code></li> <li>Vault: <code>https://vault.&lt;your-ip&gt;.nip.io</code></li> <li>SonarQube: <code>https://sonarqube.&lt;your-ip&gt;.nip.io</code></li> <li>Argo Workflows: <code>https://workflows.&lt;your-ip&gt;.nip.io</code></li> </ul> <p>Default credentials will be available in your local Vault instance.</p>"},{"location":"getting-started/quickstart/#step-5-explore","title":"Step 5: Explore","text":"<ul> <li>Check the running components in ArgoCD</li> <li>View metrics and logs in Grafana</li> <li>Run a sample workflow in Argo Workflows</li> <li>Try out SonarQube analysis</li> </ul>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Visit the Deployment guide for more detailed information about the deployment process</li> <li>Check out the Architecture to understand how the platform is structured</li> <li>Explore the platform Components including infrastructure, observability, and security stacks</li> </ul>"},{"location":"guides/contributing/","title":"Contribution Guidelines","text":"<p>This document outlines the conventions and best practices to follow when contributing to this project. Adhering to these guidelines ensures consistency, quality, and maintainability.</p>"},{"location":"guides/contributing/#development-environment","title":"Development Environment","text":"<p>This project uses a fully automated development environment based on VS Code Dev Containers and Devbox. This approach ensures that all contributors use the exact same tooling and dependencies, which are defined as code in the repository.</p> <p>All required tools (linters, <code>kubectl</code>, <code>helm</code>, etc.) are automatically installed when the container starts. Please refer to the Quick Start guide for instructions on how to launch the environment.</p>"},{"location":"guides/contributing/#code-style-and-quality-checks","title":"Code Style and Quality Checks","text":"<p>We use a variety of linters and validation tools to maintain code quality and consistency. These tools are managed via <code>devbox</code> and are orchestrated into simple commands using <code>Task</code>.</p> <p>Before submitting any changes, please run the following checks:</p> <ul> <li><code>task lint</code>: Runs all linters for YAML, Markdown, shell scripts, and Dockerfiles.</li> <li><code>task check</code>: A comprehensive command that runs all linters, manifest validation   (<code>kustomize build</code>, <code>kubeval</code>), and security scans (<code>checkov</code>, <code>trufflehog</code>).</li> </ul> <p>Running <code>task check</code> is highly recommended to ensure your contribution passes all quality gates.</p>"},{"location":"guides/contributing/#architectural-conventions","title":"Architectural Conventions","text":"<p>To maintain a clear and scalable structure, the project follows specific architectural conventions. Before adding or modifying files, please read the architecture documentation:</p> <ul> <li>Infrastructure Layer: Describes the structure of the   bootstrap layer (in the <code>IT/</code> directory), which includes core components like Cilium CNI,   Vault, and certificate management</li> <li>Application Layer: Describes the GitOps structure   (in the <code>K8s/</code> directory) for all application stacks managed by ArgoCD, including the   \"App of AppSets\" pattern</li> </ul>"},{"location":"guides/contributing/#kubernetes-manifest-conventions","title":"Kubernetes Manifest Conventions","text":"<p>To ensure all Kubernetes resources are explicit, consistent, and easy to manage, please adhere to the following rules when creating or modifying YAML manifests.</p>"},{"location":"guides/contributing/#defining-requests-and-limits","title":"Defining Requests and Limits","text":"<ol> <li> <p>Mandatory Definition: All deployed workloads (Deployments, StatefulSets,     etc.) MUST define both <code>requests</code> and <code>limits</code> for CPU and memory. This is     critical for ensuring cluster stability, predictable performance, and proper     node scheduling.</p> </li> <li> <p>CPU Limiting Philosophy: This project enforces CPU limits. While there is an     ongoing debate in the Kubernetes community about the effects of CPU limits     (potential for throttling), this blueprint prioritizes predictable resource     allocation and total capacity planning. By setting limits, we can provide     clear hardware recommendations to users and prevent any single workload from     starving others in a resource-constrained local environment.</p> </li> </ol> <p>Note on Exceptions: The only intentional exception to this rule is the <code>cilium-agent</code>. As a privileged DaemonSet critical for all node networking, applying resource limits can be counter-productive and risk node stability. Therefore, it is intentionally deployed without CPU or memory limits.</p>"},{"location":"guides/contributing/#resource-units","title":"Resource Units","text":"<p>All CPU and memory resource values (<code>requests</code> and <code>limits</code>) MUST explicitly include their units. Unitless integer values are not permitted as they are ambiguous.</p> <ul> <li>CPU: Use millicores (e.g., <code>500m</code> instead of <code>0.5</code>).</li> <li>Memory: Use Mebibytes or Gibibytes (e.g., <code>512Mi</code>, <code>2Gi</code>).</li> </ul> <p>Example:</p> <pre><code>resources:\n  requests:\n    cpu: 100m\n    memory: 256Mi\n  limits:\n    cpu: 1000m\n    memory: 1Gi\n</code></pre>"},{"location":"guides/contributing/#helm-chart-values-documentation","title":"Helm Chart Values Documentation","text":""},{"location":"guides/contributing/#justification","title":"Justification","text":"<p>To maintain consistency and enable the automatic generation of documentation from comments, all <code>values.yaml</code> files for Helm charts in this project should follow the <code>helm-docs</code> syntax.</p> <p>This allows us to treat the <code>values.yaml</code> file as the single source of truth for configuration, from which user-facing documentation can be generated automatically, and honestly, can be delegated that part to Gemini, Claude, ChatGPT or the LLM that you like!</p> <ul> <li>Official Documentation: helm-docs on GitHub</li> </ul>"},{"location":"guides/contributing/#syntax-usage","title":"Syntax Usage","text":"<p>The following conventions are the most common and are required for this project:</p> <ol> <li> <p>Sections (<code>## @section &lt;NAME&gt;</code>): Used to create major logical groups for    parameters. This helps structure the generated documentation.</p> </li> <li> <p>Section Descriptions (<code>## @description &lt;TEXT&gt;</code>): Used to provide a more detailed    explanation for a section.</p> </li> <li> <p>Parameter Comments (<code># -- &lt;DESCRIPTION&gt;</code>): This is the most important convention.    Any comment directly above a parameter that should be included in the documentation    must start with <code># --</code>.</p> </li> <li> <p>Default Values (<code># @default -- &lt;VALUE&gt;</code>): Optionally, you can specify the default    value using this annotation. It is good practice to include it.</p> </li> </ol>"},{"location":"guides/contributing/#label-standards","title":"Label Standards","text":"<p>This project enforces strict label standards to ensure consistency, policy compliance, and resource discoverability across all Kubernetes resources.</p>"},{"location":"guides/contributing/#required-documentation","title":"Required Documentation","text":"<p>All contributors MUST read and follow the standards defined in:</p> <ul> <li>Kubernetes Labeling Standards: Complete label standards,   canonical values, priority classes, and conventions</li> </ul>"},{"location":"guides/contributing/#quick-reference","title":"Quick Reference","text":"<p>Canonical Label Values:</p> <ul> <li><code>owner: platform-team</code></li> <li><code>business-unit: infrastructure</code></li> <li><code>environment: demo</code></li> <li><code>app.kubernetes.io/part-of: idp</code></li> </ul> <p>Namespace Requirements (Enforced by Kyverno): All namespaces MUST include: <code>app.kubernetes.io/part-of</code>, <code>owner</code>, <code>business-unit</code>, and <code>environment</code> labels.</p> <p>Comment Style for Values Files: Use <code># @section -- Section Name</code> (single hash with double dash) for consistency with helm-docs.</p>"},{"location":"guides/contributing/#validation","title":"Validation","text":"<p>Before submitting changes:</p> <ol> <li>Run <code>task lint</code> to validate YAML syntax</li> <li>Run <code>kustomize build &lt;directory&gt;</code> to verify kustomization files</li> <li>Check that labels comply with Kyverno policies defined in <code>Policies/rules/</code></li> </ol>"},{"location":"guides/contributing/#commit-hygiene-and-git-bisect","title":"Commit Hygiene and <code>git bisect</code>","text":"<p>This project values the use of atomic commits. Each commit must represent a single, logical, and complete change. While it can be tempting to group many changes into a single commit before pushing, this practice should be avoided as it severely harms the project's maintainability and code review efficiency.</p> <p>The primary reason for requiring atomic commits is to enable the effective use of <code>git bisect</code>, a powerful tool for finding which commit introduced a bug.</p> <ul> <li>With Atomic Commits: <code>git bisect</code> can test each small change in isolation,   precisely identifying the culprit.</li> <li>With Large Commits: If a commit contains 5 different changes, <code>git bisect</code> can only   tell us that the bug is \"somewhere in that giant commit\",   which is useless and forces manual debugging.</li> </ul> <p>Additionally, atomic commits improve:</p> <ul> <li>Easier Code Reviews: They are faster and easier to review.</li> <li>Safer Reverts: They allow a specific change to be reverted without   affecting other functionalities.</li> </ul>"},{"location":"guides/overview/","title":"Guides Overview","text":"<p>This section provides comprehensive guides for working with the IDP Blueprint platform.</p>"},{"location":"guides/overview/#available-guides","title":"Available Guides","text":"<ul> <li>Contributing: Guidelines for contributing to the project</li> <li>Labels Standard: Standards for Kubernetes labeling</li> <li>Policy Tagging: Guidelines for tagging resources with policies</li> </ul>"},{"location":"guides/overview/#getting-help","title":"Getting Help","text":"<p>If you need assistance with any aspect of the platform, these guides provide detailed information about best practices, standards, and procedures.</p>"},{"location":"guides/policy-tagging/","title":"Kubernetes Labeling Standards for Policies","text":"<p>Note: This guide has been consolidated into the complete labeling standards documentation. For comprehensive information about labels, annotations, priority classes, sync waves, and validation, please see the Kubernetes Labeling Standards.</p>"},{"location":"guides/policy-tagging/#overview","title":"Overview","text":"<p>All resources in the IDP Blueprint platform use standardized labels to ensure:</p> <ul> <li>Operational consistency across all components</li> <li>Policy enforcement via Kyverno</li> <li>Resource governance with quotas and limits</li> <li>GitOps automation with ArgoCD sync waves</li> <li>Observability and monitoring</li> </ul>"},{"location":"guides/policy-tagging/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/policy-tagging/#business-labels-required-on-namespaces","title":"Business Labels (Required on Namespaces)","text":"Label Value Purpose <code>owner</code> <code>platform-team</code> Team responsible for the resource <code>business-unit</code> <code>infrastructure</code> Business unit for cost allocation <code>environment</code> <code>demo</code> Environment classification"},{"location":"guides/policy-tagging/#application-labels-kubernetes-recommended","title":"Application Labels (Kubernetes Recommended)","text":"Label Example Purpose <code>app.kubernetes.io/part-of</code> <code>idp</code> Parent application/platform <code>app.kubernetes.io/name</code> <code>vault</code> Component name <code>app.kubernetes.io/instance</code> <code>vault-demo</code> Instance identifier <code>app.kubernetes.io/version</code> <code>1.15.0</code> Version of the component <code>app.kubernetes.io/component</code> <code>database</code> Role within the architecture"},{"location":"guides/policy-tagging/#using-labels-with-policies","title":"Using Labels with Policies","text":"<p>When creating Kyverno policies that leverage these labels:</p> <ol> <li> <p>Namespace Propagation: Labels defined on namespaces are automatically propagated    to workloads by Kyverno policies (<code>Policies/rules/enforce-namespace-labels.yaml</code>)</p> </li> <li> <p>Validation: Policies validate that required labels are present</p> </li> <li><code>enforce-namespace-labels.yaml</code>: Enforces business labels on namespaces</li> <li> <p><code>require-component-labels.yaml</code>: Audits application labels on workloads</p> </li> <li> <p>Label Selectors: Use label selectors in policy rules to target specific resources:    <pre><code>match:\n  any:\n    - resources:\n        kinds:\n          - Deployment\n        selector:\n          matchLabels:\n            app.kubernetes.io/part-of: idp\n</code></pre></p> </li> </ol>"},{"location":"guides/policy-tagging/#complete-documentation","title":"Complete Documentation","text":"<p>For the full specification including:</p> <ul> <li>Priority Classes Assignment for resource scheduling</li> <li>External Secrets RefreshInterval Strategy for secret synchronization</li> <li>ArgoCD Sync Wave Annotations for deployment ordering</li> <li>Complete Validation Rules with Kyverno examples</li> <li>Label propagation details</li> </ul> <p>Please refer to the Kubernetes Labeling Standards documentation.</p>"},{"location":"guides/policy-tagging/#see-also","title":"See Also","text":"<ul> <li>Architecture: Applications - How labels are used in GitOps</li> <li>Kyverno Policies - Policy enforcement details</li> <li>Contributing Guide - How to contribute with proper labeling</li> </ul>"},{"location":"reference/labels-standard/","title":"Label Standards","text":"<p>This document defines the canonical label values and standards used across the IDP Blueprint repository.</p>"},{"location":"reference/labels-standard/#canonical-label-values","title":"Canonical Label Values","text":"<p>All resources in the platform should use these standard values for consistency:</p>"},{"location":"reference/labels-standard/#business-labels","title":"Business Labels","text":"<ul> <li>owner: <code>platform-team</code></li> <li>business-unit: <code>infrastructure</code></li> <li>environment: <code>demo</code></li> </ul>"},{"location":"reference/labels-standard/#application-labels","title":"Application Labels","text":"<ul> <li>app.kubernetes.io/part-of: <code>idp</code></li> </ul>"},{"location":"reference/labels-standard/#label-requirements-by-resource-type","title":"Label Requirements by Resource Type","text":""},{"location":"reference/labels-standard/#namespaces-required-by-kyverno-policy-enforce-namespace-labels","title":"Namespaces (Required by Kyverno Policy: enforce-namespace-labels)","text":"<p>All namespaces MUST include: <pre><code>labels:\n  app.kubernetes.io/part-of: idp\n  owner: platform-team\n  business-unit: infrastructure\n  environment: demo\n</code></pre></p>"},{"location":"reference/labels-standard/#workloads-audited-by-kyverno-policy-require-component-labels","title":"Workloads (Audited by Kyverno Policy: require-component-labels)","text":"<p>Deployments, StatefulSets, and DaemonSets SHOULD include: <pre><code>labels:\n  app.kubernetes.io/name: &lt;component-name&gt;\n  app.kubernetes.io/instance: &lt;instance-name&gt;\n  app.kubernetes.io/version: &lt;version&gt;\n  app.kubernetes.io/component: &lt;component-type&gt;\n</code></pre></p>"},{"location":"reference/labels-standard/#other-resources","title":"Other Resources","text":"<p>All other Kubernetes resources SHOULD include at minimum: <pre><code>labels:\n  app.kubernetes.io/part-of: idp\n  app.kubernetes.io/component: &lt;component-type&gt;\n</code></pre></p>"},{"location":"reference/labels-standard/#comment-style-for-values-files","title":"Comment Style for Values Files","text":"<p>Standard: <code># @section -- Section Name</code></p> <p>Example: <pre><code># @section -- Global Configuration\n# @description Global settings for the component\n\n# -- Enable high availability mode\nha:\n  enabled: false\n</code></pre></p> <p>Rationale: This style is compatible with helm-docs and provides consistent documentation generation.</p>"},{"location":"reference/labels-standard/#priority-classes-assignment","title":"Priority Classes Assignment","text":"<p>Priority classes should be assigned based on component criticality:</p>"},{"location":"reference/labels-standard/#platform-critical-value-1000000000","title":"platform-critical (Value: 1000000000)","text":"<p>Reserved for system-critical components.</p>"},{"location":"reference/labels-standard/#platform-infrastructure-value-900000","title":"platform-infrastructure (Value: 900000)","text":"<ul> <li>argocd</li> <li>cert-manager</li> <li>vault</li> <li>external-secrets</li> <li>kyverno</li> </ul>"},{"location":"reference/labels-standard/#platform-observability-value-800000","title":"platform-observability (Value: 800000)","text":"<ul> <li>prometheus</li> <li>grafana</li> <li>loki</li> <li>fluent-bit</li> <li>policy-reporter</li> </ul>"},{"location":"reference/labels-standard/#platform-cicd-value-700000","title":"platform-cicd (Value: 700000)","text":"<ul> <li>jenkins</li> <li>sonarqube</li> <li>argo-workflows</li> </ul>"},{"location":"reference/labels-standard/#platform-security-value-750000","title":"platform-security (Value: 750000)","text":"<ul> <li>trivy</li> </ul>"},{"location":"reference/labels-standard/#platform-default-value-0","title":"platform-default (Value: 0)","text":"<p>Default for application workloads.</p>"},{"location":"reference/labels-standard/#external-secrets-refreshinterval-strategy","title":"External Secrets RefreshInterval Strategy","text":"Interval Use Case Examples Rationale 1h Rarely-changed secrets ArgoCD admin password Minimize API calls to Vault 5m Infrastructure secrets Certificate credentials Balance between freshness and load 3m Application secrets SonarQube tokens, Grafana credentials Higher change frequency <p>Guidelines: - Use <code>1h</code> for bootstrap/admin secrets that are manually rotated - Use <code>5m</code> for infrastructure components (default for most cases) - Use <code>3m</code> for application-level secrets that may rotate programmatically - Never use <code>&lt;1m</code> to avoid overwhelming Vault API</p>"},{"location":"reference/labels-standard/#argocd-sync-wave-annotations","title":"ArgoCD Sync Wave Annotations","text":"<p>Sync waves control deployment order in ArgoCD:</p> Wave Resources Purpose -3 IT namespaces Bootstrap namespaces for infrastructure -2 K8s governance namespaces Application namespaces with resource quotas -1 Priority classes, RBAC Platform-wide configurations 0 Standard applications Default deployment order"},{"location":"reference/labels-standard/#annotations","title":"Annotations","text":""},{"location":"reference/labels-standard/#common-annotations","title":"Common Annotations","text":"<pre><code>annotations:\n  contact: platform-team\n  documentation: https://github.com/rou-cru/idp-blueprint\n  description: \"&lt;Brief description of the resource&gt;\"\n</code></pre>"},{"location":"reference/labels-standard/#argocd-specific-annotations","title":"ArgoCD-specific Annotations","text":"<pre><code>annotations:\n  argocd.argoproj.io/sync-wave: \"&lt;wave-number&gt;\"\n  argocd.argoproj.io/sync-options: \"SkipDryRunOnMissingResource=true\"\n</code></pre>"},{"location":"reference/labels-standard/#validation","title":"Validation","text":"<p>All changes should be validated against: 1. Kyverno policies in <code>Policies/rules/</code> 2. Kustomize build: <code>kustomize build &lt;directory&gt;</code> 3. Helm lint: <code>helm lint --values &lt;values-file&gt;</code> 4. The validation script: <code>scripts/validate-consistency.sh</code></p>"},{"location":"reference/labels-standard/#references","title":"References","text":"<ul> <li>Kubernetes Recommended Labels</li> <li>Kyverno Best Practices</li> <li>ArgoCD Sync Waves</li> </ul>"},{"location":"reference/overview/","title":"Reference Overview","text":"<p>This section provides reference materials for the IDP Blueprint platform.</p>"},{"location":"reference/overview/#available-references","title":"Available References","text":"<ul> <li>Resource Requirements: Detailed resource requirements for the platform</li> <li>Troubleshooting: Common issues and their solutions</li> <li>Label Standards: Standards for Kubernetes resource labeling</li> </ul>"},{"location":"reference/overview/#additional-resources","title":"Additional Resources","text":"<p>For additional technical references, see the specific component documentation in their respective sections.</p>"},{"location":"reference/resource-requirements/","title":"Resource Requirements","text":"<p>Understanding the resource requirements for the IDP Blueprint platform is crucial for proper planning and operation.</p>"},{"location":"reference/resource-requirements/#minimum-system-requirements","title":"Minimum System Requirements","text":""},{"location":"reference/resource-requirements/#hardware","title":"Hardware","text":"<ul> <li>CPU: 4 cores minimum (6+ recommended)</li> <li>Memory: 8GB RAM minimum (12GB+ recommended)</li> <li>Storage: 20GB available disk space</li> <li>Network: Stable internet connection for pulling images</li> </ul>"},{"location":"reference/resource-requirements/#software","title":"Software","text":"<ul> <li>Docker with authenticated access to Docker Hub</li> <li>Git version 2.0 or higher</li> <li>Visual Studio Code with Dev Containers extension</li> </ul>"},{"location":"reference/resource-requirements/#platform-resource-footprint","title":"Platform Resource Footprint","text":""},{"location":"reference/resource-requirements/#theoretical-minimum-total-requests","title":"Theoretical Minimum (Total Requests)","text":"Resource Total Requested CPU ~3.5 cores Memory ~5.4 GiB"},{"location":"reference/resource-requirements/#theoretical-maximum-total-limits","title":"Theoretical Maximum (Total Limits)","text":"Resource Total Limited CPU ~8.9 cores Memory ~11 GiB"},{"location":"reference/resource-requirements/#component-resource-requirements","title":"Component Resource Requirements","text":""},{"location":"reference/resource-requirements/#infrastructure-layer-node-2","title":"Infrastructure Layer (Node 2)","text":"<ul> <li>Cilium: 0.5-1.0 cores CPU, 0.5-1.0 GiB RAM</li> <li>Cert-Manager: 0.3-0.5 cores CPU, 0.2-0.5 GiB RAM</li> <li>Vault: 0.5-1.0 cores CPU, 0.3-0.5 GiB RAM</li> <li>External Secrets Operator: 0.2-0.3 cores CPU, 0.1-0.3 GiB RAM</li> <li>ArgoCD: 1.0-1.5 cores CPU, 0.8-1.2 GiB RAM</li> </ul>"},{"location":"reference/resource-requirements/#gitops-workloads-node-3","title":"GitOps Workloads (Node 3)","text":"<ul> <li>Kyverno: 0.5-1.0 cores CPU, 0.5-1.0 GiB RAM</li> <li>Prometheus: 0.5-1.0 cores CPU, 1.0-2.0 GiB RAM</li> <li>Grafana: 0.1-0.3 cores CPU, 0.1-0.3 GiB RAM</li> <li>Loki: 0.5-1.0 cores CPU, 0.5-1.5 GiB RAM</li> <li>Fluent-bit: 0.2-0.5 cores CPU, 0.2-0.5 GiB RAM</li> <li>Argo Workflows: 0.5-1.0 cores CPU, 0.5-1.0 GiB RAM</li> <li>SonarQube: 1.0-2.0 cores CPU, 1.0-2.0 GiB RAM</li> <li>Trivy: 0.5-1.0 cores CPU, 0.5-1.0 GiB RAM</li> </ul>"},{"location":"reference/resource-requirements/#memory-optimization-tips","title":"Memory Optimization Tips","text":""},{"location":"reference/resource-requirements/#for-limited-resources","title":"For Limited Resources","text":"<ol> <li>Scale down non-critical components during development</li> <li>Adjust resource limits in the Helm value files</li> <li>Use node selectors to distribute workloads</li> <li>Consider disabling non-essential components during development</li> </ol>"},{"location":"reference/resource-requirements/#priority-classes-used","title":"Priority Classes Used","text":"<p>The platform uses several priority classes to ensure critical components are scheduled: - <code>platform-infrastructure</code>: Highest priority for core components - <code>platform-observability</code>: For monitoring tools - <code>platform-cicd</code>: For CI/CD workloads - <code>platform-security</code>: For security tools - <code>platform-policy</code>: For policy enforcement</p>"},{"location":"reference/resource-requirements/#storage-requirements","title":"Storage Requirements","text":""},{"location":"reference/resource-requirements/#kubernetes-volumes","title":"Kubernetes Volumes","text":"<ul> <li>Vault Data: 1Gi persistent storage</li> <li>Prometheus Data: 10Gi persistent storage</li> <li>Loki Data: 2Gi persistent storage</li> <li>SonarQube Data: 2Gi persistent storage</li> <li>Fluent-bit State: Small persistent storage for log states</li> </ul>"},{"location":"reference/resource-requirements/#docker-images","title":"Docker Images","text":"<ul> <li>Initial download: ~8-10GB depending on available cache</li> <li>Local registry cache: ~5-8GB after deployment</li> <li>Temporary build artifacts: ~1-2GB</li> </ul>"},{"location":"reference/resource-requirements/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/resource-requirements/#for-development","title":"For Development","text":"<ul> <li>The platform is optimized for resource efficiency</li> <li>Components have been tuned for local development</li> <li>Non-essential features may be disabled by default</li> </ul>"},{"location":"reference/resource-requirements/#for-production-like-testing","title":"For Production-like Testing","text":"<ul> <li>Resource requirements may be increased</li> <li>Additional replicas may be configured</li> <li>Persistent storage may be expanded</li> </ul> <p>Note: These values exclude k3d control plane overhead and OS requirements. Real-world usage may vary based on actual workloads.</p>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":"<p>This guide covers common issues and their solutions when working with the IDP Blueprint platform.</p>"},{"location":"reference/troubleshooting/#common-deployment-issues","title":"Common Deployment Issues","text":""},{"location":"reference/troubleshooting/#docker-rate-limiting","title":"Docker Rate Limiting","text":"<p>Symptoms:  - Pull errors when starting containers - \"toomanyrequests\" errors - Failed deployments</p> <p>Solution: 1. Ensure you are logged into Docker Hub: <code>docker login</code> 2. If using a free account, consider upgrading for higher rate limits 3. Pre-pull commonly used images: <code>docker pull &lt;image&gt;</code></p>"},{"location":"reference/troubleshooting/#insufficient-resources","title":"Insufficient Resources","text":"<p>Symptoms: - Deployments timing out - Pods stuck in \"Pending\" state - \"Insufficient memory\" or \"Insufficient CPU\" errors</p> <p>Solution: 1. Check system resources: <code>htop</code> or <code>top</code> 2. Ensure minimum requirements are met (4+ cores, 8GB+ RAM) 3. Terminate other resource-intensive applications 4. Increase Docker Desktop resources if applicable</p>"},{"location":"reference/troubleshooting/#network-issues","title":"Network Issues","text":"<p>Symptoms: - Services not accessible via nip.io domains - DNS resolution failures - Connection timeouts</p> <p>Solution: 1. Check firewall settings 2. Verify LAN IP detection: <code>ip route get 1.1.1.1 | awk '{print $7; exit}'</code> 3. Ensure ports 30080 and 30443 are available 4. Check if VPN is interfering with local network access</p>"},{"location":"reference/troubleshooting/#component-specific-issues","title":"Component-Specific Issues","text":""},{"location":"reference/troubleshooting/#argocd-not-accessible","title":"ArgoCD Not Accessible","text":"<ol> <li>Check if the ArgoCD server is running: <code>kubectl get pods -n argocd</code></li> <li>Verify the ingress/gateway is properly configured</li> <li>Check if the certificate is valid: <code>kubectl get certificate -n argocd</code></li> <li>If needed, access directly via port-forward: <code>kubectl port-forward -n argocd svc/argocd-server 8080:80</code></li> </ol>"},{"location":"reference/troubleshooting/#vault-initialization-problems","title":"Vault Initialization Problems","text":"<ol> <li>Check Vault pod status: <code>kubectl get pods -n vault-system</code></li> <li>Review Vault logs: <code>kubectl logs -n vault-system vault-0</code></li> <li>Verify if Vault is sealed:    <pre><code>kubectl exec -n vault-system vault-0 -- vault status\n</code></pre></li> <li>If Vault is sealed and needs to be unsealed, use the initialization task:    <pre><code>task vault:init\n</code></pre>    This will unseal Vault and configure authentication for External Secrets Operator.</li> </ol>"},{"location":"reference/troubleshooting/#grafana-dashboard-issues","title":"Grafana Dashboard Issues","text":"<ol> <li>Check if Grafana is running: <code>kubectl get pods -n observability</code></li> <li>Verify datasource connections in Grafana settings</li> <li>Check if Prometheus and Loki are accessible</li> <li>Review Grafana logs for errors</li> </ol>"},{"location":"reference/troubleshooting/#diagnostic-commands","title":"Diagnostic Commands","text":""},{"location":"reference/troubleshooting/#check-all-deployments","title":"Check All Deployments","text":"<pre><code>kubectl get all -A\n</code></pre>"},{"location":"reference/troubleshooting/#check-argocd-applications","title":"Check ArgoCD Applications","text":"<pre><code>kubectl get applications -A\n</code></pre>"},{"location":"reference/troubleshooting/#check-certificates","title":"Check Certificates","text":"<pre><code>kubectl get certificates,certificaterequests -A\n</code></pre>"},{"location":"reference/troubleshooting/#check-vault-status","title":"Check Vault Status","text":"<pre><code>kubectl exec -n vault-system &lt;vault-pod-name&gt; -- vault status\n</code></pre>"},{"location":"reference/troubleshooting/#check-external-secrets","title":"Check External Secrets","text":"<pre><code>kubectl get externalsecrets,secretstores -A\n</code></pre>"},{"location":"reference/troubleshooting/#debugging-workflows","title":"Debugging Workflows","text":""},{"location":"reference/troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<p>For most components, you can increase logging verbosity by modifying the appropriate values file before deployment.</p>"},{"location":"reference/troubleshooting/#check-component-status","title":"Check Component Status","text":"<pre><code># Check if all nodes are ready\nkubectl get nodes\n\n# Check system pods\nkubectl get pods -n kube-system\n\n# Check all namespaces\nkubectl get namespaces\n</code></pre>"},{"location":"reference/troubleshooting/#reset-and-cleanup","title":"Reset and Cleanup","text":""},{"location":"reference/troubleshooting/#complete-platform-reset","title":"Complete Platform Reset","text":"<pre><code>task destroy\n</code></pre>"},{"location":"reference/troubleshooting/#clean-registry-cache-if-needed","title":"Clean Registry Cache (if needed)","text":"<pre><code>task registry:clean\n</code></pre>"},{"location":"reference/troubleshooting/#clean-generated-documentation","title":"Clean Generated Documentation","text":"<pre><code>task docs:clean\n</code></pre>"},{"location":"reference/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"reference/troubleshooting/#slow-deployment","title":"Slow Deployment","text":"<ol> <li>Increase kubectl timeout: <code>KUBECTL_TIMEOUT=600s task deploy</code></li> <li>Check Docker image pull speeds</li> <li>Ensure sufficient I/O performance on storage device</li> </ol>"},{"location":"reference/troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<ol> <li>Check resource limits in values files</li> <li>Monitor actual usage vs. limits: <code>kubectl top nodes</code> and <code>kubectl top pods</code></li> <li>Consider scaling down non-essential components</li> </ol>"},{"location":"reference/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"reference/troubleshooting/#check-logs","title":"Check Logs","text":"<p>Most issues can be diagnosed by checking component logs: <pre><code>kubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"reference/troubleshooting/#check-events","title":"Check Events","text":"<p>Kubernetes events can provide additional context: <pre><code>kubectl get events -A --sort-by='.lastTimestamp'\n</code></pre></p>"},{"location":"reference/troubleshooting/#community-support","title":"Community Support","text":"<ul> <li>Check the GitHub Issues for similar problems</li> <li>Consider creating a new issue with detailed information about your problem</li> </ul>"}]}